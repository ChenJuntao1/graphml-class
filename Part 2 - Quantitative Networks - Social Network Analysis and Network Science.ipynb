{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f26d5a-daad-493d-aeb6-7e312ee2472d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import calendar\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from datetime import date\n",
    "\n",
    "import dateutil\n",
    "import graphistry\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c73178-317f-4bda-953e-74597e4b6f6c",
   "metadata": {},
   "source": [
    "# Part 2: Quantitative Networks: Social Network Analysis and Network Science\n",
    "\n",
    "Having reviewed the theory of network science and graph machine learning in Part 1, it is time to start coding! In this part of the course we will survey network science. Please keep in mind that as a multidisciplinary field, network science is impossible to cover completely.\n",
    "\n",
    "Albert-László Barabási has written a [comprehensive book on network science](http://networksciencebook.com/chapter/2) that provides more in depth treatment of topics like [graph theory](http://networksciencebook.com/chapter/2), [random networks](http://networksciencebook.com/chapter/3) and [communities](http://networksciencebook.com/chapter/9).\n",
    "\n",
    "## High-energy Physics Theory Citation Network\n",
    "\n",
    "We are going to be using the [High-energy physics theory citation network](https://snap.stanford.edu/data/cit-HepTh.html) from [Stanford SNAP](https://snap.stanford.edu/index.html) which has many large network datasets available in the [Stanford Large Network Dataset Collection](https://snap.stanford.edu/data/).\n",
    "\n",
    "The dataset includes:\n",
    "\n",
    "* [Citation graph edge list](https://snap.stanford.edu/data/cit-HepTh.txt.gz) contains node ID pairs. Node IDs are standard paper identifiers. This will build the core structure of our network.\n",
    "* [Paper metadata](cit-HepTh-abstracts.tar.gz) including abstracts. This will add propertis to our network.\n",
    "* [Publishing dates on arXiv](https://snap.stanford.edu/data/cit-HepTh-dates.txt.gz) will make our citation network a temporal [Directed-Acyclic-Graph (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph) since one paper can't cite another before it is written and there are no reciprocal edges. While we don't focus on this, it does affect our analysis.\n",
    "\n",
    "## Setting up Graphistry\n",
    "\n",
    "Throughout this part of the course we will be using `pygraphistry` and [Graphistry Hub](https://hub.graphistry.com/) [https://hub.graphistry.com/](https://hub.graphistry.com/) to visualize networks. Both are free for personal use and are powerful for visualizing networks large and small.\n",
    "\n",
    "You can [signup](https://hub.graphistry.com/accounts/signup/) for a Graphistry account at [https://hub.graphistry.com/accounts/signup/](https://hub.graphistry.com/accounts/signup/). <b>You should use a username/password/email to get the required credentials</b>, although after that you can login with your Github or Google account.\n",
    "\n",
    "<center><img src=\"images/graphistry_hub_registration.png\" /></center>\n",
    "\n",
    "Retain and use your credentials in the login form and in the environment variables in the next cell below. You should set the `GRAPHISTRY_USERNAME` and `GRAPHISTRY_PASSWORD` variables in the `env/graphistry.env` file, and then restart this docker container to pickup the new values.\n",
    "\n",
    "<center><img src=\"images/graphistry_hub_homepage.png\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "526ec783-7ddf-406c-913e-588c376cde4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variable setup\n",
    "GRAPHISTRY_USERNAME = os.getenv(\"GRAPHISTRY_USERNAME\")\n",
    "GRAPHISTRY_PASSWORD = os.getenv(\"GRAPHISTRY_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96638e3d-bff3-48eb-916a-34b9b042a080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graphistry.register(\n",
    "    api=3,\n",
    "    username=GRAPHISTRY_USERNAME,\n",
    "    password=GRAPHISTRY_PASSWORD,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee9d6bf-d6f4-4104-9579-e11c4f844072",
   "metadata": {},
   "source": [
    "# Network Science on Citation Networks\n",
    "\n",
    "We start with an example of a citation network: [High-energy physics theory citation network](https://snap.stanford.edu/data/cit-HepTh.html) from [Stanford SNAP](http://snap.stanford.edu/). A citation network describes how one academic paper cites another one.\n",
    "\n",
    "> Arxiv HEP-TH (high energy physics theory) citation graph is from the e-print arXiv and covers all the citations within a dataset of 27,770 papers with 352,807 edges. If a paper i cites paper j, the graph contains a directed edge from i to j. If a paper cites, or is cited by, a paper outside the dataset, the graph does not contain any information about this.\n",
    ">\n",
    "> The data covers papers in the period from January 1993 to April 2003 (124 months). It begins within a few months of the inception of the arXiv, and thus represents essentially the complete history of its HEP-TH section.\n",
    ">\n",
    "> The data was originally released as a part of [2003 KDD Cup](http://www.cs.cornell.edu/projects/kddcup/).\n",
    "\n",
    "J. Leskovec, J. Kleinberg and C. Faloutsos. [Graphs over Time: Densification Laws, Shrinking Diameters and Possible Explanations.](http://www.cs.cmu.edu/~jure/pubs/powergrowth-kdd05.pdf) ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2005.\n",
    "\n",
    "## NetworkX\n",
    "\n",
    "The [networkx Algorithms documentation](https://networkx.org/documentation/stable/reference/algorithms/index.html) lists a fairly comprehensive set of tools available for the network scientist. We will be using networkx throughout the course.\n",
    "\n",
    "## cuGraph\n",
    "\n",
    "The [NVIDIA RAPIDS cuGraph](https://docs.rapids.ai/api/cugraph/stable/) ([Github](https://github.com/rapidsai/cugraph)) is a GPU accelerated graph analytics library that is roughly compatible with `networkx`. It can rapidly compute expensive metrics compare to CPU, where processing is often limited to a single core.\n",
    "\n",
    "An NVIDIA GPU and cuGraph are optional in this course, but will be helpful as you use complex algorithms on large networks.\n",
    "\n",
    "## Extract Edge List and Build Directional Graph (DiGraph)\n",
    "\n",
    "First we download the edge list and build the structure of the network: `(paper)-cited->(paper)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b04e501-4648-46c5-85fd-64f954ff5837",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote downloaded edge file to data/cit-HepTh.txt.gz\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import io\n",
    "import networkx as nx\n",
    "import tarfile\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "# Initialize a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Download and load edges (citations) from `cit-HepTh.txt.gz`\n",
    "response = requests.get(\"https://snap.stanford.edu/data/cit-HepTh.txt.gz\")\n",
    "gzip_content = io.BytesIO(response.content)\n",
    "\n",
    "with open(\"data/cit-HepTh.txt.gz\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "    print(\"Wrote downloaded edge file to data/cit-HepTh.txt.gz\")\n",
    "\n",
    "# We need to create sequential IDs starting from 0 for littleballoffur\n",
    "file_to_fur: Dict[int, int] = {}\n",
    "fur_to_file: Dict[int, int] = {}\n",
    "current_idx = 0\n",
    "\n",
    "# Decompress the gzip content and build the edge list for our network\n",
    "with gzip.GzipFile(fileobj=gzip_content) as f:\n",
    "    for line_number, line in enumerate(f):\n",
    "\n",
    "        if line_number > 20:\n",
    "            break\n",
    "        \n",
    "        line = line.decode('utf-8')\n",
    "        \n",
    "        # Ignore comment lines that start with '#'\n",
    "        if not line.startswith('#'):\n",
    "\n",
    "            # Source (citing), desstination (cited) papers\n",
    "            citing_key, cited_key = line.strip().split('\\t')\n",
    "            citing_key, cited_key = int(citing_key), int(cited_key)\n",
    "\n",
    "            # If the either of the paper IDs don't exist, make one\n",
    "            for key in [citing_key, cited_key]:\n",
    "                \n",
    "                if key not in file_to_fur:\n",
    "\n",
    "                    # Build up an index that maps back and forth\n",
    "                    file_to_fur[key] = current_idx\n",
    "                    fur_to_file[current_idx] = key\n",
    "\n",
    "                    # Bump the current ID\n",
    "                    current_idx += 1\n",
    "            \n",
    "            G.add_edge(file_to_fur[citing_key], file_to_fur[citing_key])\n",
    "\n",
    "            # Conditionally set the keys on the nodes\n",
    "            G.nodes[file_to_fur[citing_key]][\"file\"] = citing_key\n",
    "            G.nodes[file_to_fur[citing_key]][\"fur\"] = file_to_fur[citing_key]\n",
    "            G.nodes[file_to_fur[citing_key]][\"file\"] = cited_key\n",
    "            G.nodes[file_to_fur[citing_key]][\"fur\"] = file_to_fur[cited_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc0c296-d40d-4f99-bf12-f29d5a624e8e",
   "metadata": {},
   "source": [
    "## Summarize the Properties of our DiGraph\n",
    "\n",
    "Let's check how many nodes and edges we have. This will help evaluate how we are doing when we parse the abstracts to add properties to our DiGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db5c603f-d26b-4b6c-905d-bab3b46638f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 25,059\n",
      "Total edges: 25,059\n",
      "Total components: 25,059\n",
      "Undirected is weakly connected: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total nodes: {G.number_of_nodes():,}\")\n",
    "print(f\"Total edges: {G.number_of_edges():,}\")\n",
    "print(f\"Total components: {nx.number_connected_components(G.to_undirected()):,}\")\n",
    "print(f\"Undirected is weakly connected: {nx.is_weakly_connected(G)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d824a6-b246-4539-873d-e12fb33fd7b9",
   "metadata": {},
   "source": [
    "### ChatGPT4 and Russell Jurney say..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35015fd1-50f0-4076-8392-a32084068d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def describe_graph(G):\n",
    "    \"\"\"Given a networkx Graph, describe its key properties.\"\"\"\n",
    "\n",
    "    print(f\"Number of nodes: {G.number_of_nodes():,}\")\n",
    "    print(f\"Number of edges: {G.number_of_edges():,}\")\n",
    "\n",
    "    # Compute various network properties\n",
    "    degrees = [deg for node, deg in nx.degree(G)]\n",
    "    avg_degree = sum(degrees) / G.number_of_nodes()\n",
    "    print(f\"Average degree: {avg_degree:,.3f}\")\n",
    "\n",
    "    components = nx.connected_components(G.to_undirected())\n",
    "    largest_component = max(components, key=len)\n",
    "    print(f\"Number of connected components: {nx.number_connected_components(G.to_undirected()):,}\")\n",
    "    print(f\"Size of the largest component: {len(largest_component):,}\")\n",
    "\n",
    "    # If the network is directed, you can also print the following\n",
    "    if G.is_directed():\n",
    "        print(f\"Number of strongly connected components: {nx.number_strongly_connected_components(G):,}\")\n",
    "        print(f\"Number of weakly connected components: {nx.number_weakly_connected_components(G):,}\")\n",
    "\n",
    "    avg_clustering = nx.average_clustering(G)\n",
    "    print(f\"Average clustering coefficient: {avg_clustering:.6f}\")\n",
    "\n",
    "    try:\n",
    "        avg_shortest_path_length = nx.average_shortest_path_length(G)\n",
    "        print(f\"Average shortest path length: {avg_shortest_path_length}\")\n",
    "    except nx.NetworkXError:\n",
    "        print(\"Graph is not connected, average shortest path length is not defined.\")\n",
    "\n",
    "describe_graph(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c861508c-20a0-463b-aff4-6c824db0a5c1",
   "metadata": {},
   "source": [
    "### Histogram of Connected Component Sizes\n",
    "\n",
    "134 components - our graph is not one large hairball! Let's see what sizes our connected components are. Usually these are a power law distribution, with one large component taking up 30 or more percent of nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85de86d-b403-469e-afed-c5752210dc9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get connected components and their sizes\n",
    "components = nx.connected_components(G.to_undirected())\n",
    "component_sizes = [len(c) for c in components]\n",
    "\n",
    "# Use seaborn to create the histogram\n",
    "sns.histplot(component_sizes, kde=True, bins=30, log_scale=True)\n",
    "plt.title('Histogram of Connected Component Sizes')\n",
    "plt.xlabel('Component Size')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5acd03-8e92-440a-80a5-fc66b33004af",
   "metadata": {},
   "source": [
    "## Average Clustering Coefficient\n",
    "\n",
    ">The clustering coefficient is an indicator of the interconnectedness or cohesion among nodes in a graph. It's a measure of the extent to which the immediate neighbors of a particular node link to each other.\n",
    ">\n",
    ">The range of a clustering coefficient is between 0 and 1:\n",
    ">\n",
    ">When the clustering coefficient is 0, it signifies a scenario where none of the nodes have direct connections with each other. This can be seen in graphs such as a tree or a radial network where nodes are only connected to a central hub, but not with each other.\n",
    ">\n",
    ">In contrast, a clustering coefficient of 1 indicates a fully connected graph. Here, each node in the graph is directly linked with all other nodes.\n",
    "\n",
    "--ChatGPT4 and Russell Jurney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7bbb44-99c0-4b62-856e-7cb40c9fe82b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A good start...\n",
    "print(f\"Average degree: {G.number_of_edges()/G.number_of_nodes():.3f}\")\n",
    "print(f\"Median degree: {np.median([deg for node, deg in G.degree()]):.3f}\")\n",
    "\n",
    "clustering_coefficient = nx.average_clustering(G)\n",
    "print(f\"Graph clustering coefficient: {clustering_coefficient:.3f}\")\n",
    "print(f\"Median clustering coefficient: {np.median([coeff for node, coeff in nx.clustering(G).items()]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d90ed4-ab34-452f-9a7d-abd63ecd98ab",
   "metadata": {
    "tags": []
   },
   "source": [
    ">An average clustering coefficient of 0.16 in our `nx.DiGraph` implies there is a relatively low degree of clustering or tight-knit communities. It suggests a network structure that is more spread out, with fewer connections among immediate neighbors of each node. Networks such as web pages or citation networks often present lower clustering coefficients, as these networks are characterized by a wide range of diverse connections that do not necessarily link back to each other. Citation networks are actually temporal networks and are constrained by the fact that papers can't cite future papers, but only past ones. They are therefore DAGs - directed, acyclic graphs.\n",
    "\n",
    "--ChatGPT4 and Russell Jurney\n",
    "\n",
    "### Node Clustering Coefficients\n",
    "\n",
    ">The clustering coefficient of a node is a measure of the degree to which nodes in a graph tend to cluster together. It provides an indication of how the neighbors of a _particular node_ are interconnected. It measures how close a node's neighbors are to being a complete graph or a clique. It is defined as the fraction of possible triangles that exist through the node, or equivalently, as the fraction of the node's neighbors that are also neighbors of each other.\n",
    "\n",
    "--ChatGPT4 and Russell Jurney\n",
    "\n",
    "We're going to add a property to each node in the network for each node's clustering coefficient. You can find more on clustering coefficients in citation networks in [Modeling the clustering in citation networks](https://arxiv.org/abs/1104.4209)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb65251-2580-41d3-9297-c17c0c89be26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustering_coeffs = nx.clustering(G)\n",
    "\n",
    "for node, clustering_coeff in clustering_coeffs.items():\n",
    "    G.nodes[node]['clustering_coefficient'] = clustering_coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92790ec-d942-4db9-a001-46b253096fb3",
   "metadata": {},
   "source": [
    "Let's look at how the clustering coefficient is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e863c2-61aa-426b-ba8b-0d7cd965e5fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get connected components and their sizes\n",
    "clustering_coeff_counts = [val for key, val in clustering_coeffs.items()]\n",
    "\n",
    "# Use seaborn to create the histogram\n",
    "sns.histplot(clustering_coeff_counts, kde=True, bins=30, log_scale=False)\n",
    "plt.title('Histogram of Node Clustering Coefficients')\n",
    "plt.xlabel('Clustering Coefficient')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e2c1e-9a35-4282-bc02-2460fa11d6b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "G.nodes[key_to_idx[\"9711194\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d90d7c-8ddb-453d-b0a4-2e6b01623cd8",
   "metadata": {},
   "source": [
    "A clustering coefficient of around 0.2 indicates that most nodes have relatively low (but still significant) clustering coefficients - their neighbors are sparsely connected. How does this fit with the average degree of 12 and graph clustering coefficient of 0.16? 16% of the average node's connections are connected with one another. From the histogram which has a skewed but fairly normal distribution, it looks like the _typical_ node has these properties as well. These are characteristics of a [Small World Network](https://en.wikipedia.org/wiki/Small-world_network). Most citation graphs are [small world networks](https://en.wikipedia.org/wiki/Citation_graph). \n",
    "\n",
    "Note: NetworkX has a set of functions [`networkx.algorithms.smallworld`](https://networkx.org/documentation/stable/reference/algorithms/smallworld.html) that can determine the \"small worldiness of a network.\"\n",
    "\n",
    "Why do we care what kind of theoretical graph model fits our *real world* network. Because the field of network science is multidisciplinary and papers cover topics from physics to biology to sociology to microchip design. The type of network you have - often in the same settings as yours - has been rigorously studied. Reading a paper or two describing your graph gives you the context you need to extract insights, build models and auomate tasks using machine learning and artificial intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6c3894-a900-476f-8c6b-8eeb7787d90b",
   "metadata": {},
   "source": [
    "## Add Properties to Nodes in Network\n",
    "\n",
    "Now we will use `extract_paper_info(text)` to add structured data to the nodes in our network.\n",
    "\n",
    "### Using ChatGPT to Write NetworkX Code\n",
    "\n",
    "We cover Chatbots at the end of this course, I just want to point out that the following dialogue generated the basis for the code below.\n",
    "\n",
    "```\n",
    "I am going to past some text representing some semi-structured data about an academic paper below:\n",
    "\n",
    "Paper: hep-th/0002031\n",
    "From: Maulik K. Parikh \n",
    "Date: Fri, 4 Feb 2000 17:04:51 GMT   (10kb)\n",
    "\n",
    "Title: Confinement and the AdS/CFT Correspondence\n",
    "Authors: D. S. Berman and Maulik K. Parikh\n",
    "Comments: 12 pages, 1 figure, RevTeX\n",
    "Report-no: SPIN-1999/25, UG-1999/42\n",
    "Journal-ref: Phys.Lett. B483 (2000) 271-276\n",
    "\\\\\n",
    "  We study the thermodynamics of the confined and unconfined phases of\n",
    "superconformal Yang-Mills in finite volume and at large N using the AdS/CFT\n",
    "correspondence. We discuss the necessary conditions for a smooth phase\n",
    "crossover and obtain an N-dependent curve for the phase boundary.\n",
    "\\\\\n",
    "\n",
    "Now I want you to write python code to extract the fields \"Paper\", \"From\", \"Date\", \"Title\", \"Authors\", \"Comments\", \"Report-no\", \"Journal-ref\" and the last field the \"Abstract\" text content.\n",
    "```\n",
    "\n",
    "One cycle of correction looked like:\n",
    "\n",
    "```\n",
    "That code did not work on this record:\n",
    "\n",
    "------------------------------------------------------------------------------\n",
    "\\\\\n",
    "Paper: hep-th/9711200\n",
    "From: Juan Maldacena <malda@physics.rutgers.edu>\n",
    "Date: Thu, 27 Nov 1997 23:53:13 GMT   (22kb)\n",
    "Date (revised v2): Mon, 8 Dec 1997 18:59:11 GMT   (23kb)\n",
    "Date (revised v3): Thu, 22 Jan 1998 15:42:41 GMT   (23kb)\n",
    "\n",
    "Title: The Large N Limit of Superconformal Field Theories and Supergravity\n",
    "Authors: Juan M. Maldacena\n",
    "Comments: 20 pages, harvmac, v2: section on AdS_2 corrected, references added,\n",
    "  v3: More references and a sign in eqns 2.8 and 2.9 corrected\n",
    "Report-no: HUTP-98/A097\n",
    "Journal-ref: Adv.Theor.Math.Phys. 2 (1998) 231-252; Int.J.Theor.Phys. 38 (1999)\n",
    "  1113-1133\n",
    "\\\\\n",
    "  We show that the large $N$ limit of certain conformal field theories in\n",
    "various dimensions include in their Hilbert space a sector describing\n",
    "supergravity on the product of Anti-deSitter spacetimes, spheres and other\n",
    "compact manifolds. This is shown by taking some branes in the full M/string\n",
    "theory and then taking a low energy limit where the field theory on the brane\n",
    "decouples from the bulk. We observe that, in this limit, we can still trust the\n",
    "near horizon geometry for large $N$. The enhanced supersymmetries of the near\n",
    "horizon geometry correspond to the extra supersymmetry generators present in\n",
    "the superconformal group (as opposed to just the super-Poincare group). The 't\n",
    "Hooft limit of 4-d ${\\cal N} =4$ super-Yang-Mills at the conformal point is\n",
    "shown to contain strings: they are IIB strings. We conjecture that\n",
    "compactifications of M/string theory on various Anti-deSitter spacetimes are\n",
    "dual to various conformal field theories. This leads to a new proposal for a\n",
    "definition of M-theory which could be extended to include five non-compact\n",
    "dimensions.\n",
    "\\\\\n",
    "\n",
    "It should have started the Abstract extraction with \"We show that\" but instead it includes the entire document. Please fix.\n",
    "```\n",
    "\n",
    "There were a few more iterations, so this is not \"free.\" I then had to edit the code and that is what is in the cell. ChatGPT is your running mate :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c2370b-2d51-4da3-9b2c-bf5962882c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_paper_info(record):\n",
    "    \"\"\"Extract structured information from the text of academic paper text records using regular expressions.\n",
    "       \n",
    "       Note: I was written wholly or in part by ChatGPT4 on May 23, 2023.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty dictionary to hold the information\n",
    "    info = {}\n",
    "    \n",
    "    # Match \"Paper\" field\n",
    "    paper_match = re.search(r\"Paper:\\s*(.*)\", record)\n",
    "    if paper_match:\n",
    "        info['Paper'] = paper_match.group(1)\n",
    "        \n",
    "    # # Match \"From\" field\n",
    "    # from_match = re.search(r\"From:\\s*(.*)\", record)\n",
    "    # if from_match:\n",
    "    #     info['From'] = from_match.group(1)\n",
    "\n",
    "    # Match \"From\" field\n",
    "    from_match = re.search(r\"From:\\s*([^<]*)<\", record)\n",
    "    if from_match:\n",
    "        info['From'] = from_match.group(1).strip()\n",
    "    \n",
    "    # Match \"Date\" field\n",
    "    date_match = re.search(r\"Date:\\s*(.*)\", record)\n",
    "    if date_match:\n",
    "        info['Date'] = date_match.group(1)\n",
    "    \n",
    "    # Match \"Title\" field\n",
    "    title_match = re.search(r\"Title:\\s*(.*)\", record)\n",
    "    if title_match:\n",
    "        info['Title'] = title_match.group(1)\n",
    "\n",
    "    # Match \"Authors\" field\n",
    "    authors_match = re.search(r\"Authors:\\s*(.*)\", record)\n",
    "    if authors_match:\n",
    "        info['Authors'] = authors_match.group(1)\n",
    "\n",
    "    # Match \"Comments\" field\n",
    "    comments_match = re.search(r\"Comments:\\s*(.*)\", record)\n",
    "    if comments_match:\n",
    "        info['Comments'] = comments_match.group(1)\n",
    "\n",
    "    # Match \"Report-no\" field\n",
    "    report_no_match = re.search(r\"Report-no:\\s*(.*)\", record)\n",
    "    if report_no_match:\n",
    "        info['Report-no'] = report_no_match.group(1)\n",
    "\n",
    "    # Match \"Journal-ref\" field\n",
    "    journal_ref_match = re.search(r\"Journal-ref:\\s*(.*)\", record)\n",
    "    if journal_ref_match:\n",
    "        info['Journal-ref'] = journal_ref_match.group(1)\n",
    "\n",
    "    # Extract \"Abstract\" field\n",
    "    abstract_pattern = r\"Journal-ref:[^\\\\\\\\]*\\\\\\\\[\\n\\s]*(.*?)(?=\\\\\\\\)\"\n",
    "    abstract_match = re.search(abstract_pattern, record, re.DOTALL)\n",
    "    if abstract_match:\n",
    "        abstract = abstract_match.group(1)\n",
    "        abstract = abstract.replace('\\n', ' ').replace('  ', ' ')\n",
    "        info['Abstract'] = abstract.strip()\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdffc9e7-1055-4772-bbda-4a9ae08a81e8",
   "metadata": {},
   "source": [
    "### Debugging Paper Metadata Parsing\n",
    "\n",
    "The first version didn't work, so I added some documents below. I'm leaving it in the notebook can see what I did: iteratively improve document parsing (data cleaning) until the network meets the needs of our analysis, application or audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f03d74-98fd-4540-aea4-fd687a88b02d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc1 = \"\"\"------------------------------------------------------------------------------\n",
    "\\\\\n",
    "Paper: hep-th/9612115\n",
    "From: Asato Tsuchiya <tsuchiya@theory.kek.jp>\n",
    "Date: Wed, 11 Dec 1996 17:38:56 +0900   (20kb)\n",
    "Date (revised): Tue, 31 Dec 1996 01:06:34 +0900\n",
    "\n",
    "Title: A Large-N Reduced Model as Superstring\n",
    "Authors: N. Ishibashi, H. Kawai, Y. Kitazawa and A. Tsuchiya\n",
    "Comments: 29 pages, Latex, a footnote and references added, eq.(3.52)\n",
    "  corrected, minor corrections\n",
    "Report-no: KEK-TH-503, TIT/HEP-357\n",
    "Journal-ref: Nucl.Phys. B498 (1997) 467-491\n",
    "\\\\\n",
    "  A matrix model which has the manifest ten-dimensional N=2 super Poincare\n",
    "invariance is proposed. Interactions between BPS-saturated states are analyzed\n",
    "to show that massless spectrum is the same as that of type IIB string theory.\n",
    "It is conjectured that the large-N reduced model of ten-dimensional super\n",
    "Yang-Mills theory can be regarded as a constructive definition of this model\n",
    "and therefore is equivalent to superstring theory.\n",
    "\\\\\n",
    "\"\"\"\n",
    "\n",
    "doc2 = \"\"\"------------------------------------------------------------------------------\n",
    "\\\\\n",
    "Paper: hep-th/9711029\n",
    "From: John Schwarz <jhs@theory.caltech.edu>\n",
    "Date: Wed, 5 Nov 1997 17:30:55 GMT   (20kb)\n",
    "Date (revised v2): Thu, 6 Nov 1997 23:52:45 GMT   (21kb)\n",
    "\n",
    "Title: The Status of String Theory\n",
    "Author: John H. Schwarz\n",
    "Comments: 16 pages, latex, two figures; minor corrections, references added\n",
    "Report-no: CALT-68-2140\n",
    "\\\\\n",
    "  There have been many remarkable developments in our understanding of\n",
    "superstring theory in the past few years, a period that has been described as\n",
    "``the second superstring revolution.'' Several of them are discussed here. The\n",
    "presentation is intended primarily for the benefit of nonexperts.\n",
    "\\\\\n",
    "\"\"\"\n",
    "\n",
    "paper_info = extract_paper_info(doc1)\n",
    "paper_id = paper_info.get(\"Paper\", \"\").split(\"/\")[-1]  # Get the paper ID part of the \"Paper\" field\n",
    "print(paper_id, paper_info)\n",
    "\n",
    "paper_info = extract_paper_info(doc2)\n",
    "paper_id = paper_info.get(\"Paper\", \"\").split(\"/\")[-1]  # Get the paper ID part of the \"Paper\" field\n",
    "print(paper_id, paper_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc75bfac-2f2e-4305-8d40-072e5f64aead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the abstracts from `cit-HepTh-abstracts.tar.gz`\n",
    "abstract_response = requests.get(\"https://snap.stanford.edu/data/cit-HepTh-abstracts.tar.gz\")\n",
    "\n",
    "# Convert the response content to an in-memory binary stream\n",
    "abstract_gzip_content = io.BytesIO(abstract_response.content)\n",
    "\n",
    "# Decompress the gzip content\n",
    "with gzip.GzipFile(fileobj=abstract_gzip_content) as f:\n",
    "    with tarfile.open(fileobj=f, mode='r|') as tar:\n",
    "        for member in tar:\n",
    "            abstract_file = tar.extractfile(member)\n",
    "            if abstract_file is not None:\n",
    "                content = abstract_file.read().decode('utf-8')\n",
    "                paper_info = extract_paper_info(content)\n",
    "                if paper_info:\n",
    "                    paper_id = paper_info.get(\"Paper\", \"\").split(\"/\")[-1]  # Get the paper ID part of the \"Paper\" field\n",
    "                    if paper_id in G:\n",
    "                        for field, value in paper_info.items():\n",
    "                            if paper_id in G:\n",
    "                                G.nodes[paper_id][field] = value\n",
    "                    else:\n",
    "                        # Add isolated nodes if paper_id isn't in G\n",
    "                        G.add_node(paper_id, **paper_info)\n",
    "\n",
    "# Now `G` is a property graph representing the \"High-energy physics theory citation network\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f853cc67-d820-4e5f-b18c-fd9093ad9bec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "G.nodes[key_to_idx[\"9711194\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee404ce-3aad-43a5-9e33-0d52a65a509a",
   "metadata": {},
   "source": [
    "## Adding Publishing Dates to our Nodes\n",
    "\n",
    "SNAP also makes available the time the papers were published to arXiv, which can lead to some interesting analyses. Timestamps are available [here](https://snap.stanford.edu/data/cit-HepTh-dates.txt.gz) [https://snap.stanford.edu/data/cit-HepTh-dates.txt.gz](https://snap.stanford.edu/data/cit-HepTh-dates.txt.gz). Let's add them as a property of our nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e332b2ed-51a1-46de-9ed7-a8a1174c8947",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download and load edges (citations) from `cit-HepTh.txt.gz`\n",
    "response = requests.get(\"https://snap.stanford.edu/data/cit-HepTh-dates.txt.gz\")\n",
    "gzip_content = io.BytesIO(response.content)\n",
    "\n",
    "# Decompress the gzip content and add a \"published\" date property to our nodes\n",
    "with gzip.GzipFile(fileobj=gzip_content) as f:\n",
    "    for line in f:\n",
    "        line = line.decode('utf-8')\n",
    "        # Ignore lines that start with '#'\n",
    "        if not line.startswith('#'):\n",
    "            paper_id, iso_date = line.strip().split('\\t')\n",
    "            if paper_id in G:\n",
    "                # Add a UTC timestamp for the data\n",
    "                G.nodes[paper_id][\"published\"] = calendar.timegm(date.fromisoformat(iso_date).timetuple())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8225144-da82-4fb2-9155-64371949d871",
   "metadata": {},
   "source": [
    "## Saving our Physics Citation Network Dataset\n",
    "\n",
    "We will reuse the physics dataset and `networkx` in our `Part 4 - GraphML` section, so let's save it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd5930-1299-491c-932b-4c86c86502f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write using GEXF format - the date has to be in UTC format for this to work.\n",
    "nx.write_gexf(G, path=\"data/physics.gexf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c85f9ae-d8e9-417e-92e0-5e3fb68879f0",
   "metadata": {},
   "source": [
    "# Visualizing Networks in Graphistry\n",
    "\n",
    "[Graphistry](https://graphistry.com/) can handle hundreds of thousands of nodes and edges using GPU acceleration. They provide GPU acceleration via [Graphistry Hub](https://hub.graphistry.com/), so we don't need to have a GPU in our notebook computers.\n",
    "\n",
    "Let's take a first look at our network in Graphistry to see what it looks like!\n",
    "\n",
    "## Random Node Sampling\n",
    "\n",
    "The entire network of millions of nodes is too large for Graphistry. We will sample it down to about 15% of its original size and take a peek :)\n",
    "\n",
    "> The most obvious way to create a sample graph is to uniformly at random select a set of nodes N and a sample is then a graph induced by the set of nodes N. We call this algorithm the Random Node (RN) sampling.\n",
    "\n",
    "--Leskovec and Faloutsos, 2006\n",
    "\n",
    "We are going to randomly sample nodes to get a set of nodes, and then connect them with all incident edges, the induced subgraph of the sample nodes. Rather than sample uniformly, we will weight with nodes' probabilities by their normalized PageRank scores. This technique was suggested by [Jure Leskovec](https://dblp.org/pid/l/JureLeskovec.html) and [Christos Faloutsos](https://dblp.org/pid/f/CFaloutsos.html) in the paper [Sampling from Large Graphs](https://cs.stanford.edu/~jure/pubs/sampling-kdd06.pdf) which presents a range of techniques for sampling large graphs.\n",
    "\n",
    "One thing from this paper to note, is the minimum sample size to maintain a network's structural properties.\n",
    "\n",
    "> We show that a 15% sample is usually enough, to match the properties (S1–S9 and T1–T5) of the real graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d91277-49a3-4932-b452-1afd302a251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric_indices = [index for index in range(G.number_of_nodes())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b95f3-36e0-4bf6-b457-e72570608d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from littleballoffur import PageRankBasedSampler\n",
    "\n",
    "pagerank_sampler = PageRankBasedSampler(number_of_nodes=G.number_of_nodes)\n",
    "G_pagerank_sample = pagerank_sampler.sample(G.to_undirected())\n",
    "\n",
    "print(f\"Original citation graph nodes: {G.number_of_nodes:,}\")\n",
    "print(f\"PageRank node sampled nodes: {G_pagerank_sample.number_of_nodes:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e604e-5caa-48ef-b257-077c93e914ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    graphistry.bind(source='src', destination='dst', node='nodeid')\n",
    "    #.options({\"\"})\n",
    "    .plot(G_pagerank_sample)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d213a1c5-742c-4dd3-ba31-8c71580fbabf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Describing our Citation Network `nx.DiGraph`\n",
    "\n",
    "Now we can check a few theoretical properties of our DiGraph. I'll frame it in terms of questions and metrics.\n",
    "\n",
    "* Domain question: How many citations does each paper have across the entire network?\n",
    "* Metric 1: What is the average degree, in-degree and out-degree of the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a76395-2689-4c56-b821-add6de854184",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract degree, in-degree, and out-degree for each node\n",
    "# +1 is to avoid problems with log-scale display\n",
    "degree_sequence = [d + 1 for n, d in G.degree()]\n",
    "in_degree_sequence = [d + 1 for n, d in G.in_degree()]\n",
    "out_degree_sequence = [d + 1 for n, d in G.out_degree()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd7308-16d6-469f-a296-7872845e8353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create histogram of degree\n",
    "ax = sns.histplot(degree_sequence, bins=20, kde=True, log_scale=True)\n",
    "ax.set(xlabel='Degree')\n",
    "plt.title('Degree Histogram')\n",
    "plt.show()\n",
    "\n",
    "# Create histogram of in-degree\n",
    "ax = sns.histplot(in_degree_sequence, bins=20, kde=True, log_scale=True)\n",
    "ax.set(xlabel='In-Degree')\n",
    "plt.title('In-Degree Histogram')\n",
    "plt.show()\n",
    "\n",
    "# Create histogram of out-degree\n",
    "ax = sns.histplot(out_degree_sequence, bins=20, kde=True, log_scale=True)\n",
    "ax.set(xlabel='Out-Degree')\n",
    "plt.title('Out-Degree Histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12408801-a70c-4fc0-aa03-9e4f16998d72",
   "metadata": {},
   "source": [
    "## Tactical Decision: Removing Uncited Papers\n",
    "\n",
    "Uncited papers aren't very interesting - they have degree of zero - so aren't part of any connected component of a network. Let's filter them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80321d8-337f-475d-9a95-225d8902cdd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Nodes before removal: {G.number_of_nodes():,}\")\n",
    "\n",
    "# Remove the isolated nodes\n",
    "G.remove_nodes_from(list(nx.isolates(G)))\n",
    "\n",
    "print(f\"Nodes after removal: {G.number_of_nodes():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6b8dc-b90b-4605-ac23-05e9795733a5",
   "metadata": {},
   "source": [
    "# Bridges\n",
    "\n",
    "If removing an edge between two nodes breaks a single component into multiple components, that edge was a bridge. Let's check if our network has any bridges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d26c570-051a-4504-aa9d-9766ebfe6772",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.has_bridges(G.to_undirected())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7115960-05bd-4570-85cd-7e6c7cf26e61",
   "metadata": {},
   "source": [
    "## Examining Bridges in Graphistry\n",
    "\n",
    "Let's examine bridges and local bridges in Graphistry by adding them as node properties to our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff972d6-3695-41dc-aa19-2029638e77f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bridge_edges = list(nx.bridges(G.to_undirected()))\n",
    "local_bridge_edges = list(nx.local_bridges(G.to_undirected()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d413ff-8c34-473e-a5e4-57137a23b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bridges[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6777f0-477a-456a-8266-35bd5c3819f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's add the \"bridge\" property to each edge.\n",
    "for u, v in G.edges():\n",
    "    G[u][v][\"bridge\"] = (u, v) in bridge_edges or (v, u) in bridge_edges\n",
    "\n",
    "# Let's print all edges with their properties to verify.\n",
    "for u, v, data in G.edges(data=True)[:10]:\n",
    "    print(u, v, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ae72a-e047-488e-9056-6f0fe8bb6c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01ea174-47b8-4a23-acce-aaf4e8b6f390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "756a968c-1bfd-4026-b86f-48dac96cfea1",
   "metadata": {},
   "source": [
    "## Assortativity\n",
    "\n",
    "> [Assortativity](https://en.wikipedia.org/wiki/Assortativity) in networks refers to a correlation pattern observed in real-world networks where nodes are preferentially connected to other nodes that are like (or unlike) them in some way. This is essentially a bias in connection preference.\n",
    "\n",
    "--ChatGPT4\n",
    "\n",
    "A related term is _assortative mixing_:\n",
    "\n",
    "> In the study of complex networks, assortative mixing, or assortativity, is a bias in favor of connections between network nodes with similar characteristics. In the specific case of social networks, assortative mixing is also known as homophily. The rarer disassortative mixing is a bias in favor of connections between dissimilar nodes.\n",
    "\n",
    "--Wikipedia, [Assortative Mixing](https://en.wikipedia.org/wiki/Assortative_mixing)\n",
    "\n",
    "This can be summarized via the assortativity coefficient.\n",
    "\n",
    "> The assortativity coefficient is the Pearson correlation coefficient of degree between pairs of linked nodes.\n",
    "\n",
    "--Wikipedia, [Assortativity](https://en.wikipedia.org/wiki/Assortativity)\n",
    "\n",
    "Pearson's is the covariance of two distributions divided by the product of their standard deviations. Assortativity adapts it for pairs of nodes while accounting for the degree of each node.\n",
    "\n",
    "<center><img src=\"images/pearsons_equation_no_cov.png\" width=\"500px\" alt=\"Equation for Pearson's Correlation Coefficient\" /></center>\n",
    "\n",
    "### Computing Degree Assortativity\n",
    "\n",
    "One type of assortativity is _degree assortativity_, which measures to what extent nodes connect to other nodes with similar degrees.\n",
    "\n",
    "It ranges from -1 to 1. We will use the NetworkX API [nx.algorithms.assortativity.degree_assortativity_coefficient](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.assortativity.degree_assortativity_coefficient.html#networkx.algorithms.assortativity.degree_assortativity_coefficient) to compute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be842af-e4a5-4f13-ad68-9c2f45c5334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_assort = nx.degree_assortativity_coefficient(G)\n",
    "degree_assort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c31f4ec-b718-40fb-95de-776a9dad3f11",
   "metadata": {},
   "source": [
    "The value for degree assortativity we computed above of 0.00172 indicates that overall nodes often associate with similar or dissimilar nodes by degree in roughly equal proportions. There is no correlation between the degree of nodes and their tendency to connect.\n",
    "\n",
    "> In a citation network, a higher assortativity can mean that papers with many citations (high degree nodes) tend to cite other papers with many citations. This could reflect a phenomenon where \"popular\" or foundational papers are often cited together. A lower assortativity might suggest that highly-cited papers are citing less-cited papers, indicating a wider __dispersion__ of references and potentially more novel research that builds on less well-known work.\n",
    "\n",
    "--ChatGPT4 and Russell Jurney"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26633034-e588-4d92-bf2c-9cbcf6f2eb31",
   "metadata": {},
   "source": [
    "### In-Degree Assortativity\n",
    "\n",
    "Let's check how often papers connect to similar papers according to how many in-bound citations they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25511f7-f44f-4b57-ac81-980db37397e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.degree_assortativity_coefficient(G, x=\"in\", y=\"in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8333e4f7-f2dc-4f23-835d-cb8fda369153",
   "metadata": {},
   "source": [
    "You can see there is a higher positive correlation, but only around 0.1, indicating papers display a small amount assortative mixing with respect to the number of citations they receive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56917d7a-d17c-4041-8542-ce6f101158ec",
   "metadata": {},
   "source": [
    "### Out-Degree Assortativity\n",
    "\n",
    "And... how often do papers that cite many papers cite papers that themselves cite many papers? Conversely, how often do papers that cite only a few papers cite other papers that cite only a few papers? If out-degree assortativity is high, it might be interesting to incorporate this last question into a connected components analysis, since there might be interesting components of low citation papers all on their own, encoding their own areas of science :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f95238-e8bc-4151-9d12-a8bb550c1a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.degree_assortativity_coefficient(G, x=\"out\", y=\"out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80b46f-1f06-4151-9d08-40eadf9fe42b",
   "metadata": {},
   "source": [
    "More of the same. Let's try answering the following questions:\n",
    "\n",
    "1) Is there assortative mixing between papers that are often cited and papers that themselves cite many papers?\n",
    "2) Is there assortative mixing between papers that cite many papers and papers that are often cited?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd49da65-4e27-4e7a-85cb-fcab03f807a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) x=\"in\" [cited a lot], x=\"out\" [cite a lot]\n",
    "print(f'In / Out Degree Assortativity Coefficient: {nx.degree_assortativity_coefficient(G, x=\"in\", y=\"out\"):,.3f}')\n",
    "\n",
    "# 2) x=\"out\" [cite a lot], x=\"in\" [cited a lot]\n",
    "print(f'Out / In Degree Assortativity Coefficient: {nx.degree_assortativity_coefficient(G, x=\"out\", y=\"in\"):,.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee0e94a-0490-41a2-9795-e35d1cd69c30",
   "metadata": {},
   "source": [
    "Ok, we give up! While I hope you understand this tool, it isn't very illuminating here :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4451e20-5e22-454b-96a5-4b25f49463bf",
   "metadata": {},
   "source": [
    "## Dispersion\n",
    "\n",
    "In interpreting our degree assortativity, ChatGPT4 mentioned _dispersion_, another network metric that indicates how well the neighbors of two nodes are connected to one anoter. Recall that _clustering coefficient_ indicates how well the neighbors of two nodes are _directly connected_. Dispersion is a similar measure that includes paths with lengths longer than 1.\n",
    "\n",
    "> Let's consider a simple example with nodes A and B. Suppose A and B have a set of mutual contacts or neighbors. If these mutual contacts are also connected to each other, forming a tightly-knit community, the dispersion is low. However, if these mutual contacts aren't connected to each other, the dispersion is high.\n",
    ">\n",
    "> In a social network setting, a high dispersion between two individuals might mean that they connect different parts of a network, which can be an indication of a close or important relationship. For example, in a group of friends, a romantic couple might have a high dispersion because each person connects the other to a different social group, and their mutual friends don't necessarily know each other.\n",
    "\n",
    "--ChatGPT4\n",
    "\n",
    "Below we use the [nx.algorithms.centrality.dispersion](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.dispersion.html) to compute the mean dispersion between the paper [M Theory As A Matrix Model: A Conjecture, Banks et al, 1997](https://arxiv.org/abs/hep-th/9610043) and its 1,219 citation edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8891c117-c2cb-4eea-8e1c-c6fd070f6ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dispersion = np.mean(list(nx.dispersion(G, \"9610043\").values()))\n",
    "print(f\"Mean Dispersion: {mean_dispersion:,.3f}\")\n",
    "\n",
    "median_dispersion = np.median(list(nx.dispersion(G, \"9610043\").values()))\n",
    "print(f\"Median Dispersion: {median_dispersion:,.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e2d67e-42ed-42ec-8483-47c482f72134",
   "metadata": {},
   "source": [
    "### Interpreting Dispersion = Hard\n",
    "\n",
    "Dispersion scores are relative to an individual network's context. We can't interpret the mean and median dispersion scores without putting them into the contxt of the other nodes' dispersions. Let's get the maximium dispersion and plot a histogram of mean dispersions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be7ef1-7576-4dd4-ab26-438fff0bc117",
   "metadata": {},
   "outputs": [],
   "source": [
    "dispersions = nx.dispersion(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc2466-4fd0-4de8-acef-505150aa4dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum dispersion using a nested list comprehension\n",
    "max_dispersion = np.max(np.concatenate([list(inner_dict.values()) for inner_dict in dispersions.values()]))\n",
    "\n",
    "print(f\"Maximum Dispersion: {max_dispersion:,.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a29c24-11f5-4d1d-8068-22c4b4d16b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dispersions = [np.mean(list(dispersion_dict.values())) for dispersion_dict in dispersions.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4690b98-e6b7-4b18-9b9d-ad8a44a01c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seaborn to create the histogram\n",
    "sns.histplot(mean_dispersions, kde=True, bins=30, log_scale=False)\n",
    "plt.title('Histogram of Node Mean Dispersions')\n",
    "plt.xlabel('Node Mean Dispersion Score')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e3ff5-d4d4-4514-8bf0-75430ff8c953",
   "metadata": {},
   "source": [
    "### Let's try that in log scale..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5dceaf-9ee0-4093-abbb-8fb1965b91bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out dispersion scores of 0\n",
    "logable_mean_dispersions = [x + 0.01 for x in mean_dispersions if x > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416d9e25-58d3-490b-b236-8c215fa25d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seaborn to create the histogram\n",
    "sns.histplot(logable_mean_dispersions, kde=True, bins=30, log_scale=True)\n",
    "plt.title('Histogram of Node Mean Dispersions')\n",
    "plt.xlabel('Node Mean Dispersion Score')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca8295c-ca78-4913-b190-857bbe2fbbb2",
   "metadata": {},
   "source": [
    "### Interpreting Node Mean Dispersions\n",
    "\n",
    "Nodes and their connections have a [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution) of dispersion scores. For nodes that are connected, most dispersion scores range from 0.1 to 1.\n",
    "\n",
    "While this provides some additional detail, it isn't that enlightening in this case. We could drill-down to papers with a very high dispersion score to find network bridges, but this is left as an exercise to the student. In a citation graph a network bridge might link on area of science to another, something that few other papers manage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67e2e10-83e6-44c9-8a70-9baa2aa93b06",
   "metadata": {},
   "source": [
    "# Fitting a Random Graph Model to our Network\n",
    "\n",
    "These degree histograms don't exist in a vacuum. They can be compared to theoretical models of networks to help interpret them. We mean to learn about our network, not simply compute metrics.\n",
    "\n",
    "NetworkX has many random graph models in the form of [Graph Generators](https://networkx.org/documentation/stable/reference/generators.html) that can generate instances of a random graph. For small graphs, it is neccessary to iteratively generate and measure multiple instances of random graphs. For larger graphs a single instance can work. This is due to the \n",
    "\n",
    "## Erdos Reyni Graphs\n",
    "\n",
    "We begin with a random graph model often used as a baseline, not because it fits real-world networks.\n",
    "\n",
    "> An Erdős–Rényi (ER) graph is a simple model of a random graph. This model is named after Paul Erdős and Alfréd Rényi, who first introduced one version of it.\n",
    ">\n",
    "> In the ER model, a graph is constructed by connecting nodes randomly. Each edge is included in the graph with probability p, independent of the other edges. Thus, the model has two parameters: the number of nodes n, and the edge probability p.\n",
    ">\n",
    "> The ER model can be used to generate either a G(n, M) graph or a G(n, p) graph.\n",
    ">\n",
    "> In the G(n, M) model, a graph is chosen uniformly at random from the collection of all graphs with n nodes and M edges.\n",
    "> In the G(n, p) model, a graph is constructed by connecting nodes randomly where each possible edge occurs independently with probability p.\n",
    "> The ER model is a simple model that does not capture many of the properties of real-world networks, such as their community structure and degree distribution. However, it is useful in network theory because of its analytical tractability and because it serves as a null model against which the properties of real-world networks can be compared.\n",
    "\n",
    "-- ChatGPT, verified by Russell Jurney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cfb609-c7be-4bea-9974-fe49d4e694cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the edge probability parameter for the Erdos-Renyi model\n",
    "p = nx.density(G)\n",
    "print(f\"Citation network density: {p:.5f}\")\n",
    "\n",
    "# Create an Erdos-Renyi graph with the same number of nodes and the calculated edge probability\n",
    "G_er = nx.erdos_renyi_graph(G.number_of_nodes(), p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd968ef-7934-4cd9-9c2e-5619910a8599",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_degrees(G, G_compare=None, compare_label=None):\n",
    "    \"\"\"Compute the degrees of a graph and compare them to a random graph model.\"\"\"\n",
    "\n",
    "    # Create histogram of degrees in real citation graph\n",
    "    degree_sequence = [d + 1 for n, d in G.degree()]\n",
    "    ax = sns.histplot(degree_sequence, bins=20, kde=True, log_scale=True, label=\"Citation Graph\")\n",
    "    ax.set(xlabel='Degree')\n",
    "\n",
    "    if G_compare:\n",
    "        # Create histogram of degree and plot alongside\n",
    "        compare_degree_sequence = [d + 1 for n, d in G_compare.degree()]\n",
    "        ax = sns.histplot(compare_degree_sequence, bins=20, kde=True, log_scale=True, label=compare_label if compare_label else \"Random Graph Model\")\n",
    "        ax.set(xlabel='Degree')\n",
    "    \n",
    "        plt.title(f\"Degree Histogram Comparison\")\n",
    "    else:\n",
    "        plt.title(\"Degree Histogram\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57637f48-1b22-427f-8872-bd4a881b8da6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare our graph with a random graph model\n",
    "plot_degrees(G, G_er, compare_label=\"Erdos Reyni\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a95e8f-281c-4fb5-914d-d99b9d0345d1",
   "metadata": {},
   "source": [
    "While narrower, this isn't completely dissimilar... and yet citation graphs are not known to look like random graph models. Let's check another. The [networkX Graph Generators documentation](https://networkx.org/documentation/stable/reference/generators.html) has a long list.\n",
    "\n",
    "### Barabasi-Albert Model\n",
    "\n",
    "> Barabási-Albert Model (Preferential Attachment Model, Scale Free Network): This model generates a random graph where nodes are added to the network one at a time, and each new node attaches to existing nodes with a probability proportional to their degree.\n",
    "\n",
    "A scale free network is one where its degree distribution fits a power law. In log scale, they proceed from top-left to bottom-right. Barabasi-Albert networks model step-wise growth of networks where edges are added in a way that prefers nodes with a higher degree.\n",
    "\n",
    "--ChatGPT, extended by Russell Jurney\n",
    "\n",
    "<center><img alt=\"Random network vs a Scale Free network\" src=\"images/random-vs-scale-free-network.jpg\" /></center>\n",
    "\n",
    "--Network-based approaches for anticancer therapy (Review), Seo et al, 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a542baf-97c5-4a74-8eaf-b20ae98a71e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of iterations required (This should be smaller than the total number of nodes)\n",
    "n = G.number_of_nodes()\n",
    "\n",
    "# Number of edges to attach from a new node to existing nodes\n",
    "m = G.number_of_edges() // n\n",
    "\n",
    "print(f\"n: {n:,} m: {m:,}\")\n",
    "\n",
    "# Create a Barabasi-Albert graph with the same number of nodes and edges as the real graph\n",
    "G_ba = nx.barabasi_albert_graph(n, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcba7ff-0446-47c0-ae55-e9ae153df9a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Total nodes: {G_ba.number_of_nodes():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0531abfe-0364-487e-bd5f-b3d9b9c9d285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_degrees(G, G_ba, compare_label=\"Barabasi-Albert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19adacbf-93d7-49fd-9daa-20b17e447040",
   "metadata": {},
   "source": [
    "### Price's Citation Graph Random Model\n",
    "\n",
    "Neither one of these fits the citation network very well. Some searching in the network science literature brings up this: Citation Graph Model. Wikipedia describes the first such model, Price's model as:\n",
    "\n",
    "> Price's model (named after the physicist Derek J. de Solla Price) is a mathematical model for the growth of citation networks.[1][2] It was the first model which generalized the Simon model[3] to be used for networks, especially for growing networks. Price's model belongs to the broader class of network growing models (together with the Barabási–Albert model) whose primary target is to explain the origination of networks with strongly skewed degree distributions. The model picked up the ideas of the Simon model reflecting the concept of rich get richer, also known as the Matthew effect. Price took the example of a network of citations between scientific papers and expressed its properties. His idea was that the way an old vertex (existing paper) gets new edges (new citations) should be proportional to the number of existing edges (existing citations) the vertex already has. This was referred to as cumulative advantage, now also known as preferential attachment. Price's work is also significant in providing the first known example of a scale-free network (although this term was introduced later). His ideas were used to describe many real-world networks such as the Web.\n",
    "\n",
    "--ChatGPT4 and Russell Jurney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2692c8-3d9a-43d3-951c-e19ac910f337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def citation_graph(n, m):\n",
    "    \"\"\"\n",
    "    Create a citation graph with n nodes. Each new node attaches to m existing nodes.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(range(2 * m))  # Start with 2m nodes\n",
    "    G.add_edges_from((i, 2 * m) for i in range(2 * m))  # Each of the first 2m nodes links to the node 2m\n",
    "\n",
    "    for i in range(2 * m + 1, n):\n",
    "        # A new paper cites m existing papers. The probability of citing an existing paper is proportional to its in-degree.\n",
    "        nodes = list(G.nodes())\n",
    "        probabilities = [G.in_degree(n) for n in nodes]\n",
    "        cited_nodes = random.choices(nodes, probabilities, k=m)\n",
    "        for node in cited_nodes:\n",
    "            G.add_edge(node, i)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb2ba0-13a8-4f4d-8e9d-9287546ee48a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = G.number_of_nodes()\n",
    "m = (G.number_of_edges() // n)  # integer division\n",
    "\n",
    "# Note: this can take a while\n",
    "G_cite = citation_graph(n, m)\n",
    "\n",
    "plot_degrees(G, G_cite, \"Price's Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b044e-33dc-4237-a5ff-cec4261b0885",
   "metadata": {},
   "source": [
    "### Random Graph Model Lessons\n",
    "\n",
    "We didn't fine the perfect random graph model using random graphs, but I hope these help you understand how to research what models might characterize your graph and what that means about it. As a data scientist, this is a valuable tool as it gives you the context you neex to make inferences about patterns in the network that can result in insight and clues for effective automation via machine learning. We could take this further and tune the parameters in Price's model or look for more citation network models that might fit better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddfb1fa-aba4-4b0e-9e53-83514acdb8c1",
   "metadata": {},
   "source": [
    "## Configuration Models\n",
    "\n",
    "We've tried generating random graph models - even those with fitted parameters - to our network. None of them fit. We can make one that definitely will fit the degree distribution because that is how it is generated - the [Configuration Model](https://en.wikipedia.org/wiki/Configuration_model#:~:text=In%20network%20science%2C%20the%20configuration,to%20incorporate%20arbitrary%20degree%20distributions.).\n",
    "\n",
    "<center><img src=\"images/degree_sequence_and_different_realizations_in_the_configuration_model.jpg\" width=\"600px\" /></center>\n",
    "<p>Figure 1. Degree sequence and different network realizations in the configuration model[1], Wikipedia, [Configuration Model](https://en.wikipedia.org/wiki/Configuration_model#:~:text=In%20network%20science%2C%20the%20configuration,to%20incorporate%20arbitrary%20degree%20distributions.)</p>\n",
    "\n",
    "We can use a configuration model or one of its more powerful extensions to generate random graphs that match the degree sequence - the degree histogram - of our network. This is a powerful technique as these networks can serve as a [random graph null model](https://en.wikipedia.org/wiki/Null_model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd0e14-f572-4981-969f-1912c79cf7bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming that G is your existing graph\n",
    "G_rand = nx.erdos_renyi_graph(20, 0.2)\n",
    "\n",
    "# Get the degree sequence of G\n",
    "degree_sequence = [d for n, d in G_rand.degree()]\n",
    "\n",
    "# Generate and draw 3 random graphs with the same degree sequence\n",
    "for i in range(3):\n",
    "    # Create a random graph from the degree sequence\n",
    "    H = nx.configuration_model(degree_sequence)\n",
    "\n",
    "    # In case of self-loops or parallel edges, \n",
    "    # we might want to create a simple graph with no self-loops or parallel edges.\n",
    "    H = nx.Graph(H)\n",
    "    H.remove_edges_from(nx.selfloop_edges(H))\n",
    "    \n",
    "    print(f\"Random Graph {i}\\n\")\n",
    "    describe_graph(H)\n",
    "    print()\n",
    "    \n",
    "    plt.figure()\n",
    "    nx.draw(H, with_labels=True)\n",
    "    plt.title(f'Random Graph {i+1}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d40f009-dcab-46c9-978b-2d3ff5068148",
   "metadata": {},
   "source": [
    "### Fitting Configuration Models to our Network\n",
    "\n",
    "Let's try a variety of configuration models of different types and see how their properties compare to our citation network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d415dd86-2c49-4f0d-a702-0b8f8e954d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = nx.Graph(nx.configuration_model([d for n, d in G.degree()]))\n",
    "\n",
    "print(\"Original graph G:\")\n",
    "describe_graph(G)\n",
    "\n",
    "print(\"\\nConfiguration model H:\")\n",
    "describe_graph(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c443b-4743-4751-8598-1520ed471ebf",
   "metadata": {},
   "source": [
    "### Moving On...\n",
    "\n",
    "You can see this still doesn't quite fit our model. We're going to examine a few of these networks using Grapistry to see how they differ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54a308-7259-4ee3-8d4d-bd0a2bf86a78",
   "metadata": {},
   "source": [
    "## Centrality Metrics\n",
    "\n",
    "Question: What are the most important papers in the field of physics during this period, from 1992 to 2003?\n",
    "Metrics: Network centralities - metrics designed to determine the prominence of a node in its local neighborhood or the whole network.\n",
    "\n",
    "You can find a number of centrality metrics under the [documentation for centrality](https://networkx.org/documentation/stable/reference/algorithms/centrality.html) in networkx.\n",
    "\n",
    "We are going to compute several centralities and add them to our `networkx.DiGraph` as properties. Then we can visualize them to understand what they mean.\n",
    "\n",
    "### Local Centralities\n",
    "\n",
    "- Degree Centrality: the number of connections a node has. Variants: in-degree centrality and out-degree centrality, which count the number of in-bound and out-bound connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4996f743-f267-4b7c-89cc-3648075dd314",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_degrees = nx.degree_centrality(G)\n",
    "\n",
    "for node, relative_degree in relative_degrees.items():\n",
    "    G.nodes[node][\"relative_degree\"] = relative_degree\n",
    "    G.nodes[node][\"relative_degree_log10\"] = np.log10(relative_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d5e46f-9ba3-4258-8f83-6cbe9bbb3846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "G.nodes[\"9711194\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff32a2a-7c8f-4b71-8ba7-1766c7138396",
   "metadata": {},
   "source": [
    "### Global Centralities\n",
    "\n",
    "- Eigenvector Centrality\n",
    "- Closeness Centrality\n",
    "- Betweenness Centrality\n",
    "\n",
    "#### So Many Centralities\n",
    "\n",
    "Check out the networkx [Centrality Algorithms documentation](https://networkx.org/documentation/stable/reference/algorithms/centrality.html) for a long list of other centralities. For a (not comprehnsive review) check out [Centrality Measures in Complex Networks: A Survey](https://arxiv.org/abs/2011.07190).\n",
    "\n",
    "#### Eigenvector Centrality\n",
    "\n",
    ">This is a measure of the influence of a node in a network. It assigns relative scores to all nodes in the network based on the concept that connections to high-scoring nodes contribute more to the score of the node in question than equal connections to low-scoring nodes.\n",
    "\n",
    "--ChatGPT4 and Russell Jurney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ba75d-b901-4420-bee0-2ba44c2b0b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eigenvectors = nx.eigenvector_centrality_numpy(G)\n",
    "\n",
    "for node, score in eigenvectors.items():\n",
    "    G.nodes[node][\"eigenvector\"] = score\n",
    "    G.nodes[node][\"eigenvector_log10\"] = np.log10(score + 0.000001)  # can't be zero for log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60c9c36-623b-41ef-86cc-491aecb9be9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Closeness Centrality\n",
    "\n",
    ">This measure calculates the average length of the shortest paths from a node to all other nodes in the network. Nodes with lower average shortest path lengths have higher closeness centrality and are often considered as being more central. A common interpretation of closeness centrality is that nodes with a high score are more likely to recieve information from other sources in the network than nodes with a low score.\n",
    "\n",
    "--ChatGPT4 and Russell Jurney\n",
    "\n",
    "Be aware that closeness and betweenness centrality are expensive to compute. They may not be available on large networks, or very densely connected networks. You can use NVIDIA RAPIDS [cuGraph centrality methods] if you have an NVIDIA GPU. Most data science work on GPUs is done on NVIDIA compatible GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4973663a-d4d6-402b-ba04-f91bf198c8dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "closeness = nx.closeness_centrality(G)\n",
    "\n",
    "for node, score in closeness.items():\n",
    "    G.nodes[node][\"closeness\"] = score\n",
    "    G.nodes[node][\"closeness_log10\"] = np.log10(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b754a4f-b291-4579-87bb-d89279d9291d",
   "metadata": {},
   "source": [
    "#### Betweenness Centrality\n",
    "\n",
    "> This measure considers a node's position within the network in terms of the paths between other nodes. Nodes that frequently lie on the shortest paths between other nodes in the network have a high betweenness centrality. These nodes serve as important points of control and coordination within the network.\n",
    "\n",
    "Betweenness centrality is so expensive it quickly becomes infeasible on large networks. GPUs can extend this capability in libraries like [NVIDIA RAPIDS cuGraph's](https://github.com/rapidsai/cugraph) [cugraph.betweenness_centrality](https://docs.rapids.ai/api/cugraph/legacy/api_docs/api/cugraph.betweenness_centrality.html) feature.\n",
    "\n",
    "--ChatGPT4 and Russell Jurney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2cfaae-a6f9-49d6-a6a8-9512850171e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Note: this is too slow to actually run on CPU! We need to use cuGraph on a GPU.\n",
    "# between = nx.betweenness_centrality(G)\n",
    "\n",
    "# for node, score in between.items():\n",
    "#     G.nodes[node][\"betweenness\"] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a6cef-95d4-4df4-8c28-db4b9e4371cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "G.nodes[\"9711194\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f191e5-65c7-448c-9a03-399d7b5acc5c",
   "metadata": {},
   "source": [
    "### Visualizing Centralities in Graphistry\n",
    "\n",
    "Let's zoom in on one section of the network to make it manageable and then look at just those nodes. Nodes are sized by degree by default in Graphistry. Let's click on one of the largest nodes and perform a snowball sample two hops out.\n",
    "\n",
    "# Sampling Networks\n",
    "\n",
    "> Large-scale networks, seen in contexts like social media or bioinformatics, bring forth considerable computational challenges due to their size and complexity. Traditional algorithms and infrastructures often struggle to efficiently compute global network measures, visualize data, and manage inherent noise in these large datasets. Network sampling, which involves selecting a representative subset of the network, offers a way to alleviate these issues. It enables feasible computation and interpretation of network properties. However, creating effective sampling strategies is crucial to maintain the original network's key characteristics, ensuring accurate inferences about the larger system.\n",
    "\n",
    "There are three major types of sampling a network: random sampling, snowball sampling and random walk sampling.\n",
    "\n",
    "- Random sampling selects random nodes from the network and includes edges between those nodes that are in the sample. This makes sense when your graph fits a random graph model like Erdos Reyni.\n",
    "- Snowball sampling creates an \"ego network\" which is centered on a single node of particular interest. It is useful when you want to investigate a region of a network. An ego (a node) may be of particular interest in the context of its alters (its neighbors) or a cluster or region of the graph may require your attention. Snowball sampling can \"zoom in\" to assist with visualization, which we will use below.\n",
    "\n",
    "## Snowball Sampling\n",
    "\n",
    "> In the context of network science, a snowball sample is a method of data collection that uses a cascading or \"snowball\" effect to discover and analyze nodes in a network. It's often used when you don't have access to the entire network data, or when the network is too large to feasibly analyze in its entirety.\n",
    ">\n",
    "> The process begins with a small set of \"seed\" nodes in the network. You identify all nodes that these seed nodes are directly connected to (i.e., their neighbors), and add them to your sample. This is often referred to as one \"hop\" or \"wave\" out from the seed nodes.\n",
    ">\n",
    "> But the process doesn't stop there, much like a snowball rolling down a hill and growing in size, you continue the process for these newly added nodes, finding all of their connected nodes, or neighbors, and adding them to your sample. This is the second hop, and you can continue this for as many \"hops\" as you want, each time expanding the number of nodes in your sample.\n",
    ">\n",
    "> When we say \"1.5 hop\" in the context of a snowball sampling of a network, it typically means that we start from a certain node (or set of nodes), then we include all the direct neighbors of this node (that's the first \"hop\"), and then for the \"0.5\" hop, we also include nodes that are connected to the nodes in the first hop but not include their connections (i.e., not going another full hop further). The idea behind this is to gather a slightly wider view of the network around our starting point, but without expanding to the full 2-hop neighborhood, which could significantly increase the size of the sample.\n",
    "\n",
    "--ChatGPT and Russell Jurney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22838d-77be-4e7a-8f87-6f7c1f8753cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def snowball_sample(graph, start_nodes, hops: int):\n",
    "    \"\"\"\n",
    "    Perform an n-hop snowball sample on the given graph.\n",
    "    \n",
    "    :param graph: The NetworkX graph.\n",
    "    :param start_nodes: The nodes to start the snowball sampling from.\n",
    "    :param n: The number of hops. Connections within the nodes are automatically included.\n",
    "    :return: A subgraph of the given graph that is the result of the n-hop snowball sampling.\n",
    "    \"\"\"\n",
    "    # First add all start_nodes to the sample (0 hops)\n",
    "    sampled_graph = graph.subgraph(start_nodes).copy()\n",
    "\n",
    "    # Then add all their neighbors (1 hop)\n",
    "    for i in range(0, n):\n",
    "        for node in start_nodes:\n",
    "            sampled_graph.add_nodes_from(graph.neighbors(node))\n",
    "            sampled_graph.add_edges_from((node, neighbor) for neighbor in graph.neighbors(node))\n",
    "\n",
    "    # Finally, add edges between the neighbors (.5 hops, as in a 1.5 hop snowball sample)\n",
    "    # We iterate over all nodes in the sampled graph and, for each, add edges to its neighbors that are also in the sampled graph\n",
    "    for node in list(sampled_graph.nodes):\n",
    "        neighbors = set(graph.neighbors(node))\n",
    "        sampled_neighbors = neighbors.intersection(set(sampled_graph.nodes))\n",
    "        sampled_graph.add_edges_from((node, sampled_neighbor) for sampled_neighbor in sampled_neighbors)\n",
    "\n",
    "    return sampled_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f775309-f329-4f3f-a10c-cf49b87ed245",
   "metadata": {},
   "source": [
    "## Snowball Sampling an Influential Paper\n",
    "\n",
    "The 1997 paper [M Theory As A Matrix Model: A Conjecture, Banks et al, 1997](https://arxiv.org/abs/hep-th/9610043) has a degree of 1,219. It is a highly influential paper and has now received 2,710. Let's start with it and do a snowball sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0bc483-a8a2-4394-a96b-17301352515d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "G.nodes[\"9610043\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432aeb28-eda2-4627-91d4-735669259407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This paper is very well cited\n",
    "G_snowball = snowball_sample(G, [\"9610043\"], 1)\n",
    "G_snowball.number_of_nodes(), G_snowball.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a69adf8-cc98-4539-90fb-308529691ec6",
   "metadata": {
    "tags": []
   },
   "source": [
    "We are going to load the snowball sample of `M Theory As A Matrix Model: A Conjecture` we created and investigate what the metrics we previously computed mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b13947-d406-4b21-a268-cf90649143b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sampling Real World Networks\n",
    "\n",
    "Network algorithms can be difficult to scale because comparing every node to every other node has n^2 complexity.\n",
    "\n",
    "<br /><center><img src=\"images/github_adjacency_rjurney.webp\" width=\"600px\" /></center><br />\n",
    "\n",
    "Now that we are familiar with random graph models, we can fit one to our network using the metrics above to know which\n",
    "\n",
    "There is an excellent network sampling library for Python that extends NetworkX called [littleballoffur](https://github.com/benedekrozemberczki/littleballoffur) [[paper](https://arxiv.org/abs/2006.04311), [docs](https://little-ball-of-fur.readthedocs.io/en/latest/)]. \n",
    "\n",
    "## Sampling and Network Semantics\n",
    "\n",
    "Our network is a citation graph, which means it operates forward in time and is a Directed-Acyclic-Graph (DAG). These facts are importanti n choosing a sampling algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad46b7a-1d29-41a1-b514-3c8642f1b80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "154d41e3-0d42-404b-ab5b-d70db5ba438d",
   "metadata": {},
   "source": [
    "## Beware of Supernodes!\n",
    "\n",
    "Supernodes are a DEAL STOPPER. What? To rephrase: a common obstacle in implmenting graph algorithms, using common tools or even simple graph processing is when on node has LOTS of connections and processing all of them would take until the end of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bb9185-bb80-44cf-a9f3-77644c84ba69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
