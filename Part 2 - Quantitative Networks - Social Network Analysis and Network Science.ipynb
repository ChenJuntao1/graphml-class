{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff047e8-e1c6-4429-92c4-945f040e1691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First install takes a while to download models\n",
    "!pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f26d5a-daad-493d-aeb6-7e312ee2472d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "from datetime import date\n",
    "\n",
    "import dateutil\n",
    "import graphistry\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27ed23-fd02-430d-ac15-d93430438fb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Environment variable setup\n",
    "GRAPHISTRY_USERNAME = os.getenv(\"GRAPHISTRY_USERNAME\")\n",
    "GRAPHISTRY_PASSWORD = os.getenv(\"GRAPHISTRY_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c73178-317f-4bda-953e-74597e4b6f6c",
   "metadata": {},
   "source": [
    "# Part 2: Quantitative Networks: Social Network Analysis and Network Science\n",
    "\n",
    "Having reviewed the theory of network science and graph machine learning in Part 1, it is time to start coding! In this part of the course we will survey network science. Please keep in mind that as a multidisciplinary field, network science is impossible to cover completely.\n",
    "\n",
    "Albert-László Barabási has written a [comprehensive book on network science](http://networksciencebook.com/chapter/2) that provides more in depth treatment of topics like [graph theory](http://networksciencebook.com/chapter/2), [random networks](http://networksciencebook.com/chapter/3) and [communities](http://networksciencebook.com/chapter/9).\n",
    "\n",
    "## High-energy Physics Theory Citation Network\n",
    "\n",
    "We are going to be using the [High-energy physics theory citation network](https://snap.stanford.edu/data/cit-HepTh.html) from [Stanford SNAP](https://snap.stanford.edu/index.html) which has many large network datasets available in the [Stanford Large Network Dataset Collection](https://snap.stanford.edu/data/).\n",
    "\n",
    "The dataset includes:\n",
    "\n",
    "* [Citation graph edge list](https://snap.stanford.edu/data/cit-HepTh.txt.gz) contains node ID pairs. Node IDs are standard paper identifiers. This will build the core structure of our network.\n",
    "* [Paper metadata](cit-HepTh-abstracts.tar.gz) including abstracts. This will add propertis to our network.\n",
    "* [Publishing dates on arXiv](https://snap.stanford.edu/data/cit-HepTh-dates.txt.gz) will make our citation network a temporal [Directed-Acyclic-Graph (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph) since one paper can't cite another before it is written and there are no reciprocal edges. While we don't focus on this, it does affect our analysis.\n",
    "\n",
    "## Setting up Graphistry\n",
    "\n",
    "Throughout this part of the course we will be using `pygraphistry` and [Graphistry Hub](https://hub.graphistry.com/) [https://hub.graphistry.com/](https://hub.graphistry.com/) to visualize networks. Both are free for personal use and are powerful for visualizing networks large and small.\n",
    "\n",
    "You can [signup](https://hub.graphistry.com/accounts/signup/) for a Graphistry account at [https://hub.graphistry.com/accounts/signup/](https://hub.graphistry.com/accounts/signup/) with your Github or Google account. Retain and use the username and password in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96638e3d-bff3-48eb-916a-34b9b042a080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graphistry.register(\n",
    "    api=3,\n",
    "    username=GRAPHISTRY_USERNAME,\n",
    "    password=GRAPHISTRY_PASSWORD,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee9d6bf-d6f4-4104-9579-e11c4f844072",
   "metadata": {},
   "source": [
    "# Network Science on Citation Networks\n",
    "\n",
    "We start with an example of a citation network: [High-energy physics theory citation network](https://snap.stanford.edu/data/cit-HepTh.html) from [Stanford SNAP](http://snap.stanford.edu/). A citation network describes how one academic paper cites another one.\n",
    "\n",
    "> Arxiv HEP-TH (high energy physics theory) citation graph is from the e-print arXiv and covers all the citations within a dataset of 27,770 papers with 352,807 edges. If a paper i cites paper j, the graph contains a directed edge from i to j. If a paper cites, or is cited by, a paper outside the dataset, the graph does not contain any information about this.\n",
    ">\n",
    "> The data covers papers in the period from January 1993 to April 2003 (124 months). It begins within a few months of the inception of the arXiv, and thus represents essentially the complete history of its HEP-TH section.\n",
    ">\n",
    "> The data was originally released as a part of [2003 KDD Cup](http://www.cs.cornell.edu/projects/kddcup/).\n",
    "\n",
    "J. Leskovec, J. Kleinberg and C. Faloutsos. [Graphs over Time: Densification Laws, Shrinking Diameters and Possible Explanations.](http://www.cs.cmu.edu/~jure/pubs/powergrowth-kdd05.pdf) ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2005.\n",
    "\n",
    "## NetworkX\n",
    "\n",
    "The [networkx Algorithms documentation](https://networkx.org/documentation/stable/reference/algorithms/index.html) lists a fairly comprehensive set of tools available for the network scientist. We will be using networkx throughout the course.\n",
    "\n",
    "## cuGraph\n",
    "\n",
    "The [NVIDIA RAPIDS cuGraph](https://docs.rapids.ai/api/cugraph/stable/) ([Github](https://github.com/rapidsai/cugraph)) is a GPU accelerated graph analytics library that is roughly compatible with `networkx`. It can rapidly compute expensive metrics compare to CPU, where processing is often limited to a single core.\n",
    "\n",
    "## Extract Edge List and Build Directional Graph (DiGraph)\n",
    "\n",
    "First we download the edge list and build the structure of the network: `(paper)-cited->(paper)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b04e501-4648-46c5-85fd-64f954ff5837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import io\n",
    "import networkx as nx\n",
    "import tarfile\n",
    "\n",
    "\n",
    "# Initialize a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Download and load edges (citations) from `cit-HepTh.txt.gz`\n",
    "response = requests.get(\"https://snap.stanford.edu/data/cit-HepTh.txt.gz\")\n",
    "gzip_content = io.BytesIO(response.content)\n",
    "\n",
    "# Decompress the gzip content and build the edge list for our network\n",
    "with gzip.GzipFile(fileobj=gzip_content) as f:\n",
    "    for line in f:\n",
    "        line = line.decode('utf-8')\n",
    "        # Ignore lines that start with '#'\n",
    "        if not line.startswith('#'):\n",
    "            cited, citing = line.strip().split('\\t')\n",
    "            G.add_edge(citing, cited)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc0c296-d40d-4f99-bf12-f29d5a624e8e",
   "metadata": {},
   "source": [
    "## Summarize the Properties of our DiGraph\n",
    "\n",
    "Let's check how many nodes and edges we have. This will help evaluate how we are doing when we parse the abstracts to add properties to our DiGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c603f-d26b-4b6c-905d-bab3b46638f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Total nodes: {G.number_of_nodes():,}\")\n",
    "print(f\"Total edges: {G.number_of_edges():,}\")\n",
    "print(f\"Total components: {nx.number_connected_components(G.to_undirected()):,}\")\n",
    "print(f\"Undirected is weakly connected: {nx.is_weakly_connected(G)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c861508c-20a0-463b-aff4-6c824db0a5c1",
   "metadata": {},
   "source": [
    "### Histogram of Connected Component Sizes\n",
    "\n",
    "134 components - our graph is not one large hairball! Let's see what sizes our connected components are. Usually these are a power law distribution, with one large component taking up 30 or more percent of nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85de86d-b403-469e-afed-c5752210dc9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get connected components and their sizes\n",
    "components = nx.connected_components(G.to_undirected())\n",
    "component_sizes = [len(c) for c in components]\n",
    "\n",
    "# Use seaborn to create the histogram\n",
    "sns.histplot(component_sizes, kde=True, bins=30, log_scale=True)\n",
    "plt.title('Histogram of Connected Component Sizes')\n",
    "plt.xlabel('Component Size')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc66723-f701-42dd-8ee3-369c73c345d6",
   "metadata": {},
   "source": [
    "### Average Clustering Coefficient\n",
    "\n",
    ">The clustering coefficient is an indicator of the interconnectedness or cohesion among nodes in a graph. It's a measure of the extent to which the immediate neighbors of a particular node link to each other.\n",
    ">\n",
    ">The range of a clustering coefficient is between 0 and 1:\n",
    ">\n",
    ">When the clustering coefficient is 0, it signifies a scenario where none of the nodes have direct connections with each other. This can be seen in graphs such as a tree or a radial network where nodes are only connected to a central hub, but not with each other.\n",
    ">\n",
    ">In contrast, a clustering coefficient of 1 indicates a fully connected graph. Here, each node in the graph is directly linked with all other nodes.\n",
    "\n",
    "--ChatGPT4 and Russell Jurney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7bbb44-99c0-4b62-856e-7cb40c9fe82b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A good start...\n",
    "print(f\"Average degree: {G.number_of_edges()/G.number_of_nodes():.2f}\")\n",
    "\n",
    "clustering_coefficient = nx.average_clustering(G)\n",
    "print(f\"Graph clustering coefficient: {clustering_coefficient:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d90ed4-ab34-452f-9a7d-abd63ecd98ab",
   "metadata": {
    "tags": []
   },
   "source": [
    ">An average clustering coefficient of 0.16 in our `nx.DiGraph` implies there is a relatively low degree of clustering or tight-knit communities. It suggests a network structure that is more spread out, with fewer connections among immediate neighbors of each node. Networks such as web pages or citation networks often present lower clustering coefficients, as these networks are characterized by a wide range of diverse connections that do not necessarily link back to each other. Citation networks are actually temporal networks and are constrained by the fact that papers can't cite future papers, but only past ones. They are therefore DAGs - directed, acyclic graphs.\n",
    "\n",
    "--ChatGPT4 and Russell Jurney\n",
    "\n",
    "### Node Clustering Coefficients\n",
    "\n",
    ">The clustering coefficient of a node is a measure of the degree to which nodes in a graph tend to cluster together. It provides an indication of how the neighbors of a _particular node_ are interconnected. It measures how close a node's neighbors are to being a complete graph or a clique. It is defined as the fraction of possible triangles that exist through the node, or equivalently, as the fraction of the node's neighbors that are also neighbors of each other.\n",
    "\n",
    "--ChatGPT4 and Russell Jurney\n",
    "\n",
    "We're going to add a property to each node in the network for each node's clustering coefficient. You can find more on clustering coefficients in citation networks in [Modeling the clustering in citation networks](https://arxiv.org/abs/1104.4209)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb65251-2580-41d3-9297-c17c0c89be26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustering_coeffs = nx.clustering(G)\n",
    "\n",
    "for node, clustering_coeff in clustering_coeffs.items():\n",
    "    G.nodes[node]['clustering_coefficient'] = clustering_coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92790ec-d942-4db9-a001-46b253096fb3",
   "metadata": {},
   "source": [
    "Let's look at how the clustering coefficient is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e863c2-61aa-426b-ba8b-0d7cd965e5fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get connected components and their sizes\n",
    "clustering_coeff_counts = [val for key, val in clustering_coeffs.items()]\n",
    "\n",
    "# Use seaborn to create the histogram\n",
    "sns.histplot(clustering_coeff_counts, kde=True, bins=30, log_scale=False)\n",
    "plt.title('Histogram of Node Clustering Coefficients')\n",
    "plt.xlabel('Clustering Coefficient')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e2c1e-9a35-4282-bc02-2460fa11d6b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "G.nodes[\"9711194\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d90d7c-8ddb-453d-b0a4-2e6b01623cd8",
   "metadata": {},
   "source": [
    "This indicates that most nodes have relatively low (but still significant) clustering coefficients - their neighbors are sparsely connected. How does this fit with the average degree of 12 and graph clustering coefficient of 0.16? 16% of the average node's connections are connected with one another. From the histogram which has a skewed but fairly normal distribution, it looks like the _typical_ node has these properties as well. These are characteristics of a [Small World Network](https://en.wikipedia.org/wiki/Small-world_network). Most citation graphs are [small world networks](https://en.wikipedia.org/wiki/Citation_graph)\n",
    "\n",
    "Why do we care what kind of theoretical graph model fits our *real world* network. Because the field of network science is multidisciplinary and papers cover topics from physics to biology to sociology to microchip design. The type of network you have - often in the same settings as yours - has been rigorously studied. Reading a paper or two describing your graph gives you the context you need to extract insights, build models and auomate tasks using machine learning and artificial intelligence.\n",
    "\n",
    "## Add Properties to Nodes in Network\n",
    "\n",
    "Now we will use `extract_paper_info(text)` to add structured data to the nodes in our network.\n",
    "\n",
    "### Using ChatGPT to Write NetworkX Code\n",
    "\n",
    "We cover Chatbots at the end of this course, I just want to point out that the following dialogue generated the basis for the code below.\n",
    "\n",
    "```\n",
    "I am going to past some text representing some semi-structured data about an academic paper below:\n",
    "\n",
    "Paper: hep-th/0002031\n",
    "From: Maulik K. Parikh \n",
    "Date: Fri, 4 Feb 2000 17:04:51 GMT   (10kb)\n",
    "\n",
    "Title: Confinement and the AdS/CFT Correspondence\n",
    "Authors: D. S. Berman and Maulik K. Parikh\n",
    "Comments: 12 pages, 1 figure, RevTeX\n",
    "Report-no: SPIN-1999/25, UG-1999/42\n",
    "Journal-ref: Phys.Lett. B483 (2000) 271-276\n",
    "\\\\\n",
    "  We study the thermodynamics of the confined and unconfined phases of\n",
    "superconformal Yang-Mills in finite volume and at large N using the AdS/CFT\n",
    "correspondence. We discuss the necessary conditions for a smooth phase\n",
    "crossover and obtain an N-dependent curve for the phase boundary.\n",
    "\\\\\n",
    "\n",
    "Now I want you to write python code to extract the fields \"Paper\", \"From\", \"Date\", \"Title\", \"Authors\", \"Comments\", \"Report-no\", \"Journal-ref\" and the last field the \"Abstract\" text content.\n",
    "```\n",
    "\n",
    "One cycle of correction looked like:\n",
    "\n",
    "```\n",
    "That code did not work on this record:\n",
    "\n",
    "------------------------------------------------------------------------------\n",
    "\\\\\n",
    "Paper: hep-th/9711200\n",
    "From: Juan Maldacena <malda@physics.rutgers.edu>\n",
    "Date: Thu, 27 Nov 1997 23:53:13 GMT   (22kb)\n",
    "Date (revised v2): Mon, 8 Dec 1997 18:59:11 GMT   (23kb)\n",
    "Date (revised v3): Thu, 22 Jan 1998 15:42:41 GMT   (23kb)\n",
    "\n",
    "Title: The Large N Limit of Superconformal Field Theories and Supergravity\n",
    "Authors: Juan M. Maldacena\n",
    "Comments: 20 pages, harvmac, v2: section on AdS_2 corrected, references added,\n",
    "  v3: More references and a sign in eqns 2.8 and 2.9 corrected\n",
    "Report-no: HUTP-98/A097\n",
    "Journal-ref: Adv.Theor.Math.Phys. 2 (1998) 231-252; Int.J.Theor.Phys. 38 (1999)\n",
    "  1113-1133\n",
    "\\\\\n",
    "  We show that the large $N$ limit of certain conformal field theories in\n",
    "various dimensions include in their Hilbert space a sector describing\n",
    "supergravity on the product of Anti-deSitter spacetimes, spheres and other\n",
    "compact manifolds. This is shown by taking some branes in the full M/string\n",
    "theory and then taking a low energy limit where the field theory on the brane\n",
    "decouples from the bulk. We observe that, in this limit, we can still trust the\n",
    "near horizon geometry for large $N$. The enhanced supersymmetries of the near\n",
    "horizon geometry correspond to the extra supersymmetry generators present in\n",
    "the superconformal group (as opposed to just the super-Poincare group). The 't\n",
    "Hooft limit of 4-d ${\\cal N} =4$ super-Yang-Mills at the conformal point is\n",
    "shown to contain strings: they are IIB strings. We conjecture that\n",
    "compactifications of M/string theory on various Anti-deSitter spacetimes are\n",
    "dual to various conformal field theories. This leads to a new proposal for a\n",
    "definition of M-theory which could be extended to include five non-compact\n",
    "dimensions.\n",
    "\\\\\n",
    "\n",
    "It should have started the Abstract extraction with \"We show that\" but instead it includes the entire document. Please fix.\n",
    "```\n",
    "\n",
    "There were a few more iterations, so this is not \"free.\" I then had to edit the code and that is what is in the cell. ChatGPT is your running mate :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c2370b-2d51-4da3-9b2c-bf5962882c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_paper_info(record):\n",
    "    \"\"\"Extract structured information from the text of academic paper text records using regular expressions.\n",
    "       \n",
    "       Note: I was written wholly or in part by ChatGPT4 on May 23, 2023.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty dictionary to hold the information\n",
    "    info = {}\n",
    "    \n",
    "    # Match \"Paper\" field\n",
    "    paper_match = re.search(r\"Paper:\\s*(.*)\", record)\n",
    "    if paper_match:\n",
    "        info['Paper'] = paper_match.group(1)\n",
    "        \n",
    "    # # Match \"From\" field\n",
    "    # from_match = re.search(r\"From:\\s*(.*)\", record)\n",
    "    # if from_match:\n",
    "    #     info['From'] = from_match.group(1)\n",
    "\n",
    "    # Match \"From\" field\n",
    "    from_match = re.search(r\"From:\\s*([^<]*)<\", record)\n",
    "    if from_match:\n",
    "        info['From'] = from_match.group(1).strip()\n",
    "    \n",
    "    # Match \"Date\" field\n",
    "    date_match = re.search(r\"Date:\\s*(.*)\", record)\n",
    "    if date_match:\n",
    "        info['Date'] = date_match.group(1)\n",
    "    \n",
    "    # Match \"Title\" field\n",
    "    title_match = re.search(r\"Title:\\s*(.*)\", record)\n",
    "    if title_match:\n",
    "        info['Title'] = title_match.group(1)\n",
    "\n",
    "    # Match \"Authors\" field\n",
    "    authors_match = re.search(r\"Authors:\\s*(.*)\", record)\n",
    "    if authors_match:\n",
    "        info['Authors'] = authors_match.group(1)\n",
    "\n",
    "    # Match \"Comments\" field\n",
    "    comments_match = re.search(r\"Comments:\\s*(.*)\", record)\n",
    "    if comments_match:\n",
    "        info['Comments'] = comments_match.group(1)\n",
    "\n",
    "    # Match \"Report-no\" field\n",
    "    report_no_match = re.search(r\"Report-no:\\s*(.*)\", record)\n",
    "    if report_no_match:\n",
    "        info['Report-no'] = report_no_match.group(1)\n",
    "\n",
    "    # Match \"Journal-ref\" field\n",
    "    journal_ref_match = re.search(r\"Journal-ref:\\s*(.*)\", record)\n",
    "    if journal_ref_match:\n",
    "        info['Journal-ref'] = journal_ref_match.group(1)\n",
    "\n",
    "    # Extract \"Abstract\" field\n",
    "    abstract_pattern = r\"Journal-ref:[^\\\\\\\\]*\\\\\\\\[\\n\\s]*(.*?)(?=\\\\\\\\)\"\n",
    "    abstract_match = re.search(abstract_pattern, record, re.DOTALL)\n",
    "    if abstract_match:\n",
    "        abstract = abstract_match.group(1)\n",
    "        abstract = abstract.replace('\\n', ' ').replace('  ', ' ')\n",
    "        info['Abstract'] = abstract.strip()\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdffc9e7-1055-4772-bbda-4a9ae08a81e8",
   "metadata": {},
   "source": [
    "### Debugging Paper Metadata Parsing\n",
    "\n",
    "The first version didn't work, so I added some documents below. I'm leaving it in the notebook can see what I did: iteratively improve document parsing (data cleaning) until the network meets the needs of our analysis, application or audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f03d74-98fd-4540-aea4-fd687a88b02d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc1 = \"\"\"------------------------------------------------------------------------------\n",
    "\\\\\n",
    "Paper: hep-th/9612115\n",
    "From: Asato Tsuchiya <tsuchiya@theory.kek.jp>\n",
    "Date: Wed, 11 Dec 1996 17:38:56 +0900   (20kb)\n",
    "Date (revised): Tue, 31 Dec 1996 01:06:34 +0900\n",
    "\n",
    "Title: A Large-N Reduced Model as Superstring\n",
    "Authors: N. Ishibashi, H. Kawai, Y. Kitazawa and A. Tsuchiya\n",
    "Comments: 29 pages, Latex, a footnote and references added, eq.(3.52)\n",
    "  corrected, minor corrections\n",
    "Report-no: KEK-TH-503, TIT/HEP-357\n",
    "Journal-ref: Nucl.Phys. B498 (1997) 467-491\n",
    "\\\\\n",
    "  A matrix model which has the manifest ten-dimensional N=2 super Poincare\n",
    "invariance is proposed. Interactions between BPS-saturated states are analyzed\n",
    "to show that massless spectrum is the same as that of type IIB string theory.\n",
    "It is conjectured that the large-N reduced model of ten-dimensional super\n",
    "Yang-Mills theory can be regarded as a constructive definition of this model\n",
    "and therefore is equivalent to superstring theory.\n",
    "\\\\\n",
    "\"\"\"\n",
    "\n",
    "doc2 = \"\"\"------------------------------------------------------------------------------\n",
    "\\\\\n",
    "Paper: hep-th/9711029\n",
    "From: John Schwarz <jhs@theory.caltech.edu>\n",
    "Date: Wed, 5 Nov 1997 17:30:55 GMT   (20kb)\n",
    "Date (revised v2): Thu, 6 Nov 1997 23:52:45 GMT   (21kb)\n",
    "\n",
    "Title: The Status of String Theory\n",
    "Author: John H. Schwarz\n",
    "Comments: 16 pages, latex, two figures; minor corrections, references added\n",
    "Report-no: CALT-68-2140\n",
    "\\\\\n",
    "  There have been many remarkable developments in our understanding of\n",
    "superstring theory in the past few years, a period that has been described as\n",
    "``the second superstring revolution.'' Several of them are discussed here. The\n",
    "presentation is intended primarily for the benefit of nonexperts.\n",
    "\\\\\n",
    "\"\"\"\n",
    "\n",
    "paper_info = extract_paper_info(doc1)\n",
    "paper_id = paper_info.get(\"Paper\", \"\").split(\"/\")[-1]  # Get the paper ID part of the \"Paper\" field\n",
    "print(paper_id, paper_info)\n",
    "\n",
    "paper_info = extract_paper_info(doc2)\n",
    "paper_id = paper_info.get(\"Paper\", \"\").split(\"/\")[-1]  # Get the paper ID part of the \"Paper\" field\n",
    "print(paper_id, paper_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc75bfac-2f2e-4305-8d40-072e5f64aead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the abstracts from `cit-HepTh-abstracts.tar.gz`\n",
    "abstract_response = requests.get(\"https://snap.stanford.edu/data/cit-HepTh-abstracts.tar.gz\")\n",
    "\n",
    "# Convert the response content to an in-memory binary stream\n",
    "abstract_gzip_content = io.BytesIO(abstract_response.content)\n",
    "\n",
    "# Decompress the gzip content\n",
    "with gzip.GzipFile(fileobj=abstract_gzip_content) as f:\n",
    "    with tarfile.open(fileobj=f, mode='r|') as tar:\n",
    "        for member in tar:\n",
    "            abstract_file = tar.extractfile(member)\n",
    "            if abstract_file is not None:\n",
    "                content = abstract_file.read().decode('utf-8')\n",
    "                paper_info = extract_paper_info(content)\n",
    "                if paper_info:\n",
    "                    paper_id = paper_info.get(\"Paper\", \"\").split(\"/\")[-1]  # Get the paper ID part of the \"Paper\" field\n",
    "                    if paper_id in G:\n",
    "                        for field, value in paper_info.items():\n",
    "                            if paper_id in G:\n",
    "                                G.nodes[paper_id][field] = value\n",
    "                    else:\n",
    "                        # Add isolated nodes if paper_id isn't in G\n",
    "                        G.add_node(paper_id, **paper_info)\n",
    "\n",
    "# Now `G` is a property graph representing the \"High-energy physics theory citation network\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f853cc67-d820-4e5f-b18c-fd9093ad9bec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "G.nodes[\"9711194\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee404ce-3aad-43a5-9e33-0d52a65a509a",
   "metadata": {},
   "source": [
    "## Adding Publishing Dates to our Nodes\n",
    "\n",
    "SNAP also makes available the time the papers were published to arXiv, which can lead to some interesting analyses. Timestamps are available [here](https://snap.stanford.edu/data/cit-HepTh-dates.txt.gz) [https://snap.stanford.edu/data/cit-HepTh-dates.txt.gz](https://snap.stanford.edu/data/cit-HepTh-dates.txt.gz). Let's add them as a property of our nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e332b2ed-51a1-46de-9ed7-a8a1174c8947",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download and load edges (citations) from `cit-HepTh.txt.gz`\n",
    "response = requests.get(\"https://snap.stanford.edu/data/cit-HepTh-dates.txt.gz\")\n",
    "gzip_content = io.BytesIO(response.content)\n",
    "\n",
    "# Decompress the gzip content and add a \"published\" date property to our nodes\n",
    "with gzip.GzipFile(fileobj=gzip_content) as f:\n",
    "    for line in f:\n",
    "        line = line.decode('utf-8')\n",
    "        # Ignore lines that start with '#'\n",
    "        if not line.startswith('#'):\n",
    "            paper_id, iso_date = line.strip().split('\\t')\n",
    "            if paper_id in G:\n",
    "                G.nodes[paper_id][\"published\"] = date.fromisoformat(iso_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d213a1c5-742c-4dd3-ba31-8c71580fbabf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Describing our Citation Network `nx.DiGraph`\n",
    "\n",
    "Now we can check a few theoretical properties of our DiGraph. I'll frame it in terms of questions and metrics.\n",
    "\n",
    "* Domain question: How many citations does each paper have across the entire network?\n",
    "* Metric 1: What is the average degree, in-degree and out-degree of the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a76395-2689-4c56-b821-add6de854184",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract degree, in-degree, and out-degree for each node\n",
    "# +1 is to avoid problems with log-scale display\n",
    "degree_sequence = [d + 1 for n, d in G.degree()]\n",
    "in_degree_sequence = [d + 1 for n, d in G.in_degree()]\n",
    "out_degree_sequence = [d + 1 for n, d in G.out_degree()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd7308-16d6-469f-a296-7872845e8353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create histogram of degree\n",
    "ax = sns.histplot(degree_sequence, bins=20, kde=True, log_scale=True)\n",
    "ax.set(xlabel='Degree')\n",
    "plt.title('Degree Histogram')\n",
    "plt.show()\n",
    "\n",
    "# Create histogram of in-degree\n",
    "ax = sns.histplot(in_degree_sequence, bins=20, kde=True, log_scale=True)\n",
    "ax.set(xlabel='In-Degree')\n",
    "plt.title('In-Degree Histogram')\n",
    "plt.show()\n",
    "\n",
    "# Create histogram of out-degree\n",
    "ax = sns.histplot(out_degree_sequence, bins=20, kde=True, log_scale=True)\n",
    "ax.set(xlabel='Out-Degree')\n",
    "plt.title('Out-Degree Histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12408801-a70c-4fc0-aa03-9e4f16998d72",
   "metadata": {},
   "source": [
    "## Tactical Decision: Removing Uncited Papers\n",
    "\n",
    "Uncited papers aren't very interesting - they have degree of zero - so aren't part of any connected component of a network. Let's filter them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80321d8-337f-475d-9a95-225d8902cdd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Nodes before removal: {G.number_of_nodes():,}\")\n",
    "\n",
    "# Remove the isolated nodes\n",
    "G.remove_nodes_from(list(nx.isolates(G)))\n",
    "\n",
    "print(f\"Nodes after removal: {G.number_of_nodes():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67e2e10-83e6-44c9-8a70-9baa2aa93b06",
   "metadata": {},
   "source": [
    "## Fitting a Random Graph Model to our Network\n",
    "\n",
    "These degree histograms don't exist in a vacuum. They can be compared to theoretical models of networks to help interpret them. We mean to learn about our network, not simply compute metrics.\n",
    "\n",
    "NetworkX has many random graph models in the form of [Graph Generators](https://networkx.org/documentation/stable/reference/generators.html) that can generate instances of a random graph. For small graphs, it is neccessary to iteratively generate and measure multiple instances of random graphs. For larger graphs a single instance can work. This is due to the \n",
    "\n",
    "### Erdos Reyni Graphs\n",
    "\n",
    "We begin with a random graph model often used as a baseline, not because it fits real-world networks.\n",
    "\n",
    "> An Erdős–Rényi (ER) graph is a simple model of a random graph. This model is named after Paul Erdős and Alfréd Rényi, who first introduced one version of it.\n",
    ">\n",
    "> In the ER model, a graph is constructed by connecting nodes randomly. Each edge is included in the graph with probability p, independent of the other edges. Thus, the model has two parameters: the number of nodes n, and the edge probability p.\n",
    ">\n",
    "> The ER model can be used to generate either a G(n, M) graph or a G(n, p) graph.\n",
    ">\n",
    "> In the G(n, M) model, a graph is chosen uniformly at random from the collection of all graphs with n nodes and M edges.\n",
    "> In the G(n, p) model, a graph is constructed by connecting nodes randomly where each possible edge occurs independently with probability p.\n",
    "> The ER model is a simple model that does not capture many of the properties of real-world networks, such as their community structure and degree distribution. However, it is useful in network theory because of its analytical tractability and because it serves as a null model against which the properties of real-world networks can be compared.\n",
    "\n",
    "-- ChatGPT, verified by Russell Jurney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cfb609-c7be-4bea-9974-fe49d4e694cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the edge probability parameter for the Erdos-Renyi model\n",
    "p = nx.density(G)\n",
    "print(f\"Citation network density: {p:.5f}\")\n",
    "\n",
    "# Create an Erdos-Renyi graph with the same number of nodes and the calculated edge probability\n",
    "G_er = nx.erdos_renyi_graph(G.number_of_nodes(), p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd968ef-7934-4cd9-9c2e-5619910a8599",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_degrees(G, G_compare=None, compare_label=None):\n",
    "    \"\"\"Compute the degrees of a graph and compare them to a random graph model.\"\"\"\n",
    "\n",
    "    # Create histogram of degrees in real citation graph\n",
    "    degree_sequence = [d + 1 for n, d in G.degree()]\n",
    "    ax = sns.histplot(degree_sequence, bins=20, kde=True, log_scale=True, label=\"Citation Graph\")\n",
    "    ax.set(xlabel='Degree')\n",
    "\n",
    "    if G_compare:\n",
    "        # Create histogram of degree and plot alongside\n",
    "        compare_degree_sequence = [d + 1 for n, d in G_compare.degree()]\n",
    "        ax = sns.histplot(compare_degree_sequence, bins=20, kde=True, log_scale=True, label=compare_label if compare_label else \"Random Graph Model\")\n",
    "        ax.set(xlabel='Degree')\n",
    "    \n",
    "        plt.title(f\"Degree Histogram Comparison\")\n",
    "    else:\n",
    "        plt.title(\"Degree Histogram\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57637f48-1b22-427f-8872-bd4a881b8da6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare our graph with a random graph model\n",
    "plot_degrees(G, G_er, compare_label=\"Erdos Reyni\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a95e8f-281c-4fb5-914d-d99b9d0345d1",
   "metadata": {},
   "source": [
    "While narrower, this isn't completely dissimilar... and yet citation graphs are not known to look like random graph models. Let's check another. The [networkX Graph Generators documentation](https://networkx.org/documentation/stable/reference/generators.html) has a long list.\n",
    "\n",
    "### Barabasi-Albert Model\n",
    "\n",
    "> Barabási-Albert Model (Preferential Attachment Model, Scale Free Network): This model generates a random graph where nodes are added to the network one at a time, and each new node attaches to existing nodes with a probability proportional to their degree.\n",
    "\n",
    "A scale free network is one where its degree distribution fits a power law. In log scale, they proceed from top-left to bottom-right. Barabasi-Albert networks model step-wise growth of networks where edges are added in a way that prefers nodes with a higher degree.\n",
    "\n",
    "--ChatGPT, extended by Russell Jurney\n",
    "\n",
    "<center><img alt=\"Random network vs a Scale Free network\" src=\"images/random-vs-scale-free-network.jpg\" /></center>\n",
    "\n",
    "--Network-based approaches for anticancer therapy (Review), Seo et al, 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a542baf-97c5-4a74-8eaf-b20ae98a71e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of iterations required (This should be smaller than the total number of nodes)\n",
    "n = G.number_of_nodes()\n",
    "\n",
    "# Number of edges to attach from a new node to existing nodes\n",
    "m = G.number_of_edges() // n\n",
    "\n",
    "print(f\"n: {n:,} m: {m:,}\")\n",
    "\n",
    "# Create a Barabasi-Albert graph with the same number of nodes and edges as the real graph\n",
    "G_ba = nx.barabasi_albert_graph(n, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcba7ff-0446-47c0-ae55-e9ae153df9a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Total nodes: {G_ba.number_of_nodes():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0531abfe-0364-487e-bd5f-b3d9b9c9d285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_degrees(G, G_ba, compare_label=\"Barabasi-Albert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19adacbf-93d7-49fd-9daa-20b17e447040",
   "metadata": {},
   "source": [
    "### Interpreting Random Graph Comparisons\n",
    "\n",
    "Neither one of these fits the citation network very well. Some searching in the network science literature brings up this: Citation Graph Model. Wikipedia describes the first such model, Price's model as:\n",
    "\n",
    "> Price's model (named after the physicist Derek J. de Solla Price) is a mathematical model for the growth of citation networks.[1][2] It was the first model which generalized the Simon model[3] to be used for networks, especially for growing networks. Price's model belongs to the broader class of network growing models (together with the Barabási–Albert model) whose primary target is to explain the origination of networks with strongly skewed degree distributions. The model picked up the ideas of the Simon model reflecting the concept of rich get richer, also known as the Matthew effect. Price took the example of a network of citations between scientific papers and expressed its properties. His idea was that the way an old vertex (existing paper) gets new edges (new citations) should be proportional to the number of existing edges (existing citations) the vertex already has. This was referred to as cumulative advantage, now also known as preferential attachment. Price's work is also significant in providing the first known example of a scale-free network (although this term was introduced later). His ideas were used to describe many real-world networks such as the Web.\n",
    "\n",
    "--ChatGPT4 and Russell Jurney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2692c8-3d9a-43d3-951c-e19ac910f337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def citation_graph(n, m):\n",
    "    \"\"\"\n",
    "    Create a citation graph with n nodes. Each new node attaches to m existing nodes.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(range(2 * m))  # Start with 2m nodes\n",
    "    G.add_edges_from((i, 2 * m) for i in range(2 * m))  # Each of the first 2m nodes links to the node 2m\n",
    "\n",
    "    for i in range(2 * m + 1, n):\n",
    "        # A new paper cites m existing papers. The probability of citing an existing paper is proportional to its in-degree.\n",
    "        nodes = list(G.nodes())\n",
    "        probabilities = [G.in_degree(n) for n in nodes]\n",
    "        cited_nodes = random.choices(nodes, probabilities, k=m)\n",
    "        for node in cited_nodes:\n",
    "            G.add_edge(node, i)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb2ba0-13a8-4f4d-8e9d-9287546ee48a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = G.number_of_nodes()\n",
    "m = (G.number_of_edges() // n)  # integer division\n",
    "\n",
    "# Note: this can take a while\n",
    "G_cite = citation_graph(n, m)\n",
    "\n",
    "plot_degrees(G, G_cite, \"Price's Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997b1271-4a4b-473e-91db-773b872f231d",
   "metadata": {},
   "source": [
    "Let's try reparametizing Price's model to fit our network. The number of nodes is parameter n, it doesn't make sense to change that. We can however play with m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc5c74-3e32-4c42-8a98-ddb33351c1d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = G.number_of_nodes()\n",
    "m = (G.number_of_edges() // n)  # integer division\n",
    "\n",
    "# Note: this can take a while\n",
    "G_cite = citation_graph(n, m)\n",
    "\n",
    "plot_degrees(G, G_cite, \"Price's Model 2nd Pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83bcff7-da6c-44b3-9ed1-129d2a913e0c",
   "metadata": {},
   "source": [
    "### Random Graph Model Conclusion\n",
    "\n",
    "We didn't fine the perfect random graph model, but hopefully you understand how to research what models might characterize your graph and what that means about it. As a data scientist, this is a valuable tool as it gives you the context you neex to make inferences about patterns in the network that can result in insight and clues for effective automation via machine learning. We could take this further and tune the parameters in Price's model or look for more citation network models that might fit better. For now we will stop here :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54a308-7259-4ee3-8d4d-bd0a2bf86a78",
   "metadata": {},
   "source": [
    "## Centrality Metrics\n",
    "\n",
    "Question: What are the most important papers in the field of physics during this period, from 1992 to 2003?\n",
    "Metrics: Network centralities - metrics designed to determine the prominence of a node in its local neighborhood or the whole network.\n",
    "\n",
    "You can find a number of centrality metrics under the [documentation for centrality](https://networkx.org/documentation/stable/reference/algorithms/centrality.html) in networkx.\n",
    "\n",
    "We are going to compute several centralities and add them to our `networkx.DiGraph` as properties. Then we can visualize them to understand what they mean.\n",
    "\n",
    "### Local Centralities\n",
    "\n",
    "- Degree Centrality: the number of connections a node has. Variants: in-degree centrality and out-degree centrality, which count the number of in-bound and out-bound connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4996f743-f267-4b7c-89cc-3648075dd314",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_degrees = nx.degree_centrality(G)\n",
    "\n",
    "for node, relative_degree in relative_degrees.items():\n",
    "    G.nodes[node][\"relative_degree\"] = relative_degree\n",
    "    G.nodes[node][\"relative_degree_log10\"] = np.log10(relative_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d5e46f-9ba3-4258-8f83-6cbe9bbb3846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "G.nodes[\"9711194\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff32a2a-7c8f-4b71-8ba7-1766c7138396",
   "metadata": {},
   "source": [
    "### Global Centralities\n",
    "\n",
    "- Eigenvector Centrality\n",
    "- Closeness Centrality\n",
    "- Betweenness Centrality\n",
    "\n",
    "#### So Many Centralities\n",
    "\n",
    "Check out the networkx [Centrality Algorithms documentation](https://networkx.org/documentation/stable/reference/algorithms/centrality.html) for a long list of other centralities. For a (not comprehnsive review) check out [Centrality Measures in Complex Networks: A Survey](https://arxiv.org/abs/2011.07190).\n",
    "\n",
    "#### Eigenvector Centrality\n",
    "\n",
    ">This is a measure of the influence of a node in a network. It assigns relative scores to all nodes in the network based on the concept that connections to high-scoring nodes contribute more to the score of the node in question than equal connections to low-scoring nodes.\n",
    "\n",
    "--ChatGPT4 and Russell Jurney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ba75d-b901-4420-bee0-2ba44c2b0b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eigenvectors = nx.eigenvector_centrality_numpy(G)\n",
    "\n",
    "for node, score in eigenvectors.items():\n",
    "    G.nodes[node][\"eigenvector\"] = score\n",
    "    G.nodes[node][\"eigenvector_log10\"] = np.log10(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60c9c36-623b-41ef-86cc-491aecb9be9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Closeness Centrality\n",
    "\n",
    ">This measure calculates the average length of the shortest paths from a node to all other nodes in the network. Nodes with lower average shortest path lengths have higher closeness centrality and are often considered as being more central. A common interpretation of closeness centrality is that nodes with a high score are more likely to recieve information from other sources in the network than nodes with a low score.\n",
    "\n",
    "--ChatGPT4 and Russell Jurney\n",
    "\n",
    "Be aware that closeness and betweenness centrality are expensive to compute. They may not be available on large networks, or very densely connected networks. You can use NVIDIA RAPIDS [cuGraph centrality methods] if you have an NVIDIA GPU. Most data science work on GPUs is done on NVIDIA compatible GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4973663a-d4d6-402b-ba04-f91bf198c8dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "closeness = nx.closeness_centrality(G)\n",
    "\n",
    "for node, score in closeness.items():\n",
    "    G.nodes[node][\"closeness\"] = score\n",
    "    G.nodes[node][\"closeness_log10\"] np.log10(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b754a4f-b291-4579-87bb-d89279d9291d",
   "metadata": {},
   "source": [
    "#### Betweenness Centrality\n",
    "\n",
    "> This measure considers a node's position within the network in terms of the paths between other nodes. Nodes that frequently lie on the shortest paths between other nodes in the network have a high betweenness centrality. These nodes serve as important points of control and coordination within the network.\n",
    "\n",
    "Betweenness centrality is so expensive it quickly becomes infeasible on large networks. GPUs can extend this capability in libraries like [NVIDIA RAPIDS cuGraph's](https://github.com/rapidsai/cugraph) [cugraph.betweenness_centrality](https://docs.rapids.ai/api/cugraph/legacy/api_docs/api/cugraph.betweenness_centrality.html) feature.\n",
    "\n",
    "--ChatGPT4 and Russell Jurney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2cfaae-a6f9-49d6-a6a8-9512850171e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Note: this is too slow to actually run on CPU! We need to use cuGraph on a GPU.\n",
    "# between = nx.betweenness_centrality(G)\n",
    "\n",
    "# for node, score in between.items():\n",
    "#     G.nodes[node][\"betweenness\"] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a6cef-95d4-4df4-8c28-db4b9e4371cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "G.nodes[\"9711194\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f191e5-65c7-448c-9a03-399d7b5acc5c",
   "metadata": {},
   "source": [
    "### Visualizing Centralities in Graphistry\n",
    "\n",
    "Let's zoom in on one section of the network to make it manageable and then look at just those nodes. Nodes are sized by degree by default in Graphistry. Let's click on one of the largest nodes and perform a snowball sample two hops out.\n",
    "\n",
    "# Sampling Networks\n",
    "\n",
    "> Large-scale networks, seen in contexts like social media or bioinformatics, bring forth considerable computational challenges due to their size and complexity. Traditional algorithms and infrastructures often struggle to efficiently compute global network measures, visualize data, and manage inherent noise in these large datasets. Network sampling, which involves selecting a representative subset of the network, offers a way to alleviate these issues. It enables feasible computation and interpretation of network properties. However, creating effective sampling strategies is crucial to maintain the original network's key characteristics, ensuring accurate inferences about the larger system.\n",
    "\n",
    "There are three major types of sampling a network: random sampling, snowball sampling and random walk sampling.\n",
    "\n",
    "- Random sampling selects random nodes from the network and includes edges between those nodes that are in the sample. This makes sense when your graph fits a random graph model like Erdos Reyni.\n",
    "- Snowball sampling creates an \"ego network\" which is centered on a single node of particular interest. It is useful when you want to investigate a region of a network. An ego (a node) may be of particular interest in the context of its alters (its neighbors) or a cluster or region of the graph may require your attention. Snowball sampling can \"zoom in\" to assist with visualization, which we will use below.\n",
    "\n",
    "## Snowball Sampling\n",
    "\n",
    "> In the context of network science, a snowball sample is a method of data collection that uses a cascading or \"snowball\" effect to discover and analyze nodes in a network. It's often used when you don't have access to the entire network data, or when the network is too large to feasibly analyze in its entirety.\n",
    ">\n",
    "> The process begins with a small set of \"seed\" nodes in the network. You identify all nodes that these seed nodes are directly connected to (i.e., their neighbors), and add them to your sample. This is often referred to as one \"hop\" or \"wave\" out from the seed nodes.\n",
    ">\n",
    "> But the process doesn't stop there, much like a snowball rolling down a hill and growing in size, you continue the process for these newly added nodes, finding all of their connected nodes, or neighbors, and adding them to your sample. This is the second hop, and you can continue this for as many \"hops\" as you want, each time expanding the number of nodes in your sample.\n",
    ">\n",
    "> When we say \"1.5 hop\" in the context of a snowball sampling of a network, it typically means that we start from a certain node (or set of nodes), then we include all the direct neighbors of this node (that's the first \"hop\"), and then for the \"0.5\" hop, we also include nodes that are connected to the nodes in the first hop but not include their connections (i.e., not going another full hop further). The idea behind this is to gather a slightly wider view of the network around our starting point, but without expanding to the full 2-hop neighborhood, which could significantly increase the size of the sample.\n",
    "\n",
    "--ChatGPT and Russell Jurney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22838d-77be-4e7a-8f87-6f7c1f8753cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def snowball_sample(graph, start_nodes, hops: int):\n",
    "    \"\"\"\n",
    "    Perform an n-hop snowball sample on the given graph.\n",
    "    \n",
    "    :param graph: The NetworkX graph.\n",
    "    :param start_nodes: The nodes to start the snowball sampling from.\n",
    "    :param n: The number of hops. Connections within the nodes are automatically included.\n",
    "    :return: A subgraph of the given graph that is the result of the n-hop snowball sampling.\n",
    "    \"\"\"\n",
    "    # First add all start_nodes to the sample (0 hops)\n",
    "    sampled_graph = graph.subgraph(start_nodes).copy()\n",
    "\n",
    "    # Then add all their neighbors (1 hop)\n",
    "    for i in range(0, n):\n",
    "        for node in start_nodes:\n",
    "            sampled_graph.add_nodes_from(graph.neighbors(node))\n",
    "            sampled_graph.add_edges_from((node, neighbor) for neighbor in graph.neighbors(node))\n",
    "\n",
    "    # Finally, add edges between the neighbors (.5 hops, as in a 1.5 hop snowball sample)\n",
    "    # We iterate over all nodes in the sampled graph and, for each, add edges to its neighbors that are also in the sampled graph\n",
    "    for node in list(sampled_graph.nodes):\n",
    "        neighbors = set(graph.neighbors(node))\n",
    "        sampled_neighbors = neighbors.intersection(set(sampled_graph.nodes))\n",
    "        sampled_graph.add_edges_from((node, sampled_neighbor) for sampled_neighbor in sampled_neighbors)\n",
    "\n",
    "    return sampled_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f775309-f329-4f3f-a10c-cf49b87ed245",
   "metadata": {},
   "source": [
    "## Snowball Sampling an Influential Paper\n",
    "\n",
    "The 1997 paper [M Theory As A Matrix Model: A Conjecture, Banks et al, 1997](https://arxiv.org/abs/hep-th/9610043) has a degree of 1,219. It is a highly influential paper and has now received 2,710. Let's start with it and do a snowball sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0bc483-a8a2-4394-a96b-17301352515d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "G.nodes[\"9610043\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432aeb28-eda2-4627-91d4-735669259407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This paper is very well cited\n",
    "G_snowball = snowball_sample(G, [\"9610043\"], 1)\n",
    "G_snowball.number_of_nodes(), G_snowball.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf3f96-b1b3-49b3-8b04-35446432a8b5",
   "metadata": {},
   "source": [
    "# Visualizing Networks in Graphistry\n",
    "\n",
    "[Graphistry](https://graphistry.com/) can handle hundreds of thousands of nodes and edges using GPU acceleration. They provide GPU acceleration via [Graphistry Hub](https://hub.graphistry.com/), so we don't need to have a GPU in our notebook computers.\n",
    "\n",
    "We are going to load the snowball sample of `M Theory As A Matrix Model: A Conjecture` we created and investigate what the metrics we previously computed mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8049ab8-14cd-44be-891e-4c2c84b8aa07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    graphistry.bind(source='src', destination='dst', node='nodeid')\n",
    "    #.options({\"\"})\n",
    "    .plot(G_snowball)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b13947-d406-4b21-a268-cf90649143b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sampling Real World Networks\n",
    "\n",
    "Network algorithms can be difficult to scale because comparing every node to every other node has n^2 complexity.\n",
    "\n",
    "<br /><center><img src=\"images/github_adjacency_rjurney.webp\" width=\"800px\" /></center><br />\n",
    "\n",
    "Now that we are familiar with random graph models, we can fit one to our network using the metrics above to know which\n",
    "\n",
    "There is an excellent [Graph Sampling Library](https://github.com/Ashish7129/Graph_Sampling) on Github at [https://github.com/Ashish7129/Graph_Sampling](https://github.com/Ashish7129/Graph_Sampling) that contains algorithms that range from random walks to snowball samples. It referenes [Sampling from Large Graphs](https://cs.stanford.edu/~jure/pubs/sampling-kdd06.pdf) by Jure Leskovec (a name to remember if you get excited about network science and graph ML) and Christos Faloutsos which presents a range of techniques for sampling large graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad46b7a-1d29-41a1-b514-3c8642f1b80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "154d41e3-0d42-404b-ab5b-d70db5ba438d",
   "metadata": {},
   "source": [
    "## Beware of Supernodes!\n",
    "\n",
    "Supernodes are a DEAL STOPPER. What? To rephrase: a common obstacle in implmenting graph algorithms, using common tools or even simple graph processing is when on node has LOTS of connections and processing all of them would take until the end of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bb9185-bb80-44cf-a9f3-77644c84ba69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
