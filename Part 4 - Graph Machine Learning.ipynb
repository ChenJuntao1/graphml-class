{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b497ba-5964-42b7-94cc-ae81a6184314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d424c16-2fdb-42bf-aed4-79ba4ee1d127",
   "metadata": {},
   "source": [
    "# Graph Machine Learning with Graph Neural Networks (GNNs)\n",
    "\n",
    "Having explored network science, we are about to dive into Graph Neural Networks (GNNs). The best introduction to GNNs is a long blog post by []() entitled [A Gentle Introduction to Graph Neural Networks](https://distill.pub/2021/gnn-intro/) which the authors have _generously_ licensed under the Creative Commons. This lets me utilize their work to explain how GNNs work while providing source code along with it to bring your theoretical understanding to a practical one.\n",
    "\n",
    "## Citation: A Gentle Introduction to Graph Neural Networks\n",
    "\n",
    "Parts of the content in Part 4 of this course are based upon: `Sanchez-Lengeling, et al., \"A Gentle Introduction to Graph Neural Networks\", Distill, 2021.` This content is cited inline. Students are encouraged to read this blog post before or after class, and to reference it if they become confused about concepts in their data science and machine learning practice. \n",
    "\n",
    "The full list of authors is:\n",
    "\n",
    "* [Benjamin Sanchez-Lengeling](https://research.google/people/106640/)\n",
    "* [Emily Reif](https://research.google/people/106150/)\n",
    "* [Adam Pearce](https://research.google/people/AdamPearce/)\n",
    "* [Alexander B. Wiltschko](https://www.linkedin.com/in/alex-wiltschko-0a7b7537/)\n",
    "\n",
    "During the course you will have access to the instructor, who understands GNNs and can elaborate further and answer any questions you may have :)\n",
    "\n",
    "## Why is there so much talk about Graph Neural Networks?\n",
    "\n",
    "Knowledge graphs are at the peak of the Gartner hype cycle and graph neural networks (GNNs) are soon to be high on the ramp because they tap and unlock the potential of enterprise knowledge graphs. Data lakes put data in one place, knowledge graphs link datasets together and graph neural networks automate business processes using data from across an enterprise. \n",
    "\n",
    "\n",
    "\n",
    "Most graph databases are fast becoming cloud-based GNN platforms:\n",
    "\n",
    "* Neo4j → [Neo4j Graph Data Science](https://neo4j.com/product/graph-data-science/)\n",
    "* TigerGraph → [Machine Learning Workbench](https://www.tigergraph.com/ml-workbench/)\n",
    "* ArangoDB → [ArrangoGraphML](https://www.arangodb.com/arangodb-for-machine-learning/)\n",
    "* Kumo → [SQL query the future](https://kumo.ai/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdddc82b-5fa5-44ae-832e-dac427fcd75b",
   "metadata": {},
   "source": [
    "# Notes: Extra Text\n",
    "\n",
    "Let's wrap our dataset in a `torch_geometric` `Dataset` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b583d7d9-a923-458f-a8ca-c3689dfdb3ae",
   "metadata": {},
   "source": [
    "# PyG: Pytorch Geometric aka `torch_geometric`\n",
    "\n",
    "## Describing Graphs with PyG `Data` Classes\n",
    "\n",
    "Entire graphs in PyG are described by `Data` objects. The simple 3-node, 2-edge graph with a single feature in the [PyG documentation](https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html) looks like this:\n",
    "\n",
    "Note we have to define our edges bidirectionally.\n",
    "\n",
    "<center><img src=\"images/3-node-2-edge-pyg-graph.svg\" width=\"300px\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8584c84-7ef4-4ba4-8d86-585de012e554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3, 1], edge_index=[2, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]], dtype=torch.long)\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "print(data)\n",
    "data.validate(raise_on_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a88a98-f6a3-43d4-98e5-8519e4f51ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "852882ae-901e-41a7-a7e1-d5643dbfea15",
   "metadata": {},
   "source": [
    "`Data` classes can describe themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa3555b-3e6f-4026-a479-80f5eb0705e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['edge_index', 'x']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1a381d3-e6ec-4159-8ea3-d2e2485bb2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describing our happy little Graph :)\n",
      "\n",
      "Number of nodes: 3\n",
      "Number of edges: 4\n",
      "Number of node features: 1\n",
      "Has isolated nodes: False\n",
      "Has self loops: False\n",
      "Is directed: False\n"
     ]
    }
   ],
   "source": [
    "print(\"Describing our happy little Graph :)\\n\")\n",
    "print(f\"Number of nodes: {data.num_nodes:,}\")\n",
    "print(f\"Number of edges: {data.num_edges:,}\")\n",
    "print(f\"Number of node features: {data.num_node_features:,}\")\n",
    "print(f\"Has isolated nodes: {data.has_isolated_nodes()}\")\n",
    "print(f\"Has self loops: {data.has_self_loops()}\")\n",
    "print(f\"Is directed: {data.is_directed()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d14b1d-e832-4474-a411-97eba9c8c25d",
   "metadata": {},
   "source": [
    "### Directed Graph `Data`\n",
    "\n",
    "Below we make a directed version by failing to reflect the node IDs across the diagonal of the adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ace5901-a738-46cf-b286-21a173414f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3, 1], edge_index=[2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [0, 2]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directed_data = Data(x=x, edge_index=torch.tensor([[1,1],[0,2]]))\n",
    "print(directed_data)\n",
    "directed_data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2851dfe-f13a-4f03-b014-018196d22ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directed_data.is_directed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97d8554-d1db-47f0-b9cb-ed8487147a3c",
   "metadata": {},
   "source": [
    "# Graph Neural Networks (GNNs) with DGL (Deep Graph Library)\n",
    "\n",
    "[DGL or Deep Graph Library](https://dgl.ai) is the simplest way to get started with graph machine learning using graph neural networks (GNNs).\n",
    "\n",
    "First we will cover a few common operations with each major task type we covered in the lecture: node-level, edge-level, subgraph-level and graph-level.\n",
    "\n",
    "## Node-Level Tasks: Classification\n",
    "\n",
    "Node-level tasks usually involve property prediction - classifying nodes into categories or regressing one of their numeric properties. We'll cover both.\n",
    "\n",
    "As in the network science section of this course, we will start with a Text Attributed Graph (TAG) called a Citation Graph. We are going to use the [CORA dataset](https://relational.fit.cvut.cz/dataset/CORA), [described by Papers with Code](https://paperswithcode.com/dataset/cora) as:\n",
    "\n",
    "> Introduced by Andrew McCallum et al. in [Automating the Construction of Internet Portals with Machine Learning](https://doi.org/10.1023/A:1009953814988)\n",
    ">\n",
    "> The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.\n",
    "\n",
    "### CORA Node Features: Bag of Words\n",
    "\n",
    "Note... the features for this network are a [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) model: simple and _sparse_ rather than modern text representations which are _dense_, distributed representations in the form of language models or [embeddings](https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings). Each node has a row in the feature matrix and each of 1,433 unique words get a column with the word count. Before [Word2Vec](https://arxiv.org/abs/1301.3781) introduced text embeddings in 2013, the features for NLP problems were mostly 0s, with a few non-zero values.\n",
    "\n",
    "<center><img src=\"images/sparse_vs_dense_vectors.webp\" width=\"800px\" alt=\"Bag-of-Words (BoW) sparse vectors used in traditional NLP versus dense, embedded vector representations used in modern deep learning NLP\" /></center>\n",
    "\n",
    "The [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) prevented NLP applications from realizing their modern capabilities - the more words that were added, the more dimensions the features data got and the more dimensions you add to a _sparse_ feature vector... the more all the values of that vector start to approximate the same value. They stretch out over many dimensions and look the same.\n",
    "\n",
    "Embeddings like Word2Vec related _sparse representations_ of words to the text around them by storing a middle layer of a neural network, creating _dense representations_.\n",
    "\n",
    "<center><img src=\"images/from_sparse_to_dense.webp\" width=\"800px\" alt=\"The Word2Vec's Skipgram architecture maps sparse to dense vectors via a shallow embedding technique\" /></center>\n",
    "\n",
    "These are very useful because the dimensions of the feature vector correspond to particular semantics, and because you can compare two dense vectors and get a sense of how similar the objects the represent are. This is very useful for information retreival applications like search and clustering.\n",
    "\n",
    "<center><img src=\"images/king_minus_man_plus_woman.webp\" width=\"800px\" alt=\"Given the dense embedding vector for the word 'king', if we subtract the vector for 'man' and add 'woman', we arrive at a vector very close to 'queen'.\" /></center>\n",
    "\n",
    "We could use a language model or large language model (LLM) to embed the features or the original text and get better performance from our GCN. However, it is good to start simple and worry about feature engineering lately... you can spend an endless amount of time over optimizing a task nobody cares about. Make sure they want the prototype before you engineer incredible performance. A Bag of Words representation is a fine start.\n",
    "\n",
    "### CORA Classifier: Graph Convlutional Network\n",
    "\n",
    "We are going to use a neural network architecture that may be familiar to you: a convolutional neural network. The type we will employ is called a Graph Convolutional Network (GCN). Message passing occurs between nodes and the series of input messages to a node are summarized by the layers of a GCN after each round of message passing.\n",
    "\n",
    "<center><img src=\"images/gcn-decagon-overview.png\" width=\"1000px\" alt=\"Graph Neural Networks for Multirelational Link Prediction\" /><a href=\"https://snap.stanford.edu/decagon/\">Graph Neural Networks for Multirelational Link Prediction, Zitnik et al., 2018</a></center>\n",
    "\n",
    "\n",
    "There is often a big of tinkering required to make GNNs run, so even for this simple problem in DGL, we must specify our GNN architectre. It is simple enough. Let's see how it looks...\n",
    "\n",
    "Note: Figures Sources: [Dense Vectors: Capturing Meaning with Code](https://towardsdatascience.com/dense-vectors-capturing-meaning-with-code-88fc18bd94b9) by [James Briggs](https://jamescalam.medium.com/), [Graph Neural Networks for Multirelational Link Prediction, Zitnik et al., 2018](https://snap.stanford.edu/decagon/)\n",
    "\n",
    "### Building a GCN in DGL\n",
    "\n",
    "Let's build, train and evaluate our first GNN: a graph convoltional network for classifying CORA articles into categories.\n",
    "\n",
    "Note: Source for this section is the [Blitz Tutorial, Node Classification with DGL](https://docs.dgl.ai/tutorials/blitz/1_introduction.html#sphx-glr-tutorials-blitz-1-introduction-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d27e1dac-9af7-473c-b65f-5a8d854bf81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# DGL can also use Tensorflow or MXNet\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "import dgl\n",
    "import dgl.data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5ce97-daeb-4446-9a90-6976b58d417b",
   "metadata": {},
   "source": [
    "For now we will use a pre-loaded dataset. It contains the standard CORA bag-of-word (BoW) featres. Later we will construct our own graphs to perform feature engineering on them to do more sophisticated work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18f36d76-168a-4406-bb7c-4d127c5bb3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "Number of categories: 7\n"
     ]
    }
   ],
   "source": [
    "dataset = dgl.data.CoraGraphDataset()\n",
    "\n",
    "print(f\"Number of categories: {dataset.num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b369768-a99c-4a5c-bdc6-a78d8cbe93dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=2708, num_edges=10556,\n",
       "      ndata_schemes={'feat': Scheme(shape=(1433,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'test_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'train_mask': Scheme(shape=(), dtype=torch.bool)}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There can be more than one graph, this dataset has just one\n",
    "g = dataset[0]\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3fa7a9-8084-4f1f-a8c6-67a02e0775fe",
   "metadata": {},
   "source": [
    "`train_mask`, `val_mask` and `test_mask` are bit masks that denote the rows in the `label` and `feat` [Schemes](https://github.com/dmlc/dgl/blob/master/python/dgl/frame.py#L125) which with `DGLBACKEND=pytorch` contain DGL mappings to the [torch.Tensors](https://pytorch.org/docs/stable/tensors.html) making up the training, validation and test datasets respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48502f10-280f-4230-8939-16847c30cba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features\n",
      "{'feat': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'label': tensor([3, 4, 4,  ..., 3, 3, 3]), 'test_mask': tensor([False, False, False,  ...,  True,  True,  True]), 'val_mask': tensor([False, False, False,  ..., False, False, False]), 'train_mask': tensor([ True,  True,  True,  ..., False, False, False])}\n",
      "Edge features\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(\"Node features\")\n",
    "print(g.ndata)\n",
    "\n",
    "print(\"Edge features\")\n",
    "print(g.edata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f34d9f-c178-4dd4-8d05-d796b62f9ce2",
   "metadata": {},
   "source": [
    "### GCN Model Architecture - Diagrams, then Code\n",
    "\n",
    "The model itself is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) that uses the [dgl.nn.conv.GraphConv](https://docs.dgl.ai/generated/dgl.nn.pytorch.conv.GraphConv.html) class. \n",
    "\n",
    "<center><img src=\"images/Schematic-diagram-of-a-two-layer-GCN-model-The-dark-green-denotes-target-nodes-that-need_W640.jpg\" alt=\"Diagram of 2-layer GCN from Graph neural networks in node classification: survey and evaluation, Xiao et al., 2022\" width=\"600px\" /></center>\n",
    "\n",
    "<br />\n",
    "\n",
    "<center>Image credit: <a href=\"https://www.researchgate.net/publication/355873169_Graph_neural_networks_in_node_classification_survey_and_evaluation\">Diagram of 2-layer GCN from Graph neural networks in node classification: survey and evaluation, Xiao et al., 2022</a></center>\n",
    "\n",
    "<br />\n",
    "\n",
    "Let's dig into this diagram of our GCN before coding it in DGL.\n",
    "\n",
    "### Over Smoothing in GNNs: Too Many Layers Means Too Many Hops Sampled\n",
    "\n",
    "Note that **each layer of the GCN represents a round of message passing where nodes aggregate information from their neighbors.** This is important to know, as if you have too many layers in a GNN, you run into the [oversmoothing problem](https://towardsdatascience.com/over-smoothing-issue-in-graph-neural-network-bddc8fbc2472) where nodes start to look the same as all the other nodes.\n",
    "\n",
    "<center><img src=\"images/GNN-oversmoothing-first-layer.webp\" width=\"840px\" alt=\"First layer of GNN message passing, aggregation and summarization results in features of different colors\" /></center>\n",
    "<center>The first layer of GNN message passing, aggregation and summarization results in features represented by different colors.</center>\n",
    "<center><i>Image credit: <a href=\"https://towardsdatascience.com/over-smoothing-issue-in-graph-neural-network-bddc8fbc2472\">Over-smoothing issue in graph neural network</a> by <a href=\"https://towardsdatascience.com/over-smoothing-issue-in-graph-neural-network-bddc8fbc2472\">Anas Ait Aomar</a></i></center>\n",
    "\n",
    "<br /><br />\n",
    "\n",
    "<center><img src=\"images/GNN-oversmoothing-second-layer.webp\" width=\"1000px\" alt=\"Second layer of GNN message passing, aggregation and summarization results in features with more similar colors\" /></center>\n",
    "<center>The second layer of GNN message passing, aggregation and summarization results in features represented by more similar colors.</center>\n",
    "\n",
    "<center><i>Image credit: <a href=\"https://towardsdatascience.com/over-smoothing-issue-in-graph-neural-network-bddc8fbc2472\">Over-smoothing issue in graph neural network</a> by <a href=\"https://towardsdatascience.com/over-smoothing-issue-in-graph-neural-network-bddc8fbc2472\">Anas Ait Aomar</a></i></center>\n",
    "\n",
    "### Relu Activation Function\n",
    "\n",
    "Note how the GraphConv layers in the GCN architecture diagram above are separated by a Relu layer. Without this layer, the GCN could not learn effectively. Relu is an activation function that enables nonlinearity in neural networks - it lets them model messy data in a way that is much more powerful than a linear model. Relu is defined as `max(0, x)` which means that it maps negative values to 0 and positive values are left alone. Note that there are many derivatives of Relu that attempt to improve its performance.\n",
    "\n",
    "<center><img src=\"images/relu.png\" width=\"600px\" alt=\"Relu is max(0, x), making its plot flat when x is less than zero, and evently diagonal in a 1:1 ratio when x is greater than zero.\" /></center>\n",
    "<center>The Relu activation function: <code>max(0, x)</code></center>\n",
    "<center><i>Image Credit: <a href=\"https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7\">A Practical Guide to ReLU</a> by <a href=\"https://medium.com/@danqing\">Danqing Liu</a></i></center>\n",
    "\n",
    "> we can stack as many linear classifiers as we want on top of each other, and without nonlinear functions between them, it will just be the same as one linear classifier.\n",
    ">\n",
    "> But if we put a nonlinear function between them, such as max, then this is no longer true. Now each linear layer is actually somewhat decoupled from the other ones and can do its own useful work. The max function operates as a simple if statement.\n",
    ">\n",
    "_Source: [Nonlinearity and Neural Networks](https://medium.com/unpackai/nonlinearity-and-neural-networks-2ffaaac0e6ff) by [Aravinda 加阳](https://medium.com/@aravinda-gn)_\n",
    "\n",
    "This video by [deeplizard on Youtube](https://www.youtube.com/@deeplizard) explains Relu and its significance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1526ec3-dc02-41b5-9185-7e4fb342ac24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe width=\"800\" height=\"460\" src=\"https://www.youtube.com/embed/6MmGNZsA5nI?si=sglt8BijkpykWdWP&amp;start=10\"></iframe></center>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<center><iframe width=\"800\" height=\"460\" src=\"https://www.youtube.com/embed/6MmGNZsA5nI?si=sglt8BijkpykWdWP&amp;start=10\"></iframe></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096def14-9ac0-4b30-947a-b6ab70dcb36b",
   "metadata": {},
   "source": [
    "### Coding the Above GCN Diagram\n",
    "\n",
    "The equivalent DGL code for the GCN diagram above appears below. The graph structure and CORA BoW features are shown as the input, which feeds into one GCN layer, then a Relu activaton function, another GCN layer and finally they are mapped into the labels of our classes, in this case fields of study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eef1325-e796-4619-82cd-a66f911ca2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \"\"\"2-layer Graph Convolutional Network\"\"\"\n",
    "    \n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        \"\"\"Setup two GCN layers of with the input, inner and output dimensions.\"\"\"\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        \"\"\"Operate a forward pass of the network\"\"\"\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "\n",
    "# Create the model with given dimensions\n",
    "model = GCN(g.ndata[\"feat\"].shape[1], 16, dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "447bbf5b-8caf-43d3-be50-cf6254f9f13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): GraphConv(in=1433, out=16, normalization=both, activation=None)\n",
       "  (conv2): GraphConv(in=16, out=7, normalization=both, activation=None)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7393294-5d9d-4823-b76a-530f2e56c66a",
   "metadata": {},
   "source": [
    "### Training a GCN\n",
    "\n",
    "Below we define a training function that will iteratively train our GCN using message passing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35dad0c6-bc5c-43f9-b719-e4dd97eed4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 1.946, val acc: 0.108 (best 0.108), test acc: 0.104 (best 0.104), val precision: 0.108, val recall: 0.108, val f1: 0.108\n",
      "In epoch 5, loss: 1.894, val acc: 0.528 (best 0.552), test acc: 0.553 (best 0.557), val precision: 0.528, val recall: 0.528, val f1: 0.528\n",
      "In epoch 10, loss: 1.817, val acc: 0.608 (best 0.608), test acc: 0.586 (best 0.586), val precision: 0.608, val recall: 0.608, val f1: 0.608\n",
      "In epoch 15, loss: 1.716, val acc: 0.662 (best 0.662), test acc: 0.665 (best 0.665), val precision: 0.662, val recall: 0.662, val f1: 0.662\n",
      "In epoch 20, loss: 1.591, val acc: 0.664 (best 0.672), test acc: 0.669 (best 0.666), val precision: 0.664, val recall: 0.664, val f1: 0.664\n",
      "In epoch 25, loss: 1.446, val acc: 0.674 (best 0.674), test acc: 0.679 (best 0.679), val precision: 0.674, val recall: 0.674, val f1: 0.674\n",
      "In epoch 30, loss: 1.284, val acc: 0.714 (best 0.714), test acc: 0.705 (best 0.705), val precision: 0.714, val recall: 0.714, val f1: 0.714\n",
      "In epoch 35, loss: 1.109, val acc: 0.730 (best 0.730), test acc: 0.720 (best 0.720), val precision: 0.730, val recall: 0.730, val f1: 0.730\n",
      "In epoch 40, loss: 0.934, val acc: 0.732 (best 0.732), test acc: 0.728 (best 0.725), val precision: 0.732, val recall: 0.732, val f1: 0.732\n",
      "In epoch 45, loss: 0.770, val acc: 0.744 (best 0.744), test acc: 0.732 (best 0.733), val precision: 0.744, val recall: 0.744, val f1: 0.744\n",
      "In epoch 50, loss: 0.624, val acc: 0.748 (best 0.750), test acc: 0.735 (best 0.734), val precision: 0.748, val recall: 0.748, val f1: 0.748\n",
      "In epoch 55, loss: 0.500, val acc: 0.752 (best 0.752), test acc: 0.739 (best 0.737), val precision: 0.752, val recall: 0.752, val f1: 0.752\n",
      "In epoch 60, loss: 0.400, val acc: 0.762 (best 0.762), test acc: 0.752 (best 0.749), val precision: 0.762, val recall: 0.762, val f1: 0.762\n",
      "In epoch 65, loss: 0.321, val acc: 0.760 (best 0.764), test acc: 0.752 (best 0.753), val precision: 0.760, val recall: 0.760, val f1: 0.760\n",
      "In epoch 70, loss: 0.259, val acc: 0.764 (best 0.764), test acc: 0.755 (best 0.753), val precision: 0.764, val recall: 0.764, val f1: 0.764\n",
      "In epoch 75, loss: 0.211, val acc: 0.768 (best 0.768), test acc: 0.755 (best 0.755), val precision: 0.768, val recall: 0.768, val f1: 0.768\n",
      "In epoch 80, loss: 0.174, val acc: 0.764 (best 0.770), test acc: 0.761 (best 0.758), val precision: 0.764, val recall: 0.764, val f1: 0.764\n",
      "In epoch 85, loss: 0.145, val acc: 0.770 (best 0.770), test acc: 0.764 (best 0.758), val precision: 0.770, val recall: 0.770, val f1: 0.770\n",
      "In epoch 90, loss: 0.122, val acc: 0.768 (best 0.770), test acc: 0.761 (best 0.758), val precision: 0.768, val recall: 0.768, val f1: 0.768\n",
      "In epoch 95, loss: 0.104, val acc: 0.764 (best 0.770), test acc: 0.759 (best 0.758), val precision: 0.764, val recall: 0.764, val f1: 0.764\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, average=\"micro\"),\n",
    "        \"recall\": recall_score(y_true, y_pred, average=\"micro\"),\n",
    "        \"f1\": f1_score(y_true, y_pred, average=\"micro\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def train(g, model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    features = g.ndata[\"feat\"]\n",
    "    labels = g.ndata[\"label\"]\n",
    "    train_mask = g.ndata[\"train_mask\"]\n",
    "    val_mask = g.ndata[\"val_mask\"]\n",
    "    test_mask = g.ndata[\"test_mask\"]\n",
    "    for e in range(100):\n",
    "        # Forward\n",
    "        logits = model(g, features)\n",
    "\n",
    "        # Compute prediction\n",
    "        pred = logits.argmax(1)\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that you should only compute the losses of the nodes in the training set.\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        train_scores = metrics(labels[train_mask], pred[train_mask])\n",
    "        val_scores = metrics(labels[val_mask], pred[val_mask])\n",
    "        test_scores = metrics(labels[test_mask], pred[test_mask])\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            print(\n",
    "                f\"In epoch {e}, loss: {loss:.3f}, val acc: {val_acc:.3f} (best {best_val_acc:.3f}), test acc: {test_acc:.3f} (best {best_test_acc:.3f}),\",\n",
    "                f'val precision: {val_scores[\"precision\"]:.3f}, val recall: {val_scores[\"recall\"]:.3f}, val f1: {val_scores[\"f1\"]:.3f}'\n",
    "            )\n",
    "\n",
    "\n",
    "model = GCN(g.ndata[\"feat\"].shape[1], 16, dataset.num_classes)\n",
    "train(g, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdb66cfd-b55a-42b0-88a3-e6ee9683e18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): GraphConv(in=1433, out=16, normalization=both, activation=None)\n",
       "  (conv2): GraphConv(in=16, out=7, normalization=both, activation=None)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5cac91-65c7-457e-99e7-e3fffc7d826e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
