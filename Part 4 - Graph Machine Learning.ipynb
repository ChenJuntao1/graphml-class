{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85b497ba-5964-42b7-94cc-ae81a6184314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "import tarfile\n",
    "from typing import Dict, List\n",
    "\n",
    "import dgl\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from dgl.data import DGLDataset\n",
    "from dgl.data.utils import download as dgl_download\n",
    "from dgl.data.utils import load_graphs, save_graphs\n",
    "from dgl.sampling import global_uniform_negative_sampling\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 31337"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0fb2c6-4032-4eff-a174-a41c09a1f3b8",
   "metadata": {},
   "source": [
    "# Graph Machine Learning by Hand\n",
    "\n",
    "The old-school method of machine learning that was common practice before graph embeddings and Graph Neural Networks was to hand engineer features of the nodes of a network by querying the network with `networkx` or a graph database like Neo4j using Cypher or Gremlin and to assign the results of those queries to nodes.\n",
    "\n",
    "What are some common features when hand engineering node features in graph machine learning tasks?\n",
    "\n",
    "## Hand Engineered Features in Graph ML\n",
    "\n",
    "* Scalable Centralities: Degree [in, out, total], Eigenvector, PageRank [a form of eigenvector centrality], Katz\n",
    "* Less Scalable Centralities: Closeness, Betweenness\n",
    "\n",
    "You can compute these methods on single edges or on multiple edges as a way of projecting a _higher-order_ network, a network composed of more complex semantics built from the base network. Graph ML worked this way until around 2015, when graph embeddings changed everything :)\n",
    "\n",
    "## Loading our Citation Graph\n",
    "\n",
    "In Part 1 on knowledge graph construction, we saved our graph in GEXF format. Let's load it using the same procedure we performed in Part 2. I'm going to abbreviate the comments here, look at Part 2 for more details on what I am doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d192cd97-8a45-4c60-a7a3-42c438cca9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "G: nx.DiGraph = nx.read_gexf(path=\"data/physics_labeled.gexf.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c24db786-8fb6-4904-a45b-7c59a9a9970a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27770, 352807)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356be9c2-d542-4ef5-9f9d-3cfe608bf5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1001, 0),\n",
       " (9304045, 1),\n",
       " (9308122, 2),\n",
       " (9309097, 3),\n",
       " (9311042, 4),\n",
       " (9401139, 5),\n",
       " (9404151, 6),\n",
       " (9407087, 7),\n",
       " (9408099, 8),\n",
       " (9501030, 9)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/citation/file_to_net.pkl\", \"rb\") as f:\n",
    "    file_to_net = pickle.load(f)\n",
    "\n",
    "# Everything ok? Yes!\n",
    "list(file_to_net.items())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c236eb7-c936-49cf-ae8d-2ac823f0bf65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1001),\n",
       " (1, 9304045),\n",
       " (2, 9308122),\n",
       " (3, 9309097),\n",
       " (4, 9311042),\n",
       " (5, 9401139),\n",
       " (6, 9404151),\n",
       " (7, 9407087),\n",
       " (8, 9408099),\n",
       " (9, 9501030)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/citation/net_to_file.pkl\", \"rb\") as f:\n",
    "    net_to_file = pickle.load(f)\n",
    "\n",
    "# Everything ok here too? Yes!\n",
    "list(net_to_file.items())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7ac6c2b-5ddd-40a3-9fe1-be9ccd6b0d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ids_to_int(G):\n",
    "    # Create a new directed graph\n",
    "    G_int = nx.DiGraph()\n",
    "    \n",
    "    # Create a mapping from string IDs to integer IDs\n",
    "    id_mapping = {str_id: int(str_id) for str_id in G.nodes()}\n",
    "    \n",
    "    # Copy nodes and attributes, converting IDs to integers\n",
    "    for str_id, data in G.nodes(data=True):\n",
    "        int_id = id_mapping[str_id]\n",
    "        G_int.add_node(int_id, **data)\n",
    "    \n",
    "    # Copy edges and attributes, converting IDs to integers\n",
    "    for str_id1, str_id2, data in G.edges(data=True):\n",
    "        int_id1, int_id2 = id_mapping[str_id1], id_mapping[str_id2]\n",
    "        G_int.add_edge(int_id1, int_id2, **data)\n",
    "    \n",
    "    return G_int\n",
    "\n",
    "# Convert G to use integer IDs\n",
    "G_int = convert_ids_to_int(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f12c7bc5-de7d-42e2-8277-e0b541dd73bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27770, 352807)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_int.number_of_nodes(), G_int.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15cbdb80-115f-4595-9b30-1c5480a6e18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_id': 9711194,\n",
       " 'sequential_id': 5886,\n",
       " 'Paper': 'hep-th/9711194',\n",
       " 'Date': 'Wed, 26 Nov 1997 20:26:20 GMT',\n",
       " 'Title': 'On Integrable Structure behind the Generalized WDVV Equations',\n",
       " 'Comments': 'LaTeX, 6pp',\n",
       " 'Report-no': 'ITEP/TH-67/97',\n",
       " 'Journal-ref': 'Phys.Lett. B427 (1998) 93-96',\n",
       " 'Abstract': 'In the theory of quantum cohomologies the WDVV equations imply integrability of the system $(I\\\\partial_\\\\mu - zC_\\\\mu)\\\\psi = 0$. However, in generic situation -- of which an example is provided by the Seiberg-Witten theory -- there is no distinguished direction (like $t^0$) in the moduli space, and such equations for $\\\\psi$ appear inconsistent. Instead they are substituted by $(C_\\\\mu\\\\partial_\\\\nu - C_\\\\nu\\\\partial_\\\\mu)\\\\psi^{(\\\\mu)} \\\\sim (F_\\\\mu\\\\partial_\\\\nu - F_\\\\nu\\\\partial_\\\\mu)\\\\psi^{(\\\\mu)} = 0$, where matrices $(F_\\\\mu)_{\\\\alpha\\\\beta} = \\\\partial_\\\\alpha \\\\partial_\\\\beta \\\\partial_\\\\mu F$.',\n",
       " 'Journal-ref-DBSCAN': 2,\n",
       " 'Journal-ref-Label': 'Phys.Lett.',\n",
       " 'label': '5886'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test or integer index now... we pickled it instead of JSONized it so it would retain its integer keys and values!\n",
    "test_id = file_to_net[9711194]\n",
    "G_int.nodes[test_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18e7fff-07e6-4a58-94b7-ec9fb81a755e",
   "metadata": {},
   "source": [
    "### `G_int` --> `G`\n",
    "\n",
    "Now we can assign our new integer graph back to `G` and use it below :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cfabed8-c670-49d3-845e-678dca9633e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = G_int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86319fce-9dae-4c59-aa67-c55a02b168d0",
   "metadata": {},
   "source": [
    "## Feature Engineering for Link Prediction\n",
    "\n",
    "Remember this slide? Think on it again :) We are going to implement a couple of these to drive our first Journal classifier :)\n",
    "\n",
    "<center><img src=\"images/Feature-Engineering-for-Link-Prediction.jpg\" width=\"1000px\" /></center>\n",
    "\n",
    "We are going to load our citation graph and then combine the text embeddings we prepared before with manually computed features to perform link prediction. We will see how our metrics affect performance. Try not to jump to using an embedding, this is a useful exercise and we always need baselines :)\n",
    "\n",
    "Our job is to prepare a network features vector `features` that we can append to our other features before we train a classifier to perform link prediction.\n",
    "\n",
    "### Store Features by Column in `features`\n",
    "\n",
    "We will compute features for each node or pairs of nodes and then store them in the `features` `np.ndarray` by appending one column at a time. One row is a node. This is described in [NumPy: How to add an extra column to a NumPy array](https://www.w3resource.com/python-exercises/numpy/python-numpy-exercise-86.php) as looking like:\n",
    "\n",
    "<center><img src=\"images/append-column-to-matrix.png\" width=\"500px\" /></center>\n",
    "<center>Image Source: <a href=\"https://www.w3resource.com/python-exercises/numpy/python-numpy-exercise-86.php\">w3resource: NumPy: How to add an extra column to a NumPy array</a></center>\n",
    "\n",
    "#### Re-Running Feature Appends Will Make Duplicate Feature Columns\n",
    "\n",
    "Warning: If you run the cells below that do `np.append(features, my_feat_ary, axis=1)` more than once... you will get a duplicate features column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cd25944-0efe-4c57-a145-1c4567cbd1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[10, 20, 30],\n",
       "        [40, 50, 60]]),\n",
       " array([[100],\n",
       "        [200]]),\n",
       " array([[ 10,  20,  30, 100],\n",
       "        [ 40,  50,  60, 200]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I had to look this up, so here you are :)\n",
    "import numpy as np\n",
    "x = np.array([[10,20,30], [40,50,60]])\n",
    "y = np.array([[100], [200]])\n",
    "x, y, np.append(x, y, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87ecd87-e335-4ab5-8928-aacc95b3f92c",
   "metadata": {},
   "source": [
    "### Initialize `features` `nd.array`\n",
    "\n",
    "You can re-run this to recalculate the features from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a566905-e7fc-4838-987f-aea0b082d5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(27770, 0), dtype=float64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an inner list for each node ID\n",
    "inner_lists = [[] for x in range(G.number_of_nodes())]\n",
    "\n",
    "# The shape is 27,770 nodes long with zero feature columns\n",
    "features = np.array(inner_lists)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee28f99-ca8a-4922-85cf-44517ac70359",
   "metadata": {},
   "source": [
    "### `networkx` Centrality Metrics\n",
    "\n",
    "For our first features, we will use `networkx` to compute some centrality metrics. I'm going to limit our calculations to two centralities you can use in practice on networks of most any size:\n",
    "\n",
    "* Degree Centrality (in degree, out degree, degree [both]) - this is simple and shows how well connected a node is\n",
    "* Eigenvector Centrality - measures the influence of a node in a network, where a node is considered influential if it is connected to other influential nodes.\n",
    "\n",
    "Check out this list of [networkx.centrality](https://networkx.org/documentation/stable/reference/algorithms/centrality.html) metrics for feature ideas.\n",
    "\n",
    "#### Display Metrics using `pd.Series`\n",
    "\n",
    "If you want to see a metric computer by `nx.my_function()` you can wrap the `Dict` it returns [which will ALL print in a notebok] in a `pd.Series` and it will use the node ID as the index and display the feature. It is a good idea to lay eyes on things you compute - when possible - as often something goes wrong on a first attempt :)\n",
    "\n",
    "#### Degree Centrality\n",
    "\n",
    "Degree centrality is a **local centrality** metric. It measure influence in the nearby network.\n",
    "\n",
    "NetworkX measures degree centrality as a relative value for all nodes in a network. This can complicate inference but keeps values within an interpretable range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "682b2420-ade7-42b1-af66-81f2030409f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.003349\n",
       "1        0.000612\n",
       "2        0.004393\n",
       "3        0.005186\n",
       "4        0.002053\n",
       "           ...   \n",
       "27765    0.000036\n",
       "27766    0.000216\n",
       "27767    0.000216\n",
       "27768    0.000036\n",
       "27769    0.000288\n",
       "Length: 27770, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = nx.degree_centrality(G)\n",
    "\n",
    "# See - takes just 11 lines. The Dict printed over 27K rows on my screen. Try it!\n",
    "pd.Series(degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a6c2911d-ac45-4c99-b81d-3177fa73ee49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.000360\n",
       "1        0.000576\n",
       "2        0.004141\n",
       "3        0.005042\n",
       "4        0.001981\n",
       "           ...   \n",
       "27765    0.000036\n",
       "27766    0.000000\n",
       "27767    0.000000\n",
       "27768    0.000000\n",
       "27769    0.000000\n",
       "Length: 27770, dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_degree = nx.in_degree_centrality(G)\n",
    "pd.Series(in_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2c761e8d-305e-401b-9bc2-037b6a24e0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.002989\n",
       "1        0.000036\n",
       "2        0.000252\n",
       "3        0.000144\n",
       "4        0.000072\n",
       "           ...   \n",
       "27765    0.000000\n",
       "27766    0.000216\n",
       "27767    0.000216\n",
       "27768    0.000036\n",
       "27769    0.000288\n",
       "Length: 27770, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_degree = nx.out_degree_centrality(G)\n",
    "pd.Series(out_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cea3852-a8a2-4966-a70e-29cc7210a28e",
   "metadata": {},
   "source": [
    "#### Eigenvector Centrality\n",
    "\n",
    "Eigenvector centrality is a **global centrality** metric. It measures influence within the entire network. _We always want a local and global centrality metric in our feature set_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0b4e8999-3bf2-4c69-a489-9787cf2512f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        9.470787e-10\n",
       "1        1.349713e-03\n",
       "2        1.930384e-02\n",
       "3        2.848440e-02\n",
       "4        6.179491e-03\n",
       "             ...     \n",
       "27765    3.739993e-19\n",
       "27766    3.333821e-19\n",
       "27767    2.822265e-20\n",
       "27768   -1.340824e-19\n",
       "27769    3.723726e-19\n",
       "Length: 27770, dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvector = nx.eigenvector_centrality_numpy(G)\n",
    "pd.Series(eigenvector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc84c28-1df2-465c-bde0-b5e30ea0f5f7",
   "metadata": {},
   "source": [
    "### Add to `features` `np.array`\n",
    "\n",
    "That gives us four features so far that indicate a node's prominence in the network. Let's combine them to produce a four feature long feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5ff6162b-7420-49a8-b520-524ec83e89a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 3.34905830e-03,  3.60113796e-04,  2.98894451e-03,\n",
       "          9.47078666e-10],\n",
       "        [ 6.12193453e-04,  5.76182074e-04,  3.60113796e-05,\n",
       "          1.34971287e-03],\n",
       "        [ 4.39338831e-03,  4.14130865e-03,  2.52079657e-04,\n",
       "          1.93038392e-02],\n",
       "        ...,\n",
       "        [ 2.16068278e-04,  0.00000000e+00,  2.16068278e-04,\n",
       "          2.82226515e-20],\n",
       "        [ 3.60113796e-05,  0.00000000e+00,  3.60113796e-05,\n",
       "         -1.34082393e-19],\n",
       "        [ 2.88091037e-04,  0.00000000e+00,  2.88091037e-04,\n",
       "          3.72372610e-19]]),\n",
       " (27770, 4))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare and append the degree features for addition to `features`\n",
    "for d in [degree, in_degree, out_degree, eigenvector]:\n",
    "    d_ary = np.array(list(d.values())).reshape(-1, 1)\n",
    "    features = np.append(features, d_ary, axis=1)\n",
    "\n",
    "features, features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb6dd77-7f42-4430-90fa-04bc4fb3d179",
   "metadata": {},
   "source": [
    "### Community Detection with Louvain Modularity\n",
    "\n",
    "What about other kinds of features? One important category is the local group a node belongs to after performing an operation called \n",
    "\n",
    "One thing we didn't cover in Part 2 - Network Science, was community detection. What community a node belongs to is an important feature. We're going to cluster our network into communities and visualize them in Graphistry. Then we're going to add the cluster IDs as features in our `features` `np.array`.\n",
    "\n",
    "While there are many network clustering algorithms, the most common method of community detection is [Louvain Modularity](https://en.wikipedia.org/wiki/Louvain_method). We're going to use [nx.community.louvain_communities](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.louvain.louvain_communities.html#networkx.algorithms.community.louvain.louvain_communities) because it is efficient enough to work on networks up to moderate size using one computer. You can usually find a Louvain Modularity implementation for whatever platform you use to process even large networks - big data.\n",
    "\n",
    "> The method is a greedy optimization method that appears to run in time `O(n * log(n))`.\n",
    ">\n",
    "> ...\n",
    "> \n",
    "> In the Louvain Method of community detection, first small communities are found by optimizing modularity locally on all nodes, then each small community is grouped into one node and the first step is repeated.\n",
    "\n",
    "-- Wikipedia, [Louvain method](https://en.wikipedia.org/wiki/Louvain_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf507b5-97e5-4ca3-941b-9bb6735c9ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = nx.community.louvain_communities(G, seed=SEED)\n",
    "len(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d629b24e-5b15-471c-b1d1-504ef0c9a5f7",
   "metadata": {},
   "source": [
    "#### Evaluating Modularity\n",
    "\n",
    "I am curious how many clusters there are... lets plot their size in Seaborn using log scale. You can see there is a power law distribtion in cluster size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f903978-eb1a-4ce0-af0f-9e9cca7300c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get connected components and their sizes\n",
    "cluster_size = [len(c) for c in clusters]\n",
    "\n",
    "# Use seaborn to create the histogram\n",
    "sns.histplot(cluster_size, kde=True, bins=20, log_scale=True)\n",
    "plt.title(\"Histogram of Cluster Sizes\")\n",
    "plt.xlabel(\"Clustering Coefficient\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2f43db9-4e1f-4a1b-bf8c-a85c3f52029b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1909,\n",
       " 1376,\n",
       " 1289,\n",
       " 1117,\n",
       " 1086,\n",
       " 721,\n",
       " 496,\n",
       " 393,\n",
       " 361,\n",
       " 348,\n",
       " 344,\n",
       " 340,\n",
       " 313,\n",
       " 295,\n",
       " 280,\n",
       " 262,\n",
       " 252,\n",
       " 246,\n",
       " 237,\n",
       " 232,\n",
       " 227,\n",
       " 215,\n",
       " 202,\n",
       " 195,\n",
       " 179,\n",
       " 167,\n",
       " 144,\n",
       " 139,\n",
       " 138,\n",
       " 136,\n",
       " 130,\n",
       " 128,\n",
       " 128,\n",
       " 124,\n",
       " 122,\n",
       " 118,\n",
       " 113,\n",
       " 113,\n",
       " 112,\n",
       " 109,\n",
       " 105,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 100,\n",
       " 100,\n",
       " 98,\n",
       " 97,\n",
       " 97,\n",
       " 91]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(cluster_size, reverse=True)[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b3295-e0dc-4526-8384-519b2d5448f6",
   "metadata": {},
   "source": [
    "### ChatGPT and Me\n",
    "\n",
    "I needed to do consult a reference to interpret what this might mean for a citation graph... I knew it had a power law distribution with a long tail. I fed the following prompt to ChatGPT-4 Browser:\n",
    "\n",
    "#### **CHATGPT PROMPT**\n",
    "\n",
    "I have a citation graph of physics papers from arXiv, the high energy physics citation network from Stanford SNAP. I used NetworkX to compute Louvain Modularity and measured cluster sizes. Then I plotted them in seaborn on a log scale.\n",
    "\n",
    "The first 50 cluster sizes, sorted in descending order is:\n",
    "\n",
    "```\n",
    "[1909,\n",
    " 1376,\n",
    " 1289,\n",
    " 1117,\n",
    " 1086,\n",
    " 721,\n",
    " 496,\n",
    " 393,\n",
    " 361,\n",
    " 348,\n",
    " 344,\n",
    " 340,\n",
    " 313,\n",
    " 295,\n",
    " 280,\n",
    " 262,\n",
    " 252,\n",
    " 246,\n",
    " 237,\n",
    " 232,\n",
    " 227,\n",
    " 215,\n",
    " 202,\n",
    " 195,\n",
    " 179,\n",
    " 167,\n",
    " 144,\n",
    " 139,\n",
    " 138,\n",
    " 136,\n",
    " 130,\n",
    " 128,\n",
    " 128,\n",
    " 124,\n",
    " 122,\n",
    " 118,\n",
    " 113,\n",
    " 113,\n",
    " 112,\n",
    " 109,\n",
    " 105,\n",
    " 103,\n",
    " 103,\n",
    " 103,\n",
    " 100,\n",
    " 100,\n",
    " 98,\n",
    " 97,\n",
    " 97,\n",
    " 91]\n",
    "```\n",
    "\n",
    "Please interpret this histogram of cluster sizes and tell me what it means for a citation graph.\n",
    "\n",
    "#### Comment\n",
    "\n",
    "The interesting output checks out... ChatGPT-4 can interpret a histogram in a specific domain fairly well. Check it out. I'm trying to demonstrate how to incorporate AI into your workflows... I did ask it to rewrite this in fewer words, but I'm printing the entire output below.\n",
    "\n",
    "#### **CHATGPT RESPONSE**\n",
    "\n",
    "The cluster sizes you've provided, derived from applying the Louvain Modularity algorithm on a citation graph of physics papers, represent the sizes of communities of papers that cite each other. The sizes are listed in descending order, indicating a few large clusters followed by many smaller ones. This pattern is typical in real-world networks and is often referred to as a scale-free or heavy-tailed distribution. Here are a few interpretations and insights based on this distribution:\n",
    "\n",
    "* Community Structure - There are a few large communities of papers (e.g., the largest has 1909 papers) which likely represent major topics or fields within high energy physics. These large clusters suggest that there are common themes that have garnered significant attention and citation in the community.\n",
    "* Long Tail - As we move down the list, the cluster sizes decrease, forming a long tail of smaller communities. This indicates a wide variety of specialized or niche topics within the field that have smaller groups of papers citing each other.\n",
    "* Power-Law Distribution - The distribution of cluster sizes resembles a power-law distribution, which is common in many natural and social phenomena. In a power-law distribution, a few items (in this case, topics or fields) are exceptionally common, but the vast majority are rare.\n",
    "* Interdisciplinary or Emerging Areas - The smaller clusters may represent interdisciplinary or emerging areas of research that have not yet amassed a large number of citations.\n",
    "* Potential for Further Exploration - Understanding the thematic content of the large clusters could provide insights into the major areas of focus in high energy physics, while exploring the smaller clusters might reveal emerging or under-explored areas of research.\n",
    "* Rich-Get-Richer Phenomenon - The distribution may also reflect a \"rich-get-richer\" phenomenon where well-cited papers continue to accumulate more citations over time, reinforcing the prominence of certain topics.\n",
    "* Validation of Louvain Modularity - The distribution of cluster sizes could also serve as a validation of the Louvain Modularity algorithm's effectiveness in detecting community structure within the citation network.\n",
    "\n",
    "### Featurizing Communities\n",
    "\n",
    "To featurize our communities, we need to assign numeric values to the clusters and append a column for cluster ID to `features`. Note, we could alternatively [one-hot-encode](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) the features to give them one feature column per cluster. This would make the features sparse, which means lesss signal - something neural networks don't like but that another algorithm might not mind. We could use the [hashing trick](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html) - a method of feature engineering somewhere in between one-hot-encoding and neural embeddings. We are instead going to _label encode_ them with [sklearn.preprocessing.LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "127299fd-fe7a-4e11-9dfb-780bb0b6111e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{911, 912, 2432},\n",
       " {1458,\n",
       "  1508,\n",
       "  1511,\n",
       "  1512,\n",
       "  1513,\n",
       "  1514,\n",
       "  1515,\n",
       "  4674,\n",
       "  4683,\n",
       "  5512,\n",
       "  5513,\n",
       "  5514,\n",
       "  5516,\n",
       "  6829,\n",
       "  6831,\n",
       "  7695,\n",
       "  7696,\n",
       "  10556,\n",
       "  10846,\n",
       "  10847,\n",
       "  10848,\n",
       "  10850,\n",
       "  11827,\n",
       "  14146,\n",
       "  15037,\n",
       "  15700,\n",
       "  16387,\n",
       "  16613,\n",
       "  17417,\n",
       "  18606,\n",
       "  18838,\n",
       "  19302,\n",
       "  19608,\n",
       "  19723,\n",
       "  20080,\n",
       "  20226,\n",
       "  20806,\n",
       "  21319},\n",
       " {1531, 5667, 25553, 26332, 26784}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for cluster in clusters:\n",
    "clusters[0:3]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d10360c5-59f0-4d5b-9a57-6dbe6678f675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        130\n",
       "1        864\n",
       "2         49\n",
       "3        130\n",
       "4        864\n",
       "        ... \n",
       "27765    275\n",
       "27766    939\n",
       "27767     48\n",
       "27768    995\n",
       "27769     47\n",
       "Length: 27770, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate through clusters, assigning cluster IDs, then map them to node IDs\n",
    "node_clusters: Dict[int, int] = {}\n",
    "for cluster_id, cluster in zip(range(len(clusters)), clusters):\n",
    "    for node_id in cluster:\n",
    "        node_clusters[node_id] = cluster_id\n",
    "\n",
    "node_series = pd.Series(node_clusters).sort_index()\n",
    "node_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae96de9-7b2f-4d84-af08-9bae0ada6fbf",
   "metadata": {},
   "source": [
    "### Append Clusters to `features`\n",
    "\n",
    "Note, the clusters number from approximately 1-1,500. This dwarfs the previously computed, normalized centrality scores. We will be using scikit-learn's [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to address this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d60ee73c-5f7f-4a2b-861e-6249f28e70b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([130, 864,  49, ...,  48, 995,  47])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "088dd0f1-b632-4429-8540-81b697161af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.34905830e-03, 3.60113796e-04, 2.98894451e-03],\n",
       "       [6.12193453e-04, 5.76182074e-04, 3.60113796e-05],\n",
       "       [4.39338831e-03, 4.14130865e-03, 2.52079657e-04],\n",
       "       ...,\n",
       "       [2.16068278e-04, 0.00000000e+00, 2.16068278e-04],\n",
       "       [3.60113796e-05, 0.00000000e+00, 3.60113796e-05],\n",
       "       [2.88091037e-04, 0.00000000e+00, 2.88091037e-04]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33827e-408e-412e-b15c-e1e3a8855a71",
   "metadata": {},
   "source": [
    "## Role Discovery\n",
    "\n",
    "RolX role discovery is one of my favorite algorithms for networks :) You may recall from the slides how I used it to build a visually appealing, intrepretable map of the big data market. We're going to compute RolX roles to use as feature for ML tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a0b75b8-2b2f-4f5f-9b19-caeca158cda7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphrole'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphrole\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveFeatureExtractor, RoleExtractor\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[1;32m      5\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m RecursiveFeatureExtractor(G)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphrole'"
     ]
    }
   ],
   "source": [
    "from graphrole import RecursiveFeatureExtractor, RoleExtractor\n",
    "\n",
    "\n",
    "# Extract features\n",
    "feature_extractor = RecursiveFeatureExtractor(G)\n",
    "features = feature_extractor.extract_features()\n",
    "\n",
    "# Calculate roles\n",
    "role_extractor = RoleExtractor(n_roles=None)\n",
    "role_extractor.extract_role_factors(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b51c2e-30fe-406f-a05c-cf034e47d6d0",
   "metadata": {},
   "source": [
    "## Visualizing Communities and Roles in Graphistry\n",
    "\n",
    "We're going to use `graphistry` to visualize the communities and roles we determined to see if we can interpret them the way ChatGPT did - or in some other way :) This sort of belongs in Part 2 - Network Science, but we saved it here to use as a feature for node classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf72b49-970c-4a6c-9a55-88da23d8c397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13e893-268c-4a80-ad50-ab63c75f45f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5353b7bd-be31-4a18-b11d-746cb1d896b0",
   "metadata": {},
   "source": [
    "## Classifying Nodes into Journals\n",
    "\n",
    "We are going to train a classifier and then classify the nodes without journal entries into the most common journals in the field of high energy physics to see where they most likely belong. We are going to use a simple algorithm to do this, then get more sophisticated in the next sections on _graph embeddings_ and _graph neural networks (GNNs)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12e0ee6e-0ebc-409b-ac5e-1e61b978a58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_id': 9304045,\n",
       " 'sequential_id': 1,\n",
       " 'Paper': 'hep-th/9304045',\n",
       " 'Date': 'Sun, 11 Apr 93 12:29:30 -0500',\n",
       " 'Title': 'Generalized Calabi-Yau Manifolds and the Mirror of a Rigid Manifold',\n",
       " 'Authors': 'P. Candelas, E. Derrick and L. Parkes',\n",
       " 'Comments': '39 pages, plain TeX',\n",
       " 'Report-no': 'CERN-TH.6831/93, UTTG-24-92',\n",
       " 'Journal-ref': 'Nucl.Phys. B407 (1993) 115-154',\n",
       " 'Abstract': 'We describe the mirror of the Z orbifold as a representation of a class of generalized Calabi-Yau manifolds that can be realized as manifolds of dimension five and seven. Despite their dimension these correspond to superconformal theories with $c=9$ and so are perfectly good for compactifying the heterotic string to the four dimensions of space-time. As a check of mirror symmetry we compute the structure of the space of complex structures of the mirror and check that this reproduces the known results for the Yukawa couplings and metric appropriate to the Kahler class parameters on the Z orbifold together with their instanton corrections.',\n",
       " 'Journal-ref-DBSCAN': 1,\n",
       " 'Journal-ref-Label': 'Nucl.Phys.',\n",
       " 'label': '1'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.nodes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7642130a-c9ba-45ce-a82f-db0e82492774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c9a23a-b2e9-44fd-981b-600bc075e1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80a4fe12-f7a0-4134-a466-17e1524665d5",
   "metadata": {},
   "source": [
    "# Graph Machine Learning with Graph Embeddings\n",
    "\n",
    "[DeepWalk](https://arxiv.org/abs/1403.6652), Perozzi et al., 2014, was a revolution in graph machine learning. Along with [node2vec](https://snap.stanford.edu/node2vec/), Grover, A.; Leskovec, J, 2016, which came with code on Github [[aditya-grover/node2vec](https://github.com/aditya-grover/node2vec)], it removed the need to spend as much time doing feature engineering by hand. Although initially these embeddings worked on simple graphs with one type of edge and ignore node and edge properties... by efficiently encoding topology around a node, they autoomated much of the work involved in tasks like node classification/labeling and link prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb787f8f-a403-43e2-a75a-71212ef10ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d424c16-2fdb-42bf-aed4-79ba4ee1d127",
   "metadata": {},
   "source": [
    "# Graph Machine Learning with Graph Neural Networks (GNNs)\n",
    "\n",
    "Having explored network science, we are about to dive into Graph Neural Networks (GNNs). The best introduction to GNNs is a long blog post by []() entitled [A Gentle Introduction to Graph Neural Networks](https://distill.pub/2021/gnn-intro/) which the authors have _generously_ licensed under the Creative Commons. This lets me utilize their work to explain how GNNs work while providing source code along with it to bring your theoretical understanding to a practical one.\n",
    "\n",
    "## Citation: A Gentle Introduction to Graph Neural Networks\n",
    "\n",
    "Parts of the content in Part 4 of this course are based upon: `Sanchez-Lengeling, et al., \"A Gentle Introduction to Graph Neural Networks\", Distill, 2021.` This content is cited inline. Students are encouraged to read this blog post before or after class, and to reference it if they become confused about concepts in their data science and machine learning practice. \n",
    "\n",
    "The full list of authors is:\n",
    "\n",
    "* [Benjamin Sanchez-Lengeling](https://research.google/people/106640/)\n",
    "* [Emily Reif](https://research.google/people/106150/)\n",
    "* [Adam Pearce](https://research.google/people/AdamPearce/)\n",
    "* [Alexander B. Wiltschko](https://www.linkedin.com/in/alex-wiltschko-0a7b7537/)\n",
    "\n",
    "During the course you will have access to the instructor, who understands GNNs and can elaborate further and answer any questions you may have :)\n",
    "\n",
    "## Why is there so much talk about Graph Neural Networks?\n",
    "\n",
    "Knowledge graphs are at the peak of the Gartner hype cycle and graph neural networks (GNNs) are soon to be high on the ramp because they tap and unlock the potential of enterprise knowledge graphs. Data lakes put data in one place, knowledge graphs link datasets together and graph neural networks automate business processes using data from across an enterprise. \n",
    "\n",
    "\n",
    "\n",
    "Most graph databases are fast becoming cloud-based GNN platforms:\n",
    "\n",
    "* Neo4j → [Neo4j Graph Data Science](https://neo4j.com/product/graph-data-science/)\n",
    "* TigerGraph → [Machine Learning Workbench](https://www.tigergraph.com/ml-workbench/)\n",
    "* ArangoDB → [ArrangoGraphML](https://www.arangodb.com/arangodb-for-machine-learning/)\n",
    "* Kumo → [SQL query the future](https://kumo.ai/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdddc82b-5fa5-44ae-832e-dac427fcd75b",
   "metadata": {},
   "source": [
    "# Notes: Extra Text\n",
    "\n",
    "Let's wrap our dataset in a `torch_geometric` `Dataset` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b583d7d9-a923-458f-a8ca-c3689dfdb3ae",
   "metadata": {},
   "source": [
    "# PyG: Pytorch Geometric aka `torch_geometric`\n",
    "\n",
    "## Describing Graphs with PyG `Data` Classes\n",
    "\n",
    "Entire graphs in PyG are described by `Data` objects. The simple 3-node, 2-edge graph with a single feature in the [PyG documentation](https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html) looks like this:\n",
    "\n",
    "Note we have to define our edges bidirectionally.\n",
    "\n",
    "<center><img src=\"images/3-node-2-edge-pyg-graph.svg\" width=\"300px\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8584c84-7ef4-4ba4-8d86-585de012e554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]], dtype=torch.long)\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "print(data)\n",
    "data.validate(raise_on_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a88a98-f6a3-43d4-98e5-8519e4f51ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "852882ae-901e-41a7-a7e1-d5643dbfea15",
   "metadata": {},
   "source": [
    "`Data` classes can describe themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa3555b-3e6f-4026-a479-80f5eb0705e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a381d3-e6ec-4159-8ea3-d2e2485bb2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Describing our happy little Graph :)\\n\")\n",
    "print(f\"Number of nodes: {data.num_nodes:,}\")\n",
    "print(f\"Number of edges: {data.num_edges:,}\")\n",
    "print(f\"Number of node features: {data.num_node_features:,}\")\n",
    "print(f\"Has isolated nodes: {data.has_isolated_nodes()}\")\n",
    "print(f\"Has self loops: {data.has_self_loops()}\")\n",
    "print(f\"Is directed: {data.is_directed()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d14b1d-e832-4474-a411-97eba9c8c25d",
   "metadata": {},
   "source": [
    "### Directed Graph `Data`\n",
    "\n",
    "Below we make a directed version by failing to reflect the node IDs across the diagonal of the adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace5901-a738-46cf-b286-21a173414f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "directed_data = Data(x=x, edge_index=torch.tensor([[1,1],[0,2]]))\n",
    "print(directed_data)\n",
    "directed_data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2851dfe-f13a-4f03-b014-018196d22ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "directed_data.is_directed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97d8554-d1db-47f0-b9cb-ed8487147a3c",
   "metadata": {},
   "source": [
    "# Graph Neural Networks (GNNs) with DGL (Deep Graph Library)\n",
    "\n",
    "[DGL or Deep Graph Library](https://dgl.ai) is the simplest way to get started with graph machine learning using graph neural networks (GNNs).\n",
    "\n",
    "First we will cover a few common operations with each major task type we covered in the lecture: node-level, edge-level, subgraph-level and graph-level.\n",
    "\n",
    "## Node-Level Tasks: Classification\n",
    "\n",
    "Node-level tasks usually involve property prediction - classifying nodes into categories or regressing one of their numeric properties. We'll cover both.\n",
    "\n",
    "As in the network science section of this course, we will start with a Text Attributed Graph (TAG) called a Citation Graph. We are going to use the [CORA dataset](https://relational.fit.cvut.cz/dataset/CORA), [described by Papers with Code](https://paperswithcode.com/dataset/cora) as:\n",
    "\n",
    "> Introduced by Andrew McCallum et al. in [Automating the Construction of Internet Portals with Machine Learning](https://doi.org/10.1023/A:1009953814988)\n",
    ">\n",
    "> The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.\n",
    "\n",
    "### CORA Node Features: Bag of Words\n",
    "\n",
    "Note... the features for this network are a [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) model: simple and _sparse_ rather than modern text representations which are _dense_, distributed representations in the form of language models or [embeddings](https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings). Each node has a row in the feature matrix and each of 1,433 unique words get a column with the word count. Before [Word2Vec](https://arxiv.org/abs/1301.3781) introduced text embeddings in 2013, the features for NLP problems were mostly 0s, with a few non-zero values.\n",
    "\n",
    "<center><img src=\"images/sparse_vs_dense_vectors.webp\" width=\"800px\" alt=\"Bag-of-Words (BoW) sparse vectors used in traditional NLP versus dense, embedded vector representations used in modern deep learning NLP\" /></center>\n",
    "\n",
    "The [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) prevented NLP applications from realizing their modern capabilities - the more words that were added, the more dimensions the features data got and the more dimensions you add to a _sparse_ feature vector... the more all the values of that vector start to approximate the same value. They stretch out over many dimensions and look the same.\n",
    "\n",
    "Embeddings like Word2Vec related _sparse representations_ of words to the text around them by storing a middle layer of a neural network, creating _dense representations_.\n",
    "\n",
    "<center><img src=\"images/from_sparse_to_dense.webp\" width=\"700px\" alt=\"The Word2Vec's Skipgram architecture maps sparse to dense vectors via a shallow embedding technique\" /></center>\n",
    "\n",
    "These are very useful because the dimensions of the feature vector correspond to particular semantics, and because you can compare two dense vectors and get a sense of how similar the objects the represent are. This is very useful for information retreival applications like search and clustering.\n",
    "\n",
    "<center><img src=\"images/king_minus_man_plus_woman.webp\" width=\"800px\" alt=\"Given the dense embedding vector for the word 'king', if we subtract the vector for 'man' and add 'woman', we arrive at a vector very close to 'queen'.\" /></center>\n",
    "\n",
    "We could use a language model or large language model (LLM) to embed the features or the original text and get better performance from our GCN. However, it is good to start simple and worry about feature engineering lately... you can spend an endless amount of time over optimizing a task nobody cares about. Make sure they want the prototype before you engineer incredible performance. A Bag of Words representation is a fine start.\n",
    "\n",
    "### CORA Classifier: Graph Convlutional Network\n",
    "\n",
    "We are going to use a neural network architecture that may be familiar to you: a convolutional neural network. The type we will employ is called a Graph Convolutional Network (GCN). Message passing occurs between nodes and the series of input messages to a node are summarized by the layers of a GCN after each round of message passing.\n",
    "\n",
    "<center><img src=\"images/gcn-decagon-overview.png\" width=\"1000px\" alt=\"Graph Neural Networks for Multirelational Link Prediction\" /><a href=\"https://snap.stanford.edu/decagon/\">Graph Neural Networks for Multirelational Link Prediction, Zitnik et al., 2018</a></center>\n",
    "\n",
    "\n",
    "There is often a big of tinkering required to make GNNs run, so even for this simple problem in DGL, we must specify our GNN architectre. It is simple enough. Let's see how it looks...\n",
    "\n",
    "Note: Figures Sources: [Dense Vectors: Capturing Meaning with Code](https://towardsdatascience.com/dense-vectors-capturing-meaning-with-code-88fc18bd94b9) by [James Briggs](https://jamescalam.medium.com/), [Graph Neural Networks for Multirelational Link Prediction, Zitnik et al., 2018](https://snap.stanford.edu/decagon/)\n",
    "\n",
    "### Building a GCN in DGL\n",
    "\n",
    "Let's build, train and evaluate our first GNN: a graph convoltional network for classifying CORA articles into categories.\n",
    "\n",
    "Note: Source for this section is the [Blitz Tutorial, Node Classification with DGL](https://docs.dgl.ai/tutorials/blitz/1_introduction.html#sphx-glr-tutorials-blitz-1-introduction-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27e1dac-9af7-473c-b65f-5a8d854bf81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# DGL can also use Tensorflow or MXNet\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "import dgl\n",
    "import dgl.data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5ce97-daeb-4446-9a90-6976b58d417b",
   "metadata": {},
   "source": [
    "For now we will use a pre-loaded dataset. It contains the standard CORA bag-of-word (BoW) featres. Later we will construct our own graphs to perform feature engineering on them to do more sophisticated work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f36d76-168a-4406-bb7c-4d127c5bb3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dgl.data.CoraGraphDataset()\n",
    "\n",
    "print(f\"Number of categories: {dataset.num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b369768-a99c-4a5c-bdc6-a78d8cbe93dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There can be more than one graph, this dataset has just one\n",
    "g = dataset[0]\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3fa7a9-8084-4f1f-a8c6-67a02e0775fe",
   "metadata": {},
   "source": [
    "`train_mask`, `val_mask` and `test_mask` are bit masks that denote the rows in the `label` and `feat` [Schemes](https://github.com/dmlc/dgl/blob/master/python/dgl/frame.py#L125) which with `DGLBACKEND=pytorch` contain DGL mappings to the [torch.Tensors](https://pytorch.org/docs/stable/tensors.html) making up the training, validation and test datasets respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48502f10-280f-4230-8939-16847c30cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Node features\")\n",
    "print(g.ndata)\n",
    "\n",
    "print(\"Edge features\")\n",
    "print(g.edata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f34d9f-c178-4dd4-8d05-d796b62f9ce2",
   "metadata": {},
   "source": [
    "### GCN Model Architecture - Diagrams, then Code\n",
    "\n",
    "The model itself is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) that uses the [dgl.nn.conv.GraphConv](https://docs.dgl.ai/generated/dgl.nn.pytorch.conv.GraphConv.html) class. \n",
    "\n",
    "<center><img src=\"images/Schematic-diagram-of-a-two-layer-GCN-model-The-dark-green-denotes-target-nodes-that-need_W640.jpg\" alt=\"Diagram of 2-layer GCN from Graph neural networks in node classification: survey and evaluation, Xiao et al., 2022\" width=\"600px\" /></center>\n",
    "\n",
    "<br />\n",
    "\n",
    "<center>Image credit: <a href=\"https://www.researchgate.net/publication/355873169_Graph_neural_networks_in_node_classification_survey_and_evaluation\">Diagram of 2-layer GCN from Graph neural networks in node classification: survey and evaluation, Xiao et al., 2022</a></center>\n",
    "\n",
    "<br />\n",
    "\n",
    "Let's dig into this diagram of our GCN before coding it in DGL.\n",
    "\n",
    "### Over Smoothing in GNNs: Too Many Layers Means Too Many Hops Sampled\n",
    "\n",
    "Note that **each layer of the GCN represents a round of message passing where nodes aggregate information from their neighbors.** This is important to know, as if you have too many layers in a GNN, you run into the [oversmoothing problem](https://towardsdatascience.com/over-smoothing-issue-in-graph-neural-network-bddc8fbc2472) where nodes start to look the same as all the other nodes.\n",
    "\n",
    "<center><img src=\"images/GNN-oversmoothing-first-layer.webp\" width=\"840px\" alt=\"First layer of GNN message passing, aggregation and summarization results in features of different colors\" /></center>\n",
    "<center>The first layer of GNN message passing, aggregation and summarization results in features represented by different colors.</center>\n",
    "<center><i>Image credit: <a href=\"https://towardsdatascience.com/over-smoothing-issue-in-graph-neural-network-bddc8fbc2472\">Over-smoothing issue in graph neural network</a> by <a href=\"https://towardsdatascience.com/over-smoothing-issue-in-graph-neural-network-bddc8fbc2472\">Anas Ait Aomar</a></i></center>\n",
    "\n",
    "<br /><br />\n",
    "\n",
    "<center><img src=\"images/GNN-oversmoothing-second-layer.webp\" width=\"1000px\" alt=\"Second layer of GNN message passing, aggregation and summarization results in features with more similar colors\" /></center>\n",
    "<center>The second layer of GNN message passing, aggregation and summarization results in features represented by more similar colors.</center>\n",
    "\n",
    "<center><i>Image credit: <a href=\"https://towardsdatascience.com/over-smoothing-issue-in-graph-neural-network-bddc8fbc2472\">Over-smoothing issue in graph neural network</a> by <a href=\"https://towardsdatascience.com/over-smoothing-issue-in-graph-neural-network-bddc8fbc2472\">Anas Ait Aomar</a></i></center>\n",
    "\n",
    "### Relu Activation Function\n",
    "\n",
    "Note how the GraphConv layers in the GCN architecture diagram above are separated by a Relu layer. Without this layer, the GCN could not learn effectively. Relu is an activation function that enables nonlinearity in neural networks - it lets them model messy data in a way that is much more powerful than a linear model. Relu is defined as `max(0, x)` which means that it maps negative values to 0 and positive values are left alone. Note that there are many derivatives of Relu that attempt to improve its performance.\n",
    "\n",
    "<center><img src=\"images/relu.png\" width=\"600px\" alt=\"Relu is max(0, x), making its plot flat when x is less than zero, and evently diagonal in a 1:1 ratio when x is greater than zero.\" /></center>\n",
    "<center>The Relu activation function: <code>max(0, x)</code></center>\n",
    "<center><i>Image Credit: <a href=\"https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7\">A Practical Guide to ReLU</a> by <a href=\"https://medium.com/@danqing\">Danqing Liu</a></i></center>\n",
    "\n",
    "> we can stack as many linear classifiers as we want on top of each other, and without nonlinear functions between them, it will just be the same as one linear classifier.\n",
    ">\n",
    "> But if we put a nonlinear function between them, such as max, then this is no longer true. Now each linear layer is actually somewhat decoupled from the other ones and can do its own useful work. The max function operates as a simple if statement.\n",
    ">\n",
    "_Source: [Nonlinearity and Neural Networks](https://medium.com/unpackai/nonlinearity-and-neural-networks-2ffaaac0e6ff) by [Aravinda 加阳](https://medium.com/@aravinda-gn)_\n",
    "\n",
    "This video by [deeplizard on Youtube](https://www.youtube.com/@deeplizard) explains Relu and its significance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1526ec3-dc02-41b5-9185-7e4fb342ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<center><iframe width=\"800\" height=\"460\" src=\"https://www.youtube.com/embed/6MmGNZsA5nI?si=sglt8BijkpykWdWP&amp;start=10\"></iframe></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096def14-9ac0-4b30-947a-b6ab70dcb36b",
   "metadata": {},
   "source": [
    "### Coding the Above GCN Diagram\n",
    "\n",
    "The equivalent DGL code for the GCN diagram above appears below. The graph structure and CORA BoW features are shown as the input, which feeds into one GCN layer, then a Relu activaton function, another GCN layer and finally they are mapped into the labels of our classes, in this case fields of study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eef1325-e796-4619-82cd-a66f911ca2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \"\"\"2-layer Graph Convolutional Network\"\"\"\n",
    "    \n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        \"\"\"Setup two GCN layers of with the input, inner and output dimensions.\"\"\"\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        \"\"\"Operate a forward pass of the network\"\"\"\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "\n",
    "# Create the model with given dimensions\n",
    "model = GCN(g.ndata[\"feat\"].shape[1], 16, dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447bbf5b-8caf-43d3-be50-cf6254f9f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7393294-5d9d-4823-b76a-530f2e56c66a",
   "metadata": {},
   "source": [
    "### Training a GCN\n",
    "\n",
    "Below we define a training function that will iteratively train our GCN using message passing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dad0c6-bc5c-43f9-b719-e4dd97eed4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, average=\"micro\"),\n",
    "        \"recall\": recall_score(y_true, y_pred, average=\"micro\"),\n",
    "        \"f1\": f1_score(y_true, y_pred, average=\"micro\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def train(g, model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    features = g.ndata[\"feat\"]\n",
    "    labels = g.ndata[\"label\"]\n",
    "    train_mask = g.ndata[\"train_mask\"]\n",
    "    val_mask = g.ndata[\"val_mask\"]\n",
    "    test_mask = g.ndata[\"test_mask\"]\n",
    "    for e in range(100):\n",
    "        # Forward\n",
    "        logits = model(g, features)\n",
    "\n",
    "        # Compute prediction\n",
    "        pred = logits.argmax(1)\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that you should only compute the losses of the nodes in the training set.\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        train_scores = metrics(labels[train_mask], pred[train_mask])\n",
    "        val_scores = metrics(labels[val_mask], pred[val_mask])\n",
    "        test_scores = metrics(labels[test_mask], pred[test_mask])\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            print(\n",
    "                f\"In epoch {e}, loss: {loss:.3f}, val acc: {val_acc:.3f} (best {best_val_acc:.3f}), test acc: {test_acc:.3f} (best {best_test_acc:.3f}),\",\n",
    "                f'val precision: {val_scores[\"precision\"]:.3f}, val recall: {val_scores[\"recall\"]:.3f}, val f1: {val_scores[\"f1\"]:.3f}'\n",
    "            )\n",
    "\n",
    "\n",
    "model = GCN(g.ndata[\"feat\"].shape[1], 16, dataset.num_classes)\n",
    "train(g, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae7723-2ecd-464e-8232-ad17d6e1762c",
   "metadata": {},
   "source": [
    "## Graph Attention Networks (GATs)\n",
    "\n",
    "Let's try a more sophisticated architecture for node classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5cac91-65c7-457e-99e7-e3fffc7d826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GATConv\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.layer1 = GATConv(in_dim, hidden_dim, num_heads=num_heads, activation=F.relu, feat_drop=0.3)\n",
    "        self.layer2 = GATConv(hidden_dim * num_heads, out_dim, num_heads=1)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.layer1(g, in_feat)\n",
    "        h = h.view(h.shape[0], -1)\n",
    "        h = self.layer2(g, h)\n",
    "        return h.squeeze(1)\n",
    "\n",
    "\n",
    "gatconv = GAT(g.ndata[\"feat\"].shape[1], 10, dataset.num_classes, num_heads=2)\n",
    "train(g, gatconv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8913e40-d507-4c38-bbfe-dbdd62a00908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
