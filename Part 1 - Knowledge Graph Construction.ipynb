{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed5319f2-b647-4bcc-90b2-8f748ed39eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/opt/conda/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/opt/conda/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/opt/conda/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import calendar\n",
    "import gzip\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "from datetime import date\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import umap\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca312d8d-ed3d-4f0d-811f-58d5c810a8c1",
   "metadata": {},
   "source": [
    "Part 1: Knowledge Graph Construction\n",
    "====================================\n",
    "\n",
    "In this section of the course, we will cover _knowledge graph construction_ or how to construct knowledge graphs from both _natural networks_ and _structural networks_. In the lecture I described _knowledge graph construction_ as the process of building a knowledge graph from raw data using ETL, data transformations or Natural Language Processing (NLP).\n",
    "\n",
    "There are two main types of networks in common use: simple and heterogeneous networks. We're going to start out building a simple network where nodes are _academic papers_ and edges are _citations between papers_.\n",
    "\n",
    "<center><img src=\"images/Graphs-vs-Heterogeneous-Graphs-2000px.png\" width=\"1000px\" /></center>\n",
    "\n",
    "There are two categories of data from which we can build networks: _natural networks_ and _structural networks_. We can also transform _existing networks_ that are already formatted and easy to work with.\n",
    "\n",
    "<center><img src=\"images/ETL-in-Natural-and-Structural-Graphs.jpg\" width=\"860px\" /></center>\n",
    "\n",
    "Given time, we'll be building both _simple_ and _heterogeneous networks_ from both _natural_ and _structural graphs_. We'll start with the former.\n",
    "\n",
    "# Section Textbook\n",
    "\n",
    "An excellent resource for the first two parts of this course, **Knowledge Graph Construction** and **Network Science** is the [Network Science (CC4063 / CC4095)](https://www.dcc.fc.up.pt/~pribeiro/aulas/ns2122/) class taught by [Pedro Ribeiro](https://www.dcc.fc.up.pt/~pribeiro/) at the [Center for Research in Advanced Computing Systems](https://cracs.fc.up.pt/), part of the [Computer Science Department](https://www.dcc.fc.up.pt/site/) of the [University of Porto](https://www.up.pt/portal/en/).\n",
    "\n",
    "We will be using [Section 10: Network Construction](https://www.dcc.fc.up.pt/~pribeiro/aulas/ns2122/handouts.html#construction) during this part of the course, and specifically the slides for that section: [Network Construction (selected slides from J. Leskovec and L. Lacasa)](https://www.dcc.fc.up.pt/~pribeiro/aulas/ns2122/10_netconstruction.pdf).\n",
    "\n",
    "# Setting up Graphistry\n",
    "\n",
    "First let's setup a network visualization tool to help us evaluate what we are building. Throughout this part of the course we will be using `pygraphistry` and [Graphistry Hub](https://hub.graphistry.com/) [https://hub.graphistry.com/](https://hub.graphistry.com/) to visualize networks. Both are free for personal use and are powerful for visualizing networks large and small.\n",
    "\n",
    "You can [signup](https://hub.graphistry.com/accounts/signup/) for a Graphistry account at [https://hub.graphistry.com/accounts/signup/](https://hub.graphistry.com/accounts/signup/). <b>You should use a username/password/email to get the required credentials</b>, although after that you can login with your Github or Google account.\n",
    "\n",
    "<center><img src=\"images/graphistry_hub_registration.png\" /></center>\n",
    "\n",
    "Retain and use your credentials in the login form and in the environment variables in the next cell below. You should set the `GRAPHISTRY_USERNAME` and `GRAPHISTRY_PASSWORD` variables in the `env/graphistry.env` file, and then restart this docker container to pickup the new values.\n",
    "\n",
    "<center><img src=\"images/graphistry_hub_homepage.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94962741-ff18-417f-a218-52e6a61f5a0e",
   "metadata": {},
   "source": [
    "# ETL for a Simple, Natural Graph: High-energy Physics Theory Citation Network\n",
    "\n",
    "We are going to start out by building a knowledge graph from an existing edge list and then add properties to it. We'll be using the [High-energy physics theory citation network](https://snap.stanford.edu/data/cit-HepTh.html) from [Stanford SNAP](https://snap.stanford.edu/index.html). SNAP has many large network datasets available in the [Stanford Large Network Dataset Collection](https://snap.stanford.edu/data/).\n",
    "\n",
    "The dataset includes the following files, which we will combine:\n",
    "\n",
    "* [Citation graph edge list](https://snap.stanford.edu/data/cit-HepTh.txt.gz) contains node ID pairs. Node IDs are standard paper identifiers. This will build the core structure of our network.\n",
    "* [Paper metadata](cit-HepTh-abstracts.tar.gz) including abstracts. This will add propertis to our network.\n",
    "* [Publishing dates on arXiv](https://snap.stanford.edu/data/cit-HepTh-dates.txt.gz) will make our citation network a temporal [Directed-Acyclic-Graph (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph) since one paper can't cite another before it is written and there are no reciprocal edges. While we don't focus on this, it does affect our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3c191a7-0372-4723-9909-cefd2afc61a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variable setup\n",
    "GRAPHISTRY_USERNAME = os.getenv(\"GRAPHISTRY_USERNAME\")\n",
    "GRAPHISTRY_PASSWORD = os.getenv(\"GRAPHISTRY_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06e92082-faad-40a2-836e-ca7f98c5f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphistry.register(\n",
    "    api=3,\n",
    "    username=GRAPHISTRY_USERNAME,\n",
    "    password=GRAPHISTRY_PASSWORD,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09972cd9-b909-445f-8b78-94d1978b4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Graphistry\n",
    "GRAPHISTRY_PARAMS = {\n",
    "    \"play\": 600,\n",
    "    \"pointOpacity\": 0.7,\n",
    "    \"edgeOpacity\": 0.3,\n",
    "    \"edgeCurvature\": 0.3,\n",
    "    \"showArrows\": True,\n",
    "    \"gravity\": 0.5,\n",
    "}\n",
    "FAVICON_URL = \"https://graphlet.ai/assets/icons/favicon.ico\"\n",
    "LOGO = {\"url\": \"https://graphlet.ai/assets/Branding/Graphlet%20AI.svg\", \"dimensions\": {\"maxWidth\": 100, \"maxHeight\": 100}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b3cf16-524f-49a0-9865-eb5924e357d1",
   "metadata": {},
   "source": [
    "## `NetworkX` on PyPi is `networkx` in code is `nx`.\n",
    "\n",
    "The convention we used above is to load NetworkX via `import networkx as nx` so we can use the shorthand `nx` to call its classes and algorithms.\n",
    "\n",
    "## Numeric Node IDs\n",
    "\n",
    "What follows is a demonstration of _knowledge graph construction_, which we covered in the lecture. A node/edge list was provided, but the IDs are not sequential... which a network sampling tool I hope to use called [littleballoffur](https://github.com/benedekrozemberczki/littleballoffur) requires. In fact many graph libraries require sequential IDs. We must transform the graph IDs, create a mapping back and forth and annotate the nodes with properties for both IDs.\n",
    "\n",
    "## Build a Directional Graph (nx.DiGraph) from a CSV\n",
    "\n",
    "The edge list is a `#` commented, space-delimited CSV. We will parse it, assign sequential IDs and build a [`nx.DiGraph`](https://networkx.org/documentation/stable/reference/classes/digraph.html).\n",
    "\n",
    "### Download the Citation Edge List\n",
    "\n",
    "First, we download the edge list and build the structure of the network: `(paper)-cited->(paper)`. Note that we cache the edge list so you can edit the code without having to re-download the data.\n",
    "\n",
    "The edge list is located at [https://snap.stanford.edu/data/cit-HepTh.txt.gz](https://snap.stanford.edu/data/cit-HepTh.txt.gz) and is stored in `data/cit-HepTh.txt.gz`. We will read the file in its compressed state via the `gzip` builtin library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a4b88f4-deaf-444a-b90f-10bd9b7dad4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing citation graph edge file data/cit-HepTh.txt.gz\n"
     ]
    }
   ],
   "source": [
    "# Download and load edges (citations) from `cit-HepTh.txt.gz`\n",
    "edge_path = \"data/cit-HepTh.txt.gz\"\n",
    "gzip_content = None\n",
    "\n",
    "if os.path.exists(edge_path):\n",
    "    print(f\"Using existing citation graph edge file {edge_path}\")\n",
    "    gzip_content = open(edge_path, \"rb\")\n",
    "else:\n",
    "    print(\"Fetching citation graph edge file ...\")\n",
    "    response = requests.get(f\"https://snap.stanford.edu/{edge_path}\")\n",
    "    gzip_content = io.BytesIO(response.content)\n",
    "\n",
    "    print(\"Writing edge list to file {edge_path}\")\n",
    "    with open(edge_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "        print(f\"Wrote downloaded edge file to {edge_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b1ba635-1606-4e15-9b2f-795b79e44753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Directed graph (each unordered pair of nodes is saved once): Cit-HepTh.txt \n",
      "# Paper citation network of Arxiv High Energy Physics Theory category\n",
      "# Nodes: 27770 Edges: 352807\n",
      "# FromNodeId\tToNodeId\n",
      "1001\t9304045\n",
      "1001\t9308122\n",
      "1001\t9309097\n",
      "1001\t9311042\n",
      "1001\t9401139\n",
      "1001\t9404151\n",
      "\n",
      "gzip: stdout: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "# Check the top 10 lines of our gzip text file\n",
    "!zcat data/cit-HepTh.txt.gz | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88106b1c-c9a1-4ebe-82e2-a54d2b31df9e",
   "metadata": {},
   "source": [
    "### Graph and Identifier Setup\n",
    "\n",
    "We create directional a [`nx.DiGraph`](https://networkx.org/documentation/stable/reference/classes/digraph.html) because citations are inherently directional: from citer to cited. Note that whether we model them this way or not, citation graphs are temporal networks. The citing paper's publishing date must fall after cited paper's publishing date. We'll load publishing dates below.\n",
    "\n",
    "We need to setup `file_to_net` and `net_to_file` dictionaries to map back and forth between the file format's identifiers and or own sequential identifiers we'll be assigning starting with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78b66727-06b4-4c72-b02a-1caf2a66242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a directed graph\n",
    "G = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71c3f84c-c441-46f1-988b-7bca8c561a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create sequential IDs starting from 0 for littleballoffur and DGL\n",
    "file_to_net: Dict[int, int] = {}\n",
    "net_to_file: Dict[int, int] = {}\n",
    "current_idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181ae24c-b54d-4a5a-b286-c5f0288eaab4",
   "metadata": {},
   "source": [
    "### From Text to Graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cd66174-f00d-448d-be65-b68787ea7adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network structure ...\n",
      "Network built!\n"
     ]
    }
   ],
   "source": [
    "# Decompress the gzip content and build the edge list for our network\n",
    "print(\"Building network structure ...\")\n",
    "\n",
    "# Note we reuse the `gzip_content` variable from the download cell. This is a weird way to do it :)\n",
    "with gzip.GzipFile(fileobj=gzip_content) as f:\n",
    "\n",
    "    # Iterate through the lines, using the `line_number` as an `edge_id` below.\n",
    "    # They won't quite start at 0 owing to comments, but that's ok in the case of edges.\n",
    "    for line_number, line in enumerate(f):\n",
    "        line = line.decode(\"utf-8\")\n",
    "\n",
    "        # Ignore comment lines that start with '#'\n",
    "        if not line.startswith(\"#\"):\n",
    "            # Source (citing), desstination (cited) papers\n",
    "            citing_key, cited_key = line.strip().split(\"\\t\")\n",
    "\n",
    "            # The edge list makes the paper ID an int, stripping 0001001 to 1001, for example\n",
    "            citing_key, cited_key = int(citing_key), int(cited_key)\n",
    "\n",
    "            # If the either of the paper IDs don't exist, make one\n",
    "            for key in [citing_key, cited_key]:\n",
    "                if key not in file_to_net:\n",
    "                    # Build up an index that maps back and forth\n",
    "                    file_to_net[key] = current_idx\n",
    "                    net_to_file[current_idx] = key\n",
    "\n",
    "                    # Bump the current ID\n",
    "                    current_idx += 1\n",
    "\n",
    "            # print(f\"Citing key: {citing_key}, Cited key: {cited_key}\")\n",
    "            # print(f\"Mapped key: {file_to_net[citing_key]}, Mapped key: {file_to_net[cited_key]}\")\n",
    "\n",
    "            G.add_edge(file_to_net[citing_key], file_to_net[cited_key], edge_id=line_number)\n",
    "\n",
    "            # Conditionally set the keys on the nodes\n",
    "            G.nodes[file_to_net[citing_key]][\"file_id\"] = citing_key\n",
    "            G.nodes[file_to_net[citing_key]][\"sequential_id\"] = file_to_net[citing_key]\n",
    "\n",
    "            G.nodes[file_to_net[cited_key]][\"file_id\"] = cited_key\n",
    "            G.nodes[file_to_net[cited_key]][\"sequential_id\"] = file_to_net[cited_key]\n",
    "\n",
    "print(\"Network built!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a5d993-5545-4886-966a-e622d2409215",
   "metadata": {},
   "source": [
    "## Node Properties from Abstract Metadata\n",
    "\n",
    "In addition to the edge list, SNAP provides the paper's essential metadata in another file, which we will load to provide node properties and text embeddings for citation graph.\n",
    "\n",
    "We are going to perform the following steps:\n",
    "\n",
    "1) Download and cache the metadata to `data/cit-HepTh-abstracts.tar.gz`. Note that 1 file corresponds to one paper node's metadata.\n",
    "2) Process the tarball file where one file corresponds to one node ID in the original file. See why we made or mappings `file_to_net` and `net_to_file`?\n",
    "3) Assign node properties by parsing the fields of the record using traditional information extraction with regular expressions.\n",
    "4) Use a sentence transformer paraphrase model to summarize the entire textual record and enable node comparison for journal label creation.\n",
    "\n",
    "### Downloading the Abstract Metadata\n",
    "\n",
    "Another file containing node metadata, including the abstracts, for about 90% of nodes in this network is provided at [https://snap.stanford.edu/data/cit-HepTh-abstracts.tar.gz](https://snap.stanford.edu/data/cit-HepTh-abstracts.tar.gz), which we will save to `data/cit-HepTh-abstracts.tar.gz`. This code works just like the edge list download code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4609227a-1af3-41fd-9921-7715c11d9685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching paper abstracts ...\n",
      "Downloading paper abbstracts ...\n",
      "Downloading abstract file to data/cit-HepTh-abstracts.tar.gz\n",
      "Wrote downloaded abstract file to data/cit-HepTh-abstracts.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Download the abstracts from `cit-HepTh-abstracts.tar.gz`\n",
    "print(\"Fetching paper abstracts ...\")\n",
    "abstract_path = \"data/cit-HepTh-abstracts.tar.gz\"\n",
    "abstract_gzip_content = None\n",
    "\n",
    "if os.path.exists(abstract_path):\n",
    "    print(f\"Using existing paper abstracts file {abstract_path}\")\n",
    "    with open(abstract_path, \"rb\") as f:\n",
    "        abstract_gzip_content = io.BytesIO(f.read())\n",
    "else:\n",
    "    print(\"Downloading paper abbstracts ...\")\n",
    "    abstract_response = requests.get(f\"https://snap.stanford.edu/{abstract_path}\")\n",
    "    abstract_gzip_content = io.BytesIO(abstract_response.content)\n",
    "\n",
    "    print(f\"Downloading abstract file to {abstract_path}\")\n",
    "    with open(abstract_path, \"wb\") as f:\n",
    "        f.write(abstract_response.content)\n",
    "        print(f\"Wrote downloaded abstract file to {abstract_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ee4892-b011-4b3b-bb18-78633a297a3b",
   "metadata": {},
   "source": [
    "### Manually Parsing Node Metadata\n",
    "\n",
    "As a first pass let's use regular expressions in the Python `re` builtin library to extract each paper's fields so we can assign them as properties to our `nx.DiGraph` nodes.\n",
    "\n",
    "Here is what a couple of **test documents** look like. This is corresponds to two files in our abstract tarball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce09eff3-3189-45c5-b139-3d346bf97e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "\"\"\"------------------------------------------------------------------------------\n",
    "\\\\\n",
    "Paper: hep-th/9612115\n",
    "From: Asato Tsuchiya <tsuchiya@theory.kek.jp>\n",
    "Date: Wed, 11 Dec 1996 17:38:56 +0900   (20kb)\n",
    "Date (revised): Tue, 31 Dec 1996 01:06:34 +0900\n",
    "\n",
    "Title: A Large-N Reduced Model as Superstring\n",
    "Authors: N. Ishibashi, H. Kawai, Y. Kitazawa and A. Tsuchiya\n",
    "Comments: 29 pages, Latex, a footnote and references added, eq.(3.52)\n",
    "corrected, minor corrections\n",
    "Report-no: KEK-TH-503, TIT/HEP-357\n",
    "Journal-ref: Nucl.Phys. B498 (1997) 467-491\n",
    "\\\\\n",
    "A matrix model which has the manifest ten-dimensional N=2 super Poincare\n",
    "invariance is proposed. Interactions between BPS-saturated states are analyzed\n",
    "to show that massless spectrum is the same as that of type IIB string theory.\n",
    "It is conjectured that the large-N reduced model of ten-dimensional super\n",
    "Yang-Mills theory can be regarded as a constructive definition of this model\n",
    "and therefore is equivalent to superstring theory.\n",
    "\\\\\n",
    "\"\"\",\n",
    "    \"\"\"------------------------------------------------------------------------------\n",
    "\\\\\n",
    "Paper: hep-th/9711029\n",
    "From: John Schwarz <jhs@theory.caltech.edu>\n",
    "Date: Wed, 5 Nov 1997 17:30:55 GMT   (20kb)\n",
    "Date (revised v2): Thu, 6 Nov 1997 23:52:45 GMT   (21kb)\n",
    "\n",
    "Title: The Status of String Theory\n",
    "Author: John H. Schwarz\n",
    "Comments: 16 pages, latex, two figures; minor corrections, references added\n",
    "Report-no: CALT-68-2140\n",
    "\\\\\n",
    "There have been many remarkable developments in our understanding of\n",
    "superstring theory in the past few years, a period that has been described as\n",
    "``the second superstring revolution.'' Several of them are discussed here. The\n",
    "presentation is intended primarily for the benefit of nonexperts.\n",
    "\\\\\n",
    "\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f48a8b-95b3-404e-8658-7c9fd4002acd",
   "metadata": {},
   "source": [
    "### Structured Information Extraction with a Regex Helper\n",
    "\n",
    "Our extract function was created through trial and error using the [Pythex Regex Editor](https://pythex.org/). Paste the test documents where it says `Your test data` and try a couple of the patterns such as `r\"\"` above that where it says `Your regular expression`. It will show you where the patterns match in your data. A new section displays the matches within the text and a window on the right shows the text your regular expression will return via the list the `match.groups()` command returns.\n",
    "\n",
    "A few cycles of this and we have a clean extraction. In practice, I used more test records than this, which I've spared you in the interest of time :) Regular expressions are difficult to learn, but there are resources and **ChatGPT-4 is quite capable at writing regex!** It wrote many of the ones below.\n",
    "\n",
    "<center><img src=\"images/Pythex-Regex-Helper.png\" width=\"1000px\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b66d7e1-9e7a-4f51-adc2-7d26538b5dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paper_info(record):\n",
    "    \"\"\"Extract structured information from the text of academic paper text records using regular expressions.\n",
    "\n",
    "    Note: I was written wholly or in part by ChatGPT4 on May 23, 2023.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty dictionary to hold the information\n",
    "    info = {}\n",
    "\n",
    "    # Match \"Paper\" field\n",
    "    paper_match = re.search(r\"Paper:\\s*(.*)\", record)\n",
    "    if paper_match:\n",
    "        info[\"Paper\"] = paper_match.group(1)\n",
    "\n",
    "    # # Match \"From\" field\n",
    "    # from_match = re.search(r\"From:\\s*(.*)\", record)\n",
    "    # if from_match:\n",
    "    #     info['From'] = from_match.group(1)\n",
    "\n",
    "    # Match \"From\" field\n",
    "    from_match = re.search(r\"From:\\s*([^<]*)<\", record)\n",
    "    if from_match:\n",
    "        info[\"From\"] = from_match.group(1).strip()\n",
    "\n",
    "    # Match \"Date\" field\n",
    "    date_match = re.search(r\"Date:\\s*(.*)(\\s*)(\\(\\d+kb\\))\", record)\n",
    "    if date_match:\n",
    "        info[\"Date\"] = date_match.group(1).strip()\n",
    "\n",
    "    # Match \"Title\" field\n",
    "    title_match = re.search(r\"Title:\\s*(.*)\", record)\n",
    "    if title_match:\n",
    "        info[\"Title\"] = title_match.group(1)\n",
    "\n",
    "    # Match \"Authors\" field\n",
    "    authors_match = re.search(r\"Authors:\\s*(.*)\", record)\n",
    "    if authors_match:\n",
    "        info[\"Authors\"] = authors_match.group(1)\n",
    "\n",
    "    # Match \"Comments\" field\n",
    "    comments_match = re.search(r\"Comments:\\s*(.*)\", record)\n",
    "    if comments_match:\n",
    "        info[\"Comments\"] = comments_match.group(1)\n",
    "\n",
    "    # Match \"Report-no\" field\n",
    "    report_no_match = re.search(r\"Report-no:\\s*(.*)\", record)\n",
    "    if report_no_match:\n",
    "        info[\"Report-no\"] = report_no_match.group(1)\n",
    "\n",
    "    # Match \"Journal-ref\" field\n",
    "    journal_ref_match = re.search(r\"Journal-ref:\\s*(.*)\", record)\n",
    "    if journal_ref_match:\n",
    "        info[\"Journal-ref\"] = journal_ref_match.group(1)\n",
    "\n",
    "    # Extract \"Abstract\" field\n",
    "    abstract_pattern = r\"Journal-ref:[^\\\\\\\\]*\\\\\\\\[\\n\\s]*(.*?)(?=\\\\\\\\)\"\n",
    "    abstract_match = re.search(abstract_pattern, record, re.DOTALL)\n",
    "    if abstract_match:\n",
    "        abstract = abstract_match.group(1)\n",
    "        abstract = abstract.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "        info[\"Abstract\"] = abstract.strip()\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f0c15-5080-4875-9c9a-9312ee9e5ab6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f08f8-aa75-44a7-8e9c-b941b7bf0d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f3a3557-1506-4004-84a5-78d2de956061",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1786f68-848a-442a-9e96-ccd70debf27a",
   "metadata": {},
   "source": [
    "### Setting the Node Properties\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a18b0860-2b80-4903-9d5b-1bc7c557d7e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_paper_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m paper_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(member\u001b[38;5;241m.\u001b[39mname)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# We can also parse and use those values directly or embed field-wise\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m paper_info \u001b[38;5;241m=\u001b[39m \u001b[43mextract_paper_info\u001b[49m(content)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m paper_info:\n\u001b[1;32m     18\u001b[0m     abstract_paper_id \u001b[38;5;241m=\u001b[39m paper_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPaper\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_paper_info' is not defined"
     ]
    }
   ],
   "source": [
    "hit_count, miss_count, matches = 0, 0, 0\n",
    "all_abstracts: List[str] = []\n",
    "abstracts: Dict[int, str] = {}\n",
    "paper_ids: List[int] = []\n",
    "# Decompress the gzip content, then work through the abstract files in the tarball\n",
    "with gzip.GzipFile(fileobj=abstract_gzip_content) as f:\n",
    "    with tarfile.open(fileobj=f, mode=\"r|\") as tar:\n",
    "        for member in tar:\n",
    "            abstract_file = tar.extractfile(member)\n",
    "            if abstract_file:\n",
    "                content = abstract_file.read().decode(\"utf-8\")\n",
    "\n",
    "                paper_id = int(os.path.basename(member.name).split(\".\")[0])\n",
    "\n",
    "                # We can also parse and use those values directly or embed field-wise\n",
    "                paper_info = extract_paper_info(content)\n",
    "                if paper_info:\n",
    "                    abstract_paper_id = paper_info.get(\"Paper\", \"\").split(\"/\")[-1]\n",
    "                    if paper_id != int(abstract_paper_id):\n",
    "                        matches += 1\n",
    "                        print(f\"Paper ID {paper_id} != {abstract_paper_id}\")\n",
    "\n",
    "                    # Get the paper ID part of the \"Paper\" field\n",
    "                    if paper_id in file_to_net and file_to_net[paper_id] in G:\n",
    "                        for field, value in paper_info.items():\n",
    "                            G.nodes[file_to_net[paper_id]][field] = value\n",
    "\n",
    "                        abstracts[paper_id] = content\n",
    "                        all_abstracts.append(content)\n",
    "                        paper_ids.append(paper_id)\n",
    "\n",
    "                        hit_count += 1\n",
    "\n",
    "                    else:\n",
    "                        # Add isolated nodes if paper_id isn't in G\n",
    "                        miss_count += 1\n",
    "                        # G.add_node(file_to_net[paper_id], **paper_info)\n",
    "\n",
    "# Now `G` is a property graph representing the \"High-energy physics theory citation network\" dataset\n",
    "print(f\"Added metadata to {hit_count:,} nodes, {miss_count:,} were unknown.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb9990-5e52-499b-93ed-5a8a358044fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
