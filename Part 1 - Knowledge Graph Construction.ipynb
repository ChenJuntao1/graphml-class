{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed5319f2-b647-4bcc-90b2-8f748ed39eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/opt/conda/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/opt/conda/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/opt/conda/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import calendar\n",
    "import gzip\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "from datetime import date\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import graphistry\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import umap\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import Tensor\n",
    "\n",
    "sns.set(style='white', context='poster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca312d8d-ed3d-4f0d-811f-58d5c810a8c1",
   "metadata": {},
   "source": [
    "Part 1: Knowledge Graph Construction\n",
    "====================================\n",
    "\n",
    "In this section of the course, we will cover _knowledge graph construction_ or how to construct knowledge graphs from both _natural networks_ and _structural networks_. In the lecture I described _knowledge graph construction_ as the process of building a knowledge graph from raw data using ETL, data transformations or Natural Language Processing (NLP).\n",
    "\n",
    "There are two main types of networks in common use: simple and heterogeneous networks. We're going to start out building a simple network where nodes are _academic papers_ and edges are _citations between papers_.\n",
    "\n",
    "<center><img src=\"images/Graphs-vs-Heterogeneous-Graphs-2000px.png\" width=\"1000px\" /></center>\n",
    "\n",
    "There are two categories of data from which we can build networks: _natural networks_ and _structural networks_. We can also transform _existing networks_ that are already formatted and easy to work with.\n",
    "\n",
    "<center><img src=\"images/ETL-in-Natural-and-Structural-Graphs.jpg\" width=\"860px\" /></center>\n",
    "\n",
    "Structural networks are just as important as natural networks because...\n",
    "\n",
    "<center><img src=\"images/Raw-Data-are-Often-Not-Networks.png\" width=\"960px\" /></center>\n",
    "<center>Slide on Network Construction from <a href=\"https://scholar.google.com/citations?user=Q_kKkIUAAAAJ&hl=en&oi=ao\">Jure Leskovec's</a> <a href=\"https://web.stanford.edu/class/cs224w/\">Stanford CS224W class</a> when it was still called <i>Network Analysis</i>. Today it is called <i>Machine Learning with Graphs</i></center>\n",
    "\n",
    "<br />\n",
    "\n",
    "Given time, we'll be building both _simple_ and _heterogeneous networks_ from both _natural_ and _structural graphs_. We'll start with the former.\n",
    "\n",
    "# Section Textbook\n",
    "\n",
    "An excellent resource for the first two parts of this course, **Knowledge Graph Construction** and **Network Science** is the [Network Science (CC4063 / CC4095)](https://www.dcc.fc.up.pt/~pribeiro/aulas/ns2122/) class taught by [Pedro Ribeiro](https://www.dcc.fc.up.pt/~pribeiro/) at the [Center for Research in Advanced Computing Systems](https://cracs.fc.up.pt/), part of the [Computer Science Department](https://www.dcc.fc.up.pt/site/) of the [University of Porto](https://www.up.pt/portal/en/).\n",
    "\n",
    "We will be using [Section 10: Network Construction](https://www.dcc.fc.up.pt/~pribeiro/aulas/ns2122/handouts.html#construction) during this part of the course, and specifically the slides for that section: [Network Construction (selected slides from J. Leskovec and L. Lacasa)](https://www.dcc.fc.up.pt/~pribeiro/aulas/ns2122/10_netconstruction.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33276594-d0c7-4a44-8637-a5426107134e",
   "metadata": {},
   "source": [
    "# Setting up Graphistry\n",
    "\n",
    "First let's setup a network visualization tool to help us evaluate what we are building. Throughout this part of the course we will be using `pygraphistry` and [Graphistry Hub](https://hub.graphistry.com/) [https://hub.graphistry.com/](https://hub.graphistry.com/) to visualize networks. Both are free for personal use and are powerful for visualizing networks large and small.\n",
    "\n",
    "You can [signup](https://hub.graphistry.com/accounts/signup/) for a Graphistry account at [https://hub.graphistry.com/accounts/signup/](https://hub.graphistry.com/accounts/signup/). <b>You should use a username/password/email to get the required credentials</b>, although after that you can login with your Github or Google account.\n",
    "\n",
    "<center><img src=\"images/graphistry_hub_registration.png\" /></center>\n",
    "\n",
    "Retain and use your credentials in the login form and in the environment variables in the next cell below. You should set the `GRAPHISTRY_USERNAME` and `GRAPHISTRY_PASSWORD` variables in the `env/graphistry.env` file, and then restart this docker container to pickup the new values.\n",
    "\n",
    "<center><img src=\"images/graphistry_hub_homepage.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94962741-ff18-417f-a218-52e6a61f5a0e",
   "metadata": {},
   "source": [
    "# ETL for a Simple, Natural Graph: High-energy Physics Theory Citation Network\n",
    "\n",
    "We are going to start out by building a knowledge graph from an existing edge list and then add properties to it. We'll be using the [High-energy physics theory citation network](https://snap.stanford.edu/data/cit-HepTh.html) from [Stanford SNAP](https://snap.stanford.edu/index.html). SNAP has many large network datasets available in the [Stanford Large Network Dataset Collection](https://snap.stanford.edu/data/).\n",
    "\n",
    "The dataset includes the following files, which we will combine:\n",
    "\n",
    "* [Citation graph edge list](https://snap.stanford.edu/data/cit-HepTh.txt.gz) contains node ID pairs. Node IDs are standard paper identifiers. This will build the core structure of our network.\n",
    "* [Paper metadata](cit-HepTh-abstracts.tar.gz) including abstracts. This will add propertis to our network.\n",
    "* [Publishing dates on arXiv](https://snap.stanford.edu/data/cit-HepTh-dates.txt.gz) will make our citation network a temporal [Directed-Acyclic-Graph (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph) since one paper can't cite another before it is written and there are no reciprocal edges. While we don't focus on this, it does affect our analysis.\n",
    "\n",
    "## Dataset Citation\n",
    "\n",
    "```\n",
    "Paper: hep-th/0002031\n",
    "From: Maulik K. Parikh \n",
    "Date: Fri, 4 Feb 2000 17:04:51 GMT   (10kb)\n",
    "\n",
    "Title: Confinement and the AdS/CFT Correspondence\n",
    "Authors: D. S. Berman and Maulik K. Parikh\n",
    "Comments: 12 pages, 1 figure, RevTeX\n",
    "Report-no: SPIN-1999/25, UG-1999/42\n",
    "Journal-ref: Phys.Lett. B483 (2000) 271-276\n",
    "\\\\\n",
    "  We study the thermodynamics of the confined and unconfined phases of\n",
    "superconformal Yang-Mills in finite volume and at large N using the AdS/CFT\n",
    "correspondence. We discuss the necessary conditions for a smooth phase\n",
    "crossover and obtain an N-dependent curve for the phase boundary.\n",
    "\\\\\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3c191a7-0372-4723-9909-cefd2afc61a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variable setup\n",
    "GRAPHISTRY_USERNAME = os.getenv(\"GRAPHISTRY_USERNAME\")\n",
    "GRAPHISTRY_PASSWORD = os.getenv(\"GRAPHISTRY_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06e92082-faad-40a2-836e-ca7f98c5f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphistry.register(\n",
    "    api=3,\n",
    "    username=GRAPHISTRY_USERNAME,\n",
    "    password=GRAPHISTRY_PASSWORD,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09972cd9-b909-445f-8b78-94d1978b4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Graphistry\n",
    "GRAPHISTRY_PARAMS = {\n",
    "    \"play\": 600,\n",
    "    \"pointOpacity\": 0.7,\n",
    "    \"edgeOpacity\": 0.3,\n",
    "    \"edgeCurvature\": 0.3,\n",
    "    \"showArrows\": True,\n",
    "    \"gravity\": 0.5,\n",
    "}\n",
    "FAVICON_URL = \"https://graphlet.ai/assets/icons/favicon.ico\"\n",
    "LOGO = {\"url\": \"https://graphlet.ai/assets/Branding/Graphlet%20AI.svg\", \"dimensions\": {\"maxWidth\": 100, \"maxHeight\": 100}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b3cf16-524f-49a0-9865-eb5924e357d1",
   "metadata": {},
   "source": [
    "## `NetworkX` on PyPi is `networkx` in code is `nx`.\n",
    "\n",
    "The convention we used above is to load NetworkX via `import networkx as nx` so we can use the shorthand `nx` to call its classes and algorithms.\n",
    "\n",
    "## Numeric Node IDs\n",
    "\n",
    "What follows is a demonstration of _knowledge graph construction_, which we covered in the lecture. A node/edge list was provided, but the IDs are not sequential... which a network sampling tool I hope to use called [littleballoffur](https://github.com/benedekrozemberczki/littleballoffur) requires. In fact many graph libraries require sequential IDs. We must transform the graph IDs, create a mapping back and forth and annotate the nodes with properties for both IDs.\n",
    "\n",
    "## Build a Directional Graph (nx.DiGraph) from a CSV\n",
    "\n",
    "The edge list is a `#` commented, space-delimited CSV. We will parse it, assign sequential IDs and build a [`nx.DiGraph`](https://networkx.org/documentation/stable/reference/classes/digraph.html).\n",
    "\n",
    "### Download the Citation Edge List\n",
    "\n",
    "First, we download the edge list and build the structure of the network: `(paper)-cited->(paper)`. Note that we cache the edge list so you can edit the code without having to re-download the data.\n",
    "\n",
    "The edge list is located at [https://snap.stanford.edu/data/cit-HepTh.txt.gz](https://snap.stanford.edu/data/cit-HepTh.txt.gz) and is stored in `data/cit-HepTh.txt.gz`. We will read the file in its compressed state via the `gzip` builtin library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a4b88f4-deaf-444a-b90f-10bd9b7dad4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing citation graph edge file data/cit-HepTh.txt.gz\n"
     ]
    }
   ],
   "source": [
    "# Download and load edges (citations) from `cit-HepTh.txt.gz`\n",
    "edge_path = \"data/cit-HepTh.txt.gz\"\n",
    "gzip_content = None\n",
    "\n",
    "if os.path.exists(edge_path):\n",
    "    print(f\"Using existing citation graph edge file {edge_path}\")\n",
    "    gzip_content = open(edge_path, \"rb\")\n",
    "else:\n",
    "    print(\"Fetching citation graph edge file ...\")\n",
    "    response = requests.get(f\"https://snap.stanford.edu/{edge_path}\")\n",
    "    gzip_content = io.BytesIO(response.content)\n",
    "\n",
    "    print(\"Writing edge list to file {edge_path}\")\n",
    "    with open(edge_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "        print(f\"Wrote downloaded edge file to {edge_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b1ba635-1606-4e15-9b2f-795b79e44753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Directed graph (each unordered pair of nodes is saved once): Cit-HepTh.txt \n",
      "# Paper citation network of Arxiv High Energy Physics Theory category\n",
      "# Nodes: 27770 Edges: 352807\n",
      "# FromNodeId\tToNodeId\n",
      "1001\t9304045\n",
      "1001\t9308122\n",
      "1001\t9309097\n",
      "1001\t9311042\n",
      "1001\t9401139\n",
      "1001\t9404151\n",
      "\n",
      "gzip: stdout: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "# Check the top 10 lines of our gzip text file\n",
    "!zcat data/cit-HepTh.txt.gz | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88106b1c-c9a1-4ebe-82e2-a54d2b31df9e",
   "metadata": {},
   "source": [
    "### Graph and Identifier Setup\n",
    "\n",
    "We create directional a [`nx.DiGraph`](https://networkx.org/documentation/stable/reference/classes/digraph.html) because citations are inherently directional: from citer to cited. Note that whether we model them this way or not, citation graphs are temporal networks. The citing paper's publishing date must fall after cited paper's publishing date. We'll load publishing dates below.\n",
    "\n",
    "We need to setup `file_to_net` and `net_to_file` dictionaries to map back and forth between the file format's identifiers and or own sequential identifiers we'll be assigning starting with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78b66727-06b4-4c72-b02a-1caf2a66242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a directed graph\n",
    "G = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71c3f84c-c441-46f1-988b-7bca8c561a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create sequential IDs starting from 0 for littleballoffur and DGL\n",
    "file_to_net: Dict[int, int] = {}\n",
    "net_to_file: Dict[int, int] = {}\n",
    "current_idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181ae24c-b54d-4a5a-b286-c5f0288eaab4",
   "metadata": {},
   "source": [
    "### From Text to Graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cd66174-f00d-448d-be65-b68787ea7adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network structure ...\n",
      "Network built! It contains 27,770 nodes and 352,807 edges.\n"
     ]
    }
   ],
   "source": [
    "# Decompress the gzip content and build the edge list for our network\n",
    "print(\"Building network structure ...\")\n",
    "\n",
    "# Note we reuse the `gzip_content` variable from the download cell. This is a weird way to do it :)\n",
    "with gzip.GzipFile(fileobj=gzip_content) as f:\n",
    "\n",
    "    # Iterate through the lines, using the `line_number` as an `edge_id` below.\n",
    "    # They won't quite start at 0 owing to comments, but that's ok in the case of edges.\n",
    "    for line_number, line in enumerate(f):\n",
    "        line = line.decode(\"utf-8\")\n",
    "\n",
    "        # Ignore comment lines that start with '#'\n",
    "        if not line.startswith(\"#\"):\n",
    "            # Source (citing), desstination (cited) papers\n",
    "            citing_key, cited_key = line.strip().split(\"\\t\")\n",
    "\n",
    "            # The edge list makes the paper ID an int, stripping 0001001 to 1001, for example\n",
    "            citing_key, cited_key = int(citing_key), int(cited_key)\n",
    "\n",
    "            # If the either of the paper IDs don't exist, make one\n",
    "            for key in [citing_key, cited_key]:\n",
    "                if key not in file_to_net:\n",
    "                    # Build up an index that maps back and forth\n",
    "                    file_to_net[key] = current_idx\n",
    "                    net_to_file[current_idx] = key\n",
    "\n",
    "                    # Bump the current ID\n",
    "                    current_idx += 1\n",
    "\n",
    "            # print(f\"Citing key: {citing_key}, Cited key: {cited_key}\")\n",
    "            # print(f\"Mapped key: {file_to_net[citing_key]}, Mapped key: {file_to_net[cited_key]}\")\n",
    "\n",
    "            G.add_edge(file_to_net[citing_key], file_to_net[cited_key], edge_id=line_number)\n",
    "\n",
    "            # Conditionally set the keys on the nodes\n",
    "            G.nodes[file_to_net[citing_key]][\"file_id\"] = citing_key\n",
    "            G.nodes[file_to_net[citing_key]][\"sequential_id\"] = file_to_net[citing_key]\n",
    "\n",
    "            G.nodes[file_to_net[cited_key]][\"file_id\"] = cited_key\n",
    "            G.nodes[file_to_net[cited_key]][\"sequential_id\"] = file_to_net[cited_key]\n",
    "\n",
    "print(f\"Network built! It contains {G.number_of_nodes():,} nodes and {G.number_of_edges():,} edges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a5d993-5545-4886-966a-e622d2409215",
   "metadata": {},
   "source": [
    "## Node Properties from Abstract Metadata\n",
    "\n",
    "In addition to the edge list, SNAP provides the paper's essential metadata in another file, which we will load to provide node properties and text embeddings for citation graph.\n",
    "\n",
    "We are going to perform the following steps:\n",
    "\n",
    "1) Download and cache the metadata to `data/cit-HepTh-abstracts.tar.gz`. Note that 1 file corresponds to one paper node's metadata.\n",
    "2) Process the tarball file where one file corresponds to one node ID in the original file. See why we made or mappings `file_to_net` and `net_to_file`?\n",
    "3) Assign node properties by parsing the fields of the record using traditional information extraction with regular expressions.\n",
    "4) Use a sentence transformer paraphrase model to summarize the entire textual record and enable node comparison for journal label creation.\n",
    "\n",
    "### Downloading the Abstract Metadata\n",
    "\n",
    "Another file containing node metadata, including the abstracts, for about 90% of nodes in this network is provided at [https://snap.stanford.edu/data/cit-HepTh-abstracts.tar.gz](https://snap.stanford.edu/data/cit-HepTh-abstracts.tar.gz), which we will save to `data/cit-HepTh-abstracts.tar.gz`. This code works just like the edge list download code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4609227a-1af3-41fd-9921-7715c11d9685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching paper abstracts ...\n",
      "Using existing paper abstracts file data/cit-HepTh-abstracts.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Download the abstracts from `cit-HepTh-abstracts.tar.gz`\n",
    "print(\"Fetching paper abstracts ...\")\n",
    "abstract_path = \"data/cit-HepTh-abstracts.tar.gz\"\n",
    "abstract_gzip_content = None\n",
    "\n",
    "if os.path.exists(abstract_path):\n",
    "    print(f\"Using existing paper abstracts file {abstract_path}\")\n",
    "    with open(abstract_path, \"rb\") as f:\n",
    "        abstract_gzip_content = io.BytesIO(f.read())\n",
    "else:\n",
    "    print(\"Downloading paper abbstracts ...\")\n",
    "    abstract_response = requests.get(f\"https://snap.stanford.edu/{abstract_path}\")\n",
    "    abstract_gzip_content = io.BytesIO(abstract_response.content)\n",
    "\n",
    "    print(f\"Downloading abstract file to {abstract_path}\")\n",
    "    with open(abstract_path, \"wb\") as f:\n",
    "        f.write(abstract_response.content)\n",
    "        print(f\"Wrote downloaded abstract file to {abstract_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ee4892-b011-4b3b-bb18-78633a297a3b",
   "metadata": {},
   "source": [
    "### Manually Parsing Node Metadata\n",
    "\n",
    "As a first pass let's use regular expressions in the Python `re` builtin library to extract each paper's fields so we can assign them as properties to our `nx.DiGraph` nodes.\n",
    "\n",
    "Here is what a couple of **test documents** look like. This is corresponds to two files in our abstract tarball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce09eff3-3189-45c5-b139-3d346bf97e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "\"\"\"------------------------------------------------------------------------------\n",
    "\\\\\n",
    "Paper: hep-th/9612115\n",
    "From: Asato Tsuchiya <tsuchiya@theory.kek.jp>\n",
    "Date: Wed, 11 Dec 1996 17:38:56 +0900   (20kb)\n",
    "Date (revised): Tue, 31 Dec 1996 01:06:34 +0900\n",
    "\n",
    "Title: A Large-N Reduced Model as Superstring\n",
    "Authors: N. Ishibashi, H. Kawai, Y. Kitazawa and A. Tsuchiya\n",
    "Comments: 29 pages, Latex, a footnote and references added, eq.(3.52)\n",
    "corrected, minor corrections\n",
    "Report-no: KEK-TH-503, TIT/HEP-357\n",
    "Journal-ref: Nucl.Phys. B498 (1997) 467-491\n",
    "\\\\\n",
    "A matrix model which has the manifest ten-dimensional N=2 super Poincare\n",
    "invariance is proposed. Interactions between BPS-saturated states are analyzed\n",
    "to show that massless spectrum is the same as that of type IIB string theory.\n",
    "It is conjectured that the large-N reduced model of ten-dimensional super\n",
    "Yang-Mills theory can be regarded as a constructive definition of this model\n",
    "and therefore is equivalent to superstring theory.\n",
    "\\\\\n",
    "\"\"\",\n",
    "    \"\"\"------------------------------------------------------------------------------\n",
    "\\\\\n",
    "Paper: hep-th/9711029\n",
    "From: John Schwarz <jhs@theory.caltech.edu>\n",
    "Date: Wed, 5 Nov 1997 17:30:55 GMT   (20kb)\n",
    "Date (revised v2): Thu, 6 Nov 1997 23:52:45 GMT   (21kb)\n",
    "\n",
    "Title: The Status of String Theory\n",
    "Author: John H. Schwarz\n",
    "Comments: 16 pages, latex, two figures; minor corrections, references added\n",
    "Report-no: CALT-68-2140\n",
    "\\\\\n",
    "There have been many remarkable developments in our understanding of\n",
    "superstring theory in the past few years, a period that has been described as\n",
    "``the second superstring revolution.'' Several of them are discussed here. The\n",
    "presentation is intended primarily for the benefit of nonexperts.\n",
    "\\\\\n",
    "\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f48a8b-95b3-404e-8658-7c9fd4002acd",
   "metadata": {},
   "source": [
    "### Structured Information Extraction with a Regex Helper\n",
    "\n",
    "Our extract function was created through trial and error using the [Pythex Regex Editor](https://pythex.org/). Paste the test documents where it says `Your test data` and try a couple of the patterns such as `r\"\"` above that where it says `Your regular expression`. It will show you where the patterns match in your data. A new section displays the matches within the text and a window on the right shows the text your regular expression will return via the list the `match.groups()` command returns.\n",
    "\n",
    "A few cycles of this and we have a clean extraction. In practice, I used more test records than this, which I've spared you in the interest of time :) Regular expressions are difficult to learn, but there are resources and **ChatGPT-4 is quite capable at writing regex!** It wrote many of the ones below.\n",
    "\n",
    "<center><img src=\"images/Pythex-Regex-Helper.png\" width=\"1000px\" /></center>\n",
    "\n",
    "I used Pythex to help write the `extract_paper_info(record)` method below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b66d7e1-9e7a-4f51-adc2-7d26538b5dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paper_info(record):\n",
    "    \"\"\"Extract structured information from the text of academic paper text records using regular expressions.\n",
    "\n",
    "    Note: I was written wholly or in part by ChatGPT4 on May 23, 2023.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty dictionary to hold the information\n",
    "    info = {}\n",
    "\n",
    "    # Match \"Paper\" field\n",
    "    paper_match = re.search(r\"Paper:\\s*(.*)\", record)\n",
    "    if paper_match:\n",
    "        info[\"Paper\"] = paper_match.group(1)\n",
    "\n",
    "    # # Match \"From\" field\n",
    "    # from_match = re.search(r\"From:\\s*(.*)\", record)\n",
    "    # if from_match:\n",
    "    #     info['From'] = from_match.group(1)\n",
    "\n",
    "    # Match \"From\" field\n",
    "    from_match = re.search(r\"From:\\s*([^<]*)<\", record)\n",
    "    if from_match:\n",
    "        info[\"From\"] = from_match.group(1).strip()\n",
    "\n",
    "    # Match \"Date\" field\n",
    "    date_match = re.search(r\"Date:\\s*(.*)(\\s*)(\\(\\d+kb\\))\", record)\n",
    "    if date_match:\n",
    "        info[\"Date\"] = date_match.group(1).strip()\n",
    "\n",
    "    # Match \"Title\" field\n",
    "    title_match = re.search(r\"Title:\\s*(.*)\", record)\n",
    "    if title_match:\n",
    "        info[\"Title\"] = title_match.group(1)\n",
    "\n",
    "    # Match \"Authors\" field\n",
    "    authors_match = re.search(r\"Authors:\\s*(.*)\", record)\n",
    "    if authors_match:\n",
    "        info[\"Authors\"] = authors_match.group(1)\n",
    "\n",
    "    # Match \"Comments\" field\n",
    "    comments_match = re.search(r\"Comments:\\s*(.*)\", record)\n",
    "    if comments_match:\n",
    "        info[\"Comments\"] = comments_match.group(1)\n",
    "\n",
    "    # Match \"Report-no\" field\n",
    "    report_no_match = re.search(r\"Report-no:\\s*(.*)\", record)\n",
    "    if report_no_match:\n",
    "        info[\"Report-no\"] = report_no_match.group(1)\n",
    "\n",
    "    # Match \"Journal-ref\" field\n",
    "    journal_ref_match = re.search(r\"Journal-ref:\\s*(.*)\", record)\n",
    "    if journal_ref_match:\n",
    "        info[\"Journal-ref\"] = journal_ref_match.group(1)\n",
    "\n",
    "    # Extract \"Abstract\" field\n",
    "    abstract_pattern = r\"Journal-ref:[^\\\\\\\\]*\\\\\\\\[\\n\\s]*(.*?)(?=\\\\\\\\)\"\n",
    "    abstract_match = re.search(abstract_pattern, record, re.DOTALL)\n",
    "    if abstract_match:\n",
    "        abstract = abstract_match.group(1)\n",
    "        abstract = abstract.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "        info[\"Abstract\"] = abstract.strip()\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f0c15-5080-4875-9c9a-9312ee9e5ab6",
   "metadata": {},
   "source": [
    "### Testing Our Information Extraction\n",
    "\n",
    "To develop the above I created the unit tests below. Inspect the values so you agree they work :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e92f08f8-aa75-44a7-8e9c-b941b7bf0d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "(doc1, doc2) = docs\n",
    "\n",
    "paper_info = extract_paper_info(doc1)\n",
    "# Get the paper ID part of the \"Paper\" field\n",
    "paper_id = int(paper_info.get(\"Paper\", \"\").split(\"/\")[-1])\n",
    "assert paper_info[\"Paper\"] == \"hep-th/9612115\"\n",
    "assert paper_id == 9612115\n",
    "\n",
    "paper_info = extract_paper_info(doc2)\n",
    "# Get the paper ID part of the \"Paper\" field\n",
    "paper_id = int(paper_info.get(\"Paper\", \"\").split(\"/\")[-1])\n",
    "assert paper_info[\"Paper\"] == \"hep-th/9711029\"\n",
    "assert paper_id == 9711029"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1786f68-848a-442a-9e96-ccd70debf27a",
   "metadata": {},
   "source": [
    "### Setting Node Properties\n",
    "\n",
    "With our information extraction function tested, we are ready to loop through the abstract metadata tarball's files `G.nodes()` and assign the extract function's fields as node fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a18b0860-2b80-4903-9d5b-1bc7c557d7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added metadata to 27,770 nodes, 1,785 were unknown.\n"
     ]
    }
   ],
   "source": [
    "hit_count, miss_count, matches = 0, 0, 0\n",
    "all_abstracts: List[str] = []\n",
    "abstracts: Dict[int, str] = {}\n",
    "paper_ids: List[int] = []\n",
    "# Decompress the gzip content, then work through the abstract files in the tarball\n",
    "with gzip.GzipFile(fileobj=abstract_gzip_content) as f:\n",
    "    with tarfile.open(fileobj=f, mode=\"r|\") as tar:\n",
    "        for member in tar:\n",
    "            abstract_file = tar.extractfile(member)\n",
    "            if abstract_file:\n",
    "                content = abstract_file.read().decode(\"utf-8\")\n",
    "\n",
    "                paper_id = int(os.path.basename(member.name).split(\".\")[0])\n",
    "\n",
    "                # We can also parse and use those values directly or embed field-wise\n",
    "                paper_info = extract_paper_info(content)\n",
    "                if paper_info:\n",
    "                    abstract_paper_id = paper_info.get(\"Paper\", \"\").split(\"/\")[-1]\n",
    "                    if paper_id != int(abstract_paper_id):\n",
    "                        matches += 1\n",
    "                        print(f\"Paper ID {paper_id} != {abstract_paper_id}\")\n",
    "\n",
    "                    # Get the paper ID part of the \"Paper\" field\n",
    "                    if paper_id in file_to_net and file_to_net[paper_id] in G:\n",
    "                        for field, value in paper_info.items():\n",
    "                            G.nodes[file_to_net[paper_id]][field] = value\n",
    "\n",
    "                        abstracts[paper_id] = content\n",
    "                        all_abstracts.append(content)\n",
    "                        paper_ids.append(paper_id)\n",
    "\n",
    "                        hit_count += 1\n",
    "\n",
    "                    else:\n",
    "                        # Add isolated nodes if paper_id isn't in G\n",
    "                        miss_count += 1\n",
    "                        # We could do this for some use cases to create isolated nodes. Not all graphs are connected. See Part 2, Network Science.\n",
    "                        # G.add_node(file_to_net[paper_id], **paper_info)\n",
    "\n",
    "# Now `G` is a property graph representing the \"High-energy physics theory citation network\" dataset\n",
    "print(f\"Added metadata to {hit_count:,} nodes, {miss_count:,} were unknown.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7767a17-0955-4a8f-a34e-9996fcb54b90",
   "metadata": {},
   "source": [
    "## Temporal Networks\n",
    "\n",
    "Our citation graph is a temporal network. Temporal networks have a determined sequence in which nodes are added to the graph - just as real networks evolve. Network science and graph machine learning for temporal networks that don't take time into account can result in incorrect analyses and inaccurate machine learning inference.\n",
    "\n",
    "<center><img src=\"images/Temporal-vs-static-networks-A-The-sequence-of-contacts-among-three-nodes-capturing_W640.jpg\" width=\"800px\" /></center>\n",
    "<center>Image source: <a href=\"https://www.researchgate.net/publication/305492361_The_fundamental_advantages_of_temporal_networks\">The fundamental advantages of temporal networks, Li et al., 2016</a></center>\n",
    "\n",
    "### Downloading Publishing Dates\n",
    "\n",
    "The timestamps are available at [https://snap.stanford.edu/data/cit-HepTh-dates.txt.gz](https://snap.stanford.edu/data/cit-HepTh-dates.txt.gz) and are downloaded to `data/cit-HepTh-dates.txt.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d35f49a5-d300-4c10-8cef-ccbd664257fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing paper dates file data/cit-HepTh-dates.txt.gz\n"
     ]
    }
   ],
   "source": [
    "# Download and load edges (citations) from `cit-HepTh.txt.gz`\n",
    "dates_path = \"data/cit-HepTh-dates.txt.gz\"\n",
    "date_gzip_content = None\n",
    "\n",
    "if os.path.exists(dates_path):\n",
    "    print(f\"Using existing paper dates file {dates_path}\")\n",
    "    date_gzip_content = open(dates_path, \"rb\")\n",
    "else:\n",
    "    print(\"Downloading paper publishing dates ...\")\n",
    "    date_response = requests.get(f\"https://snap.stanford.edu/{dates_path}\")\n",
    "    date_gzip_content = io.BytesIO(date_response.content)\n",
    "\n",
    "    with open(dates_path, \"wb\") as f:\n",
    "        f.write(date_response.content)\n",
    "        print(\"Wrote downloaded publishing dates file to {dates_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dc8390-c714-4c75-b702-5f3c243e1d0d",
   "metadata": {},
   "source": [
    "### Adding a Temporal Property\n",
    "\n",
    "Publishing dates go under the `Published` node property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acaca8e5-e0c1-4fbc-ba9a-a514218acccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding publising dates ...\n"
     ]
    }
   ],
   "source": [
    "# Decompress the gzip content and add a \"published\" date property to our nodes\n",
    "print(\"Adding publising dates ...\")\n",
    "with gzip.GzipFile(fileobj=date_gzip_content) as f:\n",
    "    for line in f:\n",
    "        line = line.decode(\"utf-8\")\n",
    "        # Ignore lines that start with '#'\n",
    "        if not line.startswith(\"#\"):\n",
    "            paper_id, iso_date = line.strip().split(\"\\t\")\n",
    "\n",
    "            # The edge list makes the paper ID an int, stripping 0001001 to 1001, for example\n",
    "            paper_id = int(paper_id)\n",
    "\n",
    "            if paper_id in file_to_net and file_to_net[paper_id] in G:\n",
    "                # Add a UTC timestamp for the data\n",
    "                G.nodes[file_to_net[paper_id]][\"Published\"] = calendar.timegm(\n",
    "                    date.fromisoformat(iso_date).timetuple()\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a45ded-092d-49e7-bcbd-55018997b7e7",
   "metadata": {},
   "source": [
    "### Test our Network Build\n",
    "\n",
    "Let's make sure everything built as expected..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2a6d03a-4bbe-4997-b3fd-6ee1f728c927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"file_id\": 1001,\n",
      "    \"sequential_id\": 0,\n",
      "    \"Paper\": \"hep-th/0001001\",\n",
      "    \"From\": \"Paul S. Aspinwall\",\n",
      "    \"Date\": \"Sat, 1 Jan 2000 00:02:31 GMT\",\n",
      "    \"Title\": \"Compactification, Geometry and Duality: N=2\",\n",
      "    \"Authors\": \"Paul S. Aspinwall\",\n",
      "    \"Comments\": \"82 pages, 8 figures, LaTeX2e, TASI99, refs added and some typos fixed\",\n",
      "    \"Report-no\": \"DUKE-CGTP-00-01\",\n",
      "    \"Published\": 946684800\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# What is the first node?\n",
    "test_node = G.nodes[0]\n",
    "\n",
    "assert test_node[\"sequential_id\"] == 0\n",
    "assert test_node[\"file_id\"] == 1001\n",
    "\n",
    "print(json.dumps(test_node, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5228d39-a3d1-40ef-ab76-f1bf99b79c37",
   "metadata": {},
   "source": [
    "## Abstract Embeddings\n",
    "\n",
    "In addition to parsing the data, we will embed the entire record. First we create a utility to embed string columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0043881c-c4d7-4357-b284-7e65ffb2063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa37dfea-c8da-429d-83df-011c3fbc755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrase_model = SentenceTransformer(\"sentence-transformers/paraphrase-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01bb1116-6b21-45f7-94f2-49fe6af97658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_paper_info(\n",
    "    records: Union[str, List[str]], convert_to_tensor=True\n",
    ") -> Union[np.ndarray, Tensor]:\n",
    "    if records and isinstance(records, str):\n",
    "        records = [records]\n",
    "    return paraphrase_model.encode(records, convert_to_tensor=convert_to_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a38e4d-40f6-4484-be82-743f496cfdf8",
   "metadata": {},
   "source": [
    "Then we embed all the nodes' metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0734266-b939-4438-8711-f675bfa93479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the abstracts for GNN features. Embedding is a generic approach for retrieval as well.\n",
    "# Note: NetworkX can't save lists in GEXF format, so we'll JSONize the list & save the embeddings separately.\n",
    "embedded_abstracts: np.ndarray = None\n",
    "if os.path.exists(\"data/embedded_abstracts.npy\"):\n",
    "    embedded_abstracts = np.load(\"data/embedded_abstracts.npy\")\n",
    "else:\n",
    "    embedded_abstracts = embed_paper_info(all_abstracts, convert_to_tensor=False)\n",
    "    np.save(\"data/embedded_abstracts.npy\", embedded_abstracts)\n",
    "\n",
    "node_embedding_dict: Dict[int, List[float]] = {}\n",
    "if os.path.exists(\"data/node_embedding_dict.json.gz\"):\n",
    "    node_embedding_dict = json.load(\n",
    "        gzip.GzipFile(\"data/node_embedding_dict.json.gz\", \"r\"),\n",
    "        # encoding=\"utf-8\",\n",
    "    )\n",
    "else:\n",
    "    for paper_id, emb in zip(paper_ids, embedded_abstracts):\n",
    "        assert emb.shape == (384,)\n",
    "\n",
    "        # Gephi assumes a list of floats is a time series, so we need to convert to a string\n",
    "        emb_list = emb.tolist()\n",
    "        G.nodes[file_to_net[paper_id]][\"Embedding-JSON\"] = json.dumps(emb_list)\n",
    "\n",
    "        node_embedding_dict[paper_id] = emb_list\n",
    "\n",
    "# Write the mapping from paper ID to embedding to JSON.\n",
    "# Note: All JSON keys are strings. We will have to int(key) to read the data back.\n",
    "json.dump(\n",
    "    node_embedding_dict,\n",
    "    io.TextIOWrapper(\n",
    "        gzip.GzipFile(\"data/node_embedding_dict.json.gz\", \"w\"),\n",
    "        encoding=\"utf-8\",\n",
    "    ),\n",
    "    indent=4,\n",
    "    sort_keys=True,\n",
    ")\n",
    "\n",
    "# Write the entire network using GEXF format - the date has to be in UTC format for this to work.\n",
    "nx.write_gexf(G, path=\"data/physics_embeddings.gexf\", prettyprint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6a32b3-319e-43ac-ab12-a4d906018c2d",
   "metadata": {},
   "source": [
    "# Label Making and K-Nearest-Neighbors (KNN) Graph Building\n",
    "\n",
    "Now we are going to build a pandas `DataFrame` or `pd.DataFrame` of our nodes so we can create clean labels for our journals. These will serve as labels for our machine-learning tasks using this network.\n",
    "\n",
    "In this section, we will also demonstrate another method of building a network - K-nearest-neighbors construction.\n",
    "\n",
    "## Building a Node `pd.DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65ea0cd4-6b8d-40b6-a3d9-af30157e999b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node</th>\n",
       "      <th>file_id</th>\n",
       "      <th>sequential_id</th>\n",
       "      <th>Paper</th>\n",
       "      <th>From</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Report-no</th>\n",
       "      <th>Published</th>\n",
       "      <th>Journal-ref</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>hep-th/0001001</td>\n",
       "      <td>Paul S. Aspinwall</td>\n",
       "      <td>Sat, 1 Jan 2000 00:02:31 GMT</td>\n",
       "      <td>Compactification, Geometry and Duality: N=2</td>\n",
       "      <td>Paul S. Aspinwall</td>\n",
       "      <td>82 pages, 8 figures, LaTeX2e, TASI99, refs add...</td>\n",
       "      <td>DUKE-CGTP-00-01</td>\n",
       "      <td>946684800.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9304045</td>\n",
       "      <td>1</td>\n",
       "      <td>hep-th/9304045</td>\n",
       "      <td></td>\n",
       "      <td>Sun, 11 Apr 93 12:29:30 -0500</td>\n",
       "      <td>Generalized Calabi-Yau Manifolds and the Mirro...</td>\n",
       "      <td>P. Candelas, E. Derrick and L. Parkes</td>\n",
       "      <td>39 pages, plain TeX</td>\n",
       "      <td>CERN-TH.6831/93, UTTG-24-92</td>\n",
       "      <td></td>\n",
       "      <td>Nucl.Phys. B407 (1993) 115-154</td>\n",
       "      <td>We describe the mirror of the Z orbifold as a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9308122</td>\n",
       "      <td>2</td>\n",
       "      <td>hep-th/9308122</td>\n",
       "      <td></td>\n",
       "      <td>Thu, 26 Aug 93 14:09:47 SET</td>\n",
       "      <td>Mirror Symmetry, Mirror Map and Applications t...</td>\n",
       "      <td>S. Hosono, A. Klemm, S. Theisen</td>\n",
       "      <td>59 pages. Some changes in the references, a fe...</td>\n",
       "      <td>HUTMP-93/0801, LMU-TPW-93-22</td>\n",
       "      <td></td>\n",
       "      <td>Commun.Math.Phys. 167 (1995) 301-350</td>\n",
       "      <td>Mirror Symmetry, Picard-Fuchs equations and in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9309097</td>\n",
       "      <td>3</td>\n",
       "      <td>hep-th/9309097</td>\n",
       "      <td></td>\n",
       "      <td>Fri, 17 Sep 93 17:18:41 EDT</td>\n",
       "      <td>Calabi-Yau Moduli Space, Mirror Manifolds and ...</td>\n",
       "      <td>P.S. Aspinwall, B.R. Greene and D.R. Morrison</td>\n",
       "      <td>74 pages (with 20 figures)</td>\n",
       "      <td>IASSNS-HEP-93/38, CNLS-93/1236</td>\n",
       "      <td></td>\n",
       "      <td>Nucl.Phys. B416 (1994) 414-480</td>\n",
       "      <td>We analyze the moduli spaces of Calabi-Yau thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9311042</td>\n",
       "      <td>4</td>\n",
       "      <td>hep-th/9311042</td>\n",
       "      <td></td>\n",
       "      <td>Sun, 7 Nov 93 23:00:47 EST</td>\n",
       "      <td>Measuring Small Distances in N=2 Sigma Models</td>\n",
       "      <td>Paul S. Aspinwall, Brian R. Greene, and David ...</td>\n",
       "      <td>62 pp. with 6 figs., LaTeX and epsf.tex</td>\n",
       "      <td>IASSNS-HEP-93/49</td>\n",
       "      <td></td>\n",
       "      <td>Nucl.Phys. B420 (1994) 184-242</td>\n",
       "      <td>We analyze global aspects of the moduli space ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   node  file_id  sequential_id           Paper               From  \\\n",
       "0     0     1001              0  hep-th/0001001  Paul S. Aspinwall   \n",
       "1     1  9304045              1  hep-th/9304045                      \n",
       "2     2  9308122              2  hep-th/9308122                      \n",
       "3     3  9309097              3  hep-th/9309097                      \n",
       "4     4  9311042              4  hep-th/9311042                      \n",
       "\n",
       "                            Date  \\\n",
       "0   Sat, 1 Jan 2000 00:02:31 GMT   \n",
       "1  Sun, 11 Apr 93 12:29:30 -0500   \n",
       "2    Thu, 26 Aug 93 14:09:47 SET   \n",
       "3    Fri, 17 Sep 93 17:18:41 EDT   \n",
       "4     Sun, 7 Nov 93 23:00:47 EST   \n",
       "\n",
       "                                               Title  \\\n",
       "0        Compactification, Geometry and Duality: N=2   \n",
       "1  Generalized Calabi-Yau Manifolds and the Mirro...   \n",
       "2  Mirror Symmetry, Mirror Map and Applications t...   \n",
       "3  Calabi-Yau Moduli Space, Mirror Manifolds and ...   \n",
       "4      Measuring Small Distances in N=2 Sigma Models   \n",
       "\n",
       "                                             Authors  \\\n",
       "0                                  Paul S. Aspinwall   \n",
       "1              P. Candelas, E. Derrick and L. Parkes   \n",
       "2                    S. Hosono, A. Klemm, S. Theisen   \n",
       "3      P.S. Aspinwall, B.R. Greene and D.R. Morrison   \n",
       "4  Paul S. Aspinwall, Brian R. Greene, and David ...   \n",
       "\n",
       "                                            Comments  \\\n",
       "0  82 pages, 8 figures, LaTeX2e, TASI99, refs add...   \n",
       "1                                39 pages, plain TeX   \n",
       "2  59 pages. Some changes in the references, a fe...   \n",
       "3                         74 pages (with 20 figures)   \n",
       "4            62 pp. with 6 figs., LaTeX and epsf.tex   \n",
       "\n",
       "                        Report-no    Published  \\\n",
       "0                 DUKE-CGTP-00-01  946684800.0   \n",
       "1     CERN-TH.6831/93, UTTG-24-92                \n",
       "2    HUTMP-93/0801, LMU-TPW-93-22                \n",
       "3  IASSNS-HEP-93/38, CNLS-93/1236                \n",
       "4                IASSNS-HEP-93/49                \n",
       "\n",
       "                            Journal-ref  \\\n",
       "0                                         \n",
       "1        Nucl.Phys. B407 (1993) 115-154   \n",
       "2  Commun.Math.Phys. 167 (1995) 301-350   \n",
       "3        Nucl.Phys. B416 (1994) 414-480   \n",
       "4        Nucl.Phys. B420 (1994) 184-242   \n",
       "\n",
       "                                            Abstract  \n",
       "0                                                     \n",
       "1  We describe the mirror of the Z orbifold as a ...  \n",
       "2  Mirror Symmetry, Picard-Fuchs equations and in...  \n",
       "3  We analyze the moduli spaces of Calabi-Yau thr...  \n",
       "4  We analyze global aspects of the moduli space ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract nodes and their attributes into a list of dictionaries\n",
    "node_data = [{**{\"node\": node}, **attr} for node, attr in G.nodes(data=True)]\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "node_df = pd.DataFrame(node_data)\n",
    "\n",
    "# Cleanup\n",
    "node_df.fillna(\"\", inplace=True)\n",
    "\n",
    "node_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e83edc4-87f5-483b-874b-e4715569dbb0",
   "metadata": {},
   "source": [
    "## Clustering `Journal-ref`\n",
    "\n",
    "Taking a look at the field representing the journal a paper appeared in, we have a problem if we want to use this field as a label for a categorical classification... this is a fuzzy string problem, not a regular expression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d92b9e53-2e5b-4896-998f-5661722acb21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18147          Nucl.Phys. B388 (1992) 539-569\n",
       "12226                                        \n",
       "19902          Phys.Lett. B532 (2002) 297-304\n",
       "25669                                        \n",
       "7449             Phys.Lett. A212 (1996) 22-28\n",
       "13628         AIP Conf.Proc. 453 (1998) 49-52\n",
       "22455                                        \n",
       "12328             Nonlinearity 13 (2000) 2163\n",
       "8844       Int.J.Mod.Phys. A17 (2002) 625-642\n",
       "6681     Int.J.Mod.Phys. A11 (1996) 3049-3096\n",
       "Name: Journal-ref, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_df[\"Journal-ref\"].sample(n=10).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ab1d4-98e8-458e-80eb-1226342d7a6d",
   "metadata": {},
   "source": [
    "### Embedding `Title`, `Abstract` and `Journal-ref`\n",
    "\n",
    "That's ok! We will embed the `Journal-ref` field and cluster it to arrive at our class labels for each journal. This will give us a head start on presenting network construction using K-Nearest-Neighbors in the next section :)\n",
    "\n",
    "#### Simplify `Journal-ref` for Clustering\n",
    "\n",
    "What if we remove all the dates entirely? This should help us find titles :) While I am demonstrating vector operations, we might just use a Bag-of-Words model :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7d78186-7b6a-47f3-b87f-3a98918b8d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         \n",
       "1             Nucl.Phys. B\n",
       "2        Commun.Math.Phys.\n",
       "3             Nucl.Phys. B\n",
       "4             Nucl.Phys. B\n",
       "               ...        \n",
       "27765    Int.J.Theor.Phys.\n",
       "27766        J.Phys. A  LL\n",
       "27767          Phys.Rev. D\n",
       "27768         Nucl.Phys. B\n",
       "27769                     \n",
       "Name: Journal-ref-Letters, Length: 27770, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all non-text characters from Journal-ref to make it cluster better\n",
    "node_df[\"Journal-ref-Letters\"] = node_df[\"Journal-ref\"].str.replace(r\"[^a-zA-Z. ]\", \"\", regex=True).str.strip()\n",
    "node_df[\"Journal-ref-Letters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45bf4e54-40a5-4dc9-b77b-86e4ff42d17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded Title using paraphrase-MiniLM-L6-v2\n",
      "Embedded Title in all-MiniLM-L6-v2\n",
      "Embedded Abstract using paraphrase-MiniLM-L6-v2\n",
      "Embedded Abstract in all-MiniLM-L6-v2\n",
      "Embedded Journal-ref using paraphrase-MiniLM-L6-v2\n",
      "Embedded Journal-ref in all-MiniLM-L6-v2\n",
      "Embedded Journal-ref-Letters using paraphrase-MiniLM-L6-v2\n",
      "Embedded Journal-ref-Letters in all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Embed the relevant text columns Journal-ref and cluster them to produce journal class labels.\n",
    "paraphrase_model = SentenceTransformer(\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "all_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Embed these columns\n",
    "paraphrase_embeddings: Dict[str, np.ndarray] = {}\n",
    "all_embeddings: Dict[str, np.ndarray] = {}\n",
    "\n",
    "for column in [\"Title\", \"Abstract\", \"Journal-ref\", \"Journal-ref-Letters\"]:\n",
    "\n",
    "    # Paraphrase model\n",
    "    paraphrase_embeddings[column] = paraphrase_model.encode(node_df[column].tolist())\n",
    "    node_df[f\"{column}-Paraphrase-Embedding\"] = paraphrase_embeddings[column].tolist()\n",
    "    print(f\"Embedded {column} using paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "    # All model\n",
    "    all_embeddings[column] = all_model.encode(node_df[column].tolist())\n",
    "    node_df[f\"{column}-All-Embedding\"] = all_embeddings[column].tolist()   \n",
    "    print(f\"Embedded {column} in all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27846474-5bb9-4617-b223-d69074c81673",
   "metadata": {},
   "source": [
    "#### Assign Embeddings back to `node_df` `pd.DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52417cab-bb35-4667-90ff-4d605c920025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Journal-ref</th>\n",
       "      <th>Journal-ref-Letters</th>\n",
       "      <th>Journal-ref-Letters-All-Embedding</th>\n",
       "      <th>Journal-ref-Letters-Paraphrase-Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15966</th>\n",
       "      <td>Phys.Rev. D62 (2000) 053002</td>\n",
       "      <td>Phys.Rev. D</td>\n",
       "      <td>[-0.10500571876764297, -0.0022888958919793367,...</td>\n",
       "      <td>[-0.5412883758544922, -0.1532248705625534, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11063</th>\n",
       "      <td>Phys.Rev. D61 (2000) 085005</td>\n",
       "      <td>Phys.Rev. D</td>\n",
       "      <td>[-0.10500571876764297, -0.0022888958919793367,...</td>\n",
       "      <td>[-0.5412883758544922, -0.1532248705625534, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8951</th>\n",
       "      <td>Mod.Phys.Lett. A15 (2000) 931-938</td>\n",
       "      <td>Mod.Phys.Lett. A</td>\n",
       "      <td>[-0.12852071225643158, 0.0011828168062493205, ...</td>\n",
       "      <td>[-0.4199517071247101, -0.008999708108603954, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15168</th>\n",
       "      <td>Fortsch.Phys. 49 (2001) 419-431</td>\n",
       "      <td>Fortsch.Phys.</td>\n",
       "      <td>[-0.05704524368047714, -0.0031675263307988644,...</td>\n",
       "      <td>[-0.4551039934158325, 0.12124650180339813, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13488</th>\n",
       "      <td>Lett.Math.Phys. 54 (2000) 33-42</td>\n",
       "      <td>Lett.Math.Phys.</td>\n",
       "      <td>[-0.04240620881319046, -0.020049065351486206, ...</td>\n",
       "      <td>[-0.309988796710968, 0.02344934642314911, 0.20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25943</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[-0.11883842200040817, 0.04829862713813782, -0...</td>\n",
       "      <td>[0.15472735464572906, 0.18004417419433594, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16364</th>\n",
       "      <td>Commun.Math.Phys. 233 (2003) 355-381</td>\n",
       "      <td>Commun.Math.Phys.</td>\n",
       "      <td>[-0.017231842502951622, -0.05298766493797302, ...</td>\n",
       "      <td>[-0.5973626971244812, 0.048218220472335815, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5924</th>\n",
       "      <td>Int.J.Mod.Phys. A12 (1997) 1227-1236</td>\n",
       "      <td>Int.J.Mod.Phys. A</td>\n",
       "      <td>[-0.1208014115691185, 0.005057428497821093, -0...</td>\n",
       "      <td>[-0.2659245431423187, -0.1617102324962616, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7182</th>\n",
       "      <td>Commun.Math.Phys. 175 (1996) 297-318</td>\n",
       "      <td>Commun.Math.Phys.</td>\n",
       "      <td>[-0.017231842502951622, -0.05298766493797302, ...</td>\n",
       "      <td>[-0.5973626971244812, 0.048218220472335815, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11358</th>\n",
       "      <td>JHEP 0010 (2000) 017</td>\n",
       "      <td>JHEP</td>\n",
       "      <td>[-0.06988012045621872, 0.05596404895186424, 0....</td>\n",
       "      <td>[-0.5526080131530762, 0.3303481638431549, -0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18795</th>\n",
       "      <td>J.Math.Phys. 39 (1998) 5446-5457</td>\n",
       "      <td>J.Math.Phys.</td>\n",
       "      <td>[-0.0685109943151474, -0.041765790432691574, -...</td>\n",
       "      <td>[-0.4498366117477417, -0.06066422164440155, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7895</th>\n",
       "      <td>Nucl. Phys. B439 (1995) 679-691</td>\n",
       "      <td>Nucl. Phys. B</td>\n",
       "      <td>[-0.1056145504117012, -0.024545304477214813, -...</td>\n",
       "      <td>[-0.8201866745948792, 0.3066941499710083, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4349</th>\n",
       "      <td>Nucl.Phys. B460 (1996) 615-631</td>\n",
       "      <td>Nucl.Phys. B</td>\n",
       "      <td>[-0.10561453551054001, -0.024545324966311455, ...</td>\n",
       "      <td>[-0.8201866149902344, 0.3066941797733307, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25619</th>\n",
       "      <td>Mod.Phys.Lett. A10 (1995) 2831</td>\n",
       "      <td>Mod.Phys.Lett. A</td>\n",
       "      <td>[-0.12852071225643158, 0.0011828168062493205, ...</td>\n",
       "      <td>[-0.4199517071247101, -0.008999708108603954, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4087</th>\n",
       "      <td>Nucl.Phys. B465 (1996) 521-539</td>\n",
       "      <td>Nucl.Phys. B</td>\n",
       "      <td>[-0.10561453551054001, -0.024545324966311455, ...</td>\n",
       "      <td>[-0.8201866149902344, 0.3066941797733307, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20142</th>\n",
       "      <td>JHEP 0205 (2002) 021</td>\n",
       "      <td>JHEP</td>\n",
       "      <td>[-0.06988012045621872, 0.05596404895186424, 0....</td>\n",
       "      <td>[-0.5526080131530762, 0.3303481638431549, -0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5216</th>\n",
       "      <td>Nucl.Phys. B384 (1992) 334-351</td>\n",
       "      <td>Nucl.Phys. B</td>\n",
       "      <td>[-0.1056145504117012, -0.024545304477214813, -...</td>\n",
       "      <td>[-0.8201866745948792, 0.3066941499710083, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15287</th>\n",
       "      <td>Phys.Rev. D63 (2001) 103511</td>\n",
       "      <td>Phys.Rev. D</td>\n",
       "      <td>[-0.10500571876764297, -0.0022888935636729, 0....</td>\n",
       "      <td>[-0.5412886142730713, -0.15322495996952057, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25556</th>\n",
       "      <td>Phys.Lett. B366 (1996) 113-116</td>\n",
       "      <td>Phys.Lett. B</td>\n",
       "      <td>[-0.06783752888441086, -0.021384548395872116, ...</td>\n",
       "      <td>[-0.3830682635307312, -0.231096088886261, 0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>J.Geom.Phys. 26 (1998) 272-290</td>\n",
       "      <td>J.Geom.Phys.</td>\n",
       "      <td>[-0.05511826276779175, -0.047520287334918976, ...</td>\n",
       "      <td>[-0.5620306730270386, -0.09416887909173965, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16175</th>\n",
       "      <td>Nucl.Phys. B623 (2002) 474-492</td>\n",
       "      <td>Nucl.Phys. B</td>\n",
       "      <td>[-0.10561453551054001, -0.024545324966311455, ...</td>\n",
       "      <td>[-0.8201866149902344, 0.3066941797733307, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11480</th>\n",
       "      <td>Nucl.Phys. B422 (1994) 237-257</td>\n",
       "      <td>Nucl.Phys. B</td>\n",
       "      <td>[-0.10561453551054001, -0.024545324966311455, ...</td>\n",
       "      <td>[-0.8201866149902344, 0.3066941797733307, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24492</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[-0.11883842200040817, 0.04829862713813782, -0...</td>\n",
       "      <td>[0.15472735464572906, 0.18004417419433594, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2598</th>\n",
       "      <td>Phys.Rev. D56 (1997) 3582-3590</td>\n",
       "      <td>Phys.Rev. D</td>\n",
       "      <td>[-0.10500571876764297, -0.0022888958919793367,...</td>\n",
       "      <td>[-0.5412883758544922, -0.1532248705625534, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21209</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[-0.11883842200040817, 0.04829862713813782, -0...</td>\n",
       "      <td>[0.15472735464572906, 0.18004417419433594, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19099</th>\n",
       "      <td>Phys.Rev. D67 (2003) 026004</td>\n",
       "      <td>Phys.Rev. D</td>\n",
       "      <td>[-0.10500571876764297, -0.0022888935636729, 0....</td>\n",
       "      <td>[-0.5412886142730713, -0.15322495996952057, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>Phys.Rev. D50 (1994) 6404-6411</td>\n",
       "      <td>Phys.Rev. D</td>\n",
       "      <td>[-0.10500571876764297, -0.0022888958919793367,...</td>\n",
       "      <td>[-0.5412883758544922, -0.1532248705625534, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3282</th>\n",
       "      <td>Nucl.Phys. B542 (1999) 139-156</td>\n",
       "      <td>Nucl.Phys. B</td>\n",
       "      <td>[-0.10561453551054001, -0.024545324966311455, ...</td>\n",
       "      <td>[-0.8201866149902344, 0.3066941797733307, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16131</th>\n",
       "      <td>JHEP 0103 (2001) 032</td>\n",
       "      <td>JHEP</td>\n",
       "      <td>[-0.06988012045621872, 0.05596404895186424, 0....</td>\n",
       "      <td>[-0.5526080131530762, 0.3303481638431549, -0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7887</th>\n",
       "      <td>Nucl.Phys. B535 (1998) 197-218</td>\n",
       "      <td>Nucl.Phys. B</td>\n",
       "      <td>[-0.10561453551054001, -0.024545324966311455, ...</td>\n",
       "      <td>[-0.8201866149902344, 0.3066941797733307, 0.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Journal-ref Journal-ref-Letters  \\\n",
       "15966           Phys.Rev. D62 (2000) 053002         Phys.Rev. D   \n",
       "11063           Phys.Rev. D61 (2000) 085005         Phys.Rev. D   \n",
       "8951      Mod.Phys.Lett. A15 (2000) 931-938    Mod.Phys.Lett. A   \n",
       "15168       Fortsch.Phys. 49 (2001) 419-431       Fortsch.Phys.   \n",
       "13488       Lett.Math.Phys. 54 (2000) 33-42     Lett.Math.Phys.   \n",
       "25943                                                             \n",
       "16364  Commun.Math.Phys. 233 (2003) 355-381   Commun.Math.Phys.   \n",
       "5924   Int.J.Mod.Phys. A12 (1997) 1227-1236   Int.J.Mod.Phys. A   \n",
       "7182   Commun.Math.Phys. 175 (1996) 297-318   Commun.Math.Phys.   \n",
       "11358                  JHEP 0010 (2000) 017                JHEP   \n",
       "18795      J.Math.Phys. 39 (1998) 5446-5457        J.Math.Phys.   \n",
       "7895        Nucl. Phys. B439 (1995) 679-691       Nucl. Phys. B   \n",
       "4349         Nucl.Phys. B460 (1996) 615-631        Nucl.Phys. B   \n",
       "25619        Mod.Phys.Lett. A10 (1995) 2831    Mod.Phys.Lett. A   \n",
       "4087         Nucl.Phys. B465 (1996) 521-539        Nucl.Phys. B   \n",
       "20142                  JHEP 0205 (2002) 021                JHEP   \n",
       "5216         Nucl.Phys. B384 (1992) 334-351        Nucl.Phys. B   \n",
       "15287           Phys.Rev. D63 (2001) 103511         Phys.Rev. D   \n",
       "25556        Phys.Lett. B366 (1996) 113-116        Phys.Lett. B   \n",
       "433          J.Geom.Phys. 26 (1998) 272-290        J.Geom.Phys.   \n",
       "16175        Nucl.Phys. B623 (2002) 474-492        Nucl.Phys. B   \n",
       "11480        Nucl.Phys. B422 (1994) 237-257        Nucl.Phys. B   \n",
       "24492                                                             \n",
       "2598         Phys.Rev. D56 (1997) 3582-3590         Phys.Rev. D   \n",
       "21209                                                             \n",
       "19099           Phys.Rev. D67 (2003) 026004         Phys.Rev. D   \n",
       "10101        Phys.Rev. D50 (1994) 6404-6411         Phys.Rev. D   \n",
       "3282         Nucl.Phys. B542 (1999) 139-156        Nucl.Phys. B   \n",
       "16131                  JHEP 0103 (2001) 032                JHEP   \n",
       "7887         Nucl.Phys. B535 (1998) 197-218        Nucl.Phys. B   \n",
       "\n",
       "                       Journal-ref-Letters-All-Embedding  \\\n",
       "15966  [-0.10500571876764297, -0.0022888958919793367,...   \n",
       "11063  [-0.10500571876764297, -0.0022888958919793367,...   \n",
       "8951   [-0.12852071225643158, 0.0011828168062493205, ...   \n",
       "15168  [-0.05704524368047714, -0.0031675263307988644,...   \n",
       "13488  [-0.04240620881319046, -0.020049065351486206, ...   \n",
       "25943  [-0.11883842200040817, 0.04829862713813782, -0...   \n",
       "16364  [-0.017231842502951622, -0.05298766493797302, ...   \n",
       "5924   [-0.1208014115691185, 0.005057428497821093, -0...   \n",
       "7182   [-0.017231842502951622, -0.05298766493797302, ...   \n",
       "11358  [-0.06988012045621872, 0.05596404895186424, 0....   \n",
       "18795  [-0.0685109943151474, -0.041765790432691574, -...   \n",
       "7895   [-0.1056145504117012, -0.024545304477214813, -...   \n",
       "4349   [-0.10561453551054001, -0.024545324966311455, ...   \n",
       "25619  [-0.12852071225643158, 0.0011828168062493205, ...   \n",
       "4087   [-0.10561453551054001, -0.024545324966311455, ...   \n",
       "20142  [-0.06988012045621872, 0.05596404895186424, 0....   \n",
       "5216   [-0.1056145504117012, -0.024545304477214813, -...   \n",
       "15287  [-0.10500571876764297, -0.0022888935636729, 0....   \n",
       "25556  [-0.06783752888441086, -0.021384548395872116, ...   \n",
       "433    [-0.05511826276779175, -0.047520287334918976, ...   \n",
       "16175  [-0.10561453551054001, -0.024545324966311455, ...   \n",
       "11480  [-0.10561453551054001, -0.024545324966311455, ...   \n",
       "24492  [-0.11883842200040817, 0.04829862713813782, -0...   \n",
       "2598   [-0.10500571876764297, -0.0022888958919793367,...   \n",
       "21209  [-0.11883842200040817, 0.04829862713813782, -0...   \n",
       "19099  [-0.10500571876764297, -0.0022888935636729, 0....   \n",
       "10101  [-0.10500571876764297, -0.0022888958919793367,...   \n",
       "3282   [-0.10561453551054001, -0.024545324966311455, ...   \n",
       "16131  [-0.06988012045621872, 0.05596404895186424, 0....   \n",
       "7887   [-0.10561453551054001, -0.024545324966311455, ...   \n",
       "\n",
       "                Journal-ref-Letters-Paraphrase-Embedding  \n",
       "15966  [-0.5412883758544922, -0.1532248705625534, 0.4...  \n",
       "11063  [-0.5412883758544922, -0.1532248705625534, 0.4...  \n",
       "8951   [-0.4199517071247101, -0.008999708108603954, 0...  \n",
       "15168  [-0.4551039934158325, 0.12124650180339813, -0....  \n",
       "13488  [-0.309988796710968, 0.02344934642314911, 0.20...  \n",
       "25943  [0.15472735464572906, 0.18004417419433594, 0.0...  \n",
       "16364  [-0.5973626971244812, 0.048218220472335815, -0...  \n",
       "5924   [-0.2659245431423187, -0.1617102324962616, -0....  \n",
       "7182   [-0.5973626971244812, 0.048218220472335815, -0...  \n",
       "11358  [-0.5526080131530762, 0.3303481638431549, -0.5...  \n",
       "18795  [-0.4498366117477417, -0.06066422164440155, 0....  \n",
       "7895   [-0.8201866745948792, 0.3066941499710083, 0.00...  \n",
       "4349   [-0.8201866149902344, 0.3066941797733307, 0.00...  \n",
       "25619  [-0.4199517071247101, -0.008999708108603954, 0...  \n",
       "4087   [-0.8201866149902344, 0.3066941797733307, 0.00...  \n",
       "20142  [-0.5526080131530762, 0.3303481638431549, -0.5...  \n",
       "5216   [-0.8201866745948792, 0.3066941499710083, 0.00...  \n",
       "15287  [-0.5412886142730713, -0.15322495996952057, 0....  \n",
       "25556  [-0.3830682635307312, -0.231096088886261, 0.09...  \n",
       "433    [-0.5620306730270386, -0.09416887909173965, 0....  \n",
       "16175  [-0.8201866149902344, 0.3066941797733307, 0.00...  \n",
       "11480  [-0.8201866149902344, 0.3066941797733307, 0.00...  \n",
       "24492  [0.15472735464572906, 0.18004417419433594, 0.0...  \n",
       "2598   [-0.5412883758544922, -0.1532248705625534, 0.4...  \n",
       "21209  [0.15472735464572906, 0.18004417419433594, 0.0...  \n",
       "19099  [-0.5412886142730713, -0.15322495996952057, 0....  \n",
       "10101  [-0.5412883758544922, -0.1532248705625534, 0.4...  \n",
       "3282   [-0.8201866149902344, 0.3066941797733307, 0.00...  \n",
       "16131  [-0.5526080131530762, 0.3303481638431549, -0.5...  \n",
       "7887   [-0.8201866149902344, 0.3066941797733307, 0.00...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's go with Journal-ref-Letters-Embedding below, less variance\n",
    "node_df[[\"Journal-ref\", \"Journal-ref-Letters\", \"Journal-ref-Letters-All-Embedding\", \"Journal-ref-Letters-Paraphrase-Embedding\"]].sample(30).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39622cf8-0814-43fe-86a6-b1680754610d",
   "metadata": {},
   "source": [
    "### Clustering `Journal-ref-Letters-Embedding`\n",
    "\n",
    "Now we will use [UMAP](https://umap-learn.readthedocs.io/en/latest/) via the [umap-learn](https://umap-learn.readthedocs.io/en/latest/basic_usage.html) PyPi library to reduce our data to 2 dimensions and then [DBSCAN](https://en.wikipedia.org/wiki/DBSCAN) [which workds with 2D data and infers circles aroud centrioids] via [scikit-learn]() to cluster it into journal names. Our final step is to nominate a real journal name for each cluster, which we will do manually as this course is about graph ML and this is NLP :)\n",
    "\n",
    "Note that while normally you need to [standardize data](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#standardscaler) to give it a normal distribution with something like [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), you do not need to do this when sentence encoding data with a sentence transformer as we have done.\n",
    "\n",
    "#### KMeans - Did Not Work :)\n",
    "\n",
    "KMeans didn't work, which is probably because I didn't know how to tune it effectively, but DBSCAN is fairly automatic and produced good clusters, so I used that. If you think about it though, KMeans starts with random centroids... and that is why it is splitting journals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0920893c-b620-4bac-bffa-32c419d1b880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cluster_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Apply to both embeddings\u001b[39;00m\n\u001b[1;32m      8\u001b[0m paraphrase_class_scores \u001b[38;5;241m=\u001b[39m km\u001b[38;5;241m.\u001b[39mfit_transform(node_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJournal-ref-Letters-Paraphrase-Embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m----> 9\u001b[0m paraphrase_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mcluster_scores\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m node_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJournal-ref-Letters-Paraphrase-KMeans\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m paraphrase_classes\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     12\u001b[0m all_class_scores \u001b[38;5;241m=\u001b[39m km\u001b[38;5;241m.\u001b[39mfit_transform(node_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJournal-ref-Letters-All-Embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster_scores' is not defined"
     ]
    }
   ],
   "source": [
    "km = KMeans(\n",
    "    n_clusters=20,\n",
    "    init=\"k-means++\",\n",
    "    max_iter=500,\n",
    ")\n",
    "\n",
    "# Apply to both embeddings\n",
    "paraphrase_class_scores = km.fit_transform(node_df[\"Journal-ref-Letters-Paraphrase-Embedding\"].tolist())\n",
    "paraphrase_classes = np.argmax(cluster_scores, axis=1)\n",
    "node_df[\"Journal-ref-Letters-Paraphrase-KMeans\"] = paraphrase_classes.tolist()\n",
    "\n",
    "all_class_scores = km.fit_transform(node_df[\"Journal-ref-Letters-All-Embedding\"].tolist())\n",
    "all_classes = np.argmax(cluster_scores, axis=1)\n",
    "node_df[\"Journal-ref-Letters-All-KMeans\"] = all_classes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5a83c5-dbbe-4ccf-b81d-07a57e33d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(paraphrase_classes, return_counts=True), paraphrase_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b814ead0-4090-420a-8956-24d9b9bad9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(all_classes, return_counts=True), all_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a52bee-c522-4553-9529-e36c91fc3c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    node_df[[\"Journal-ref\", \"Journal-ref-Letters\", \"Journal-ref-Letters-Paraphrase-KMeans\", \"Journal-ref-Letters-All-KMeans\"]]\n",
    "    .sample(30)\n",
    "    .sort_values(by=\"Journal-ref-Letters-Paraphrase-KMeans\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1b937d-67e5-458f-a8d0-079d941d6159",
   "metadata": {},
   "source": [
    "#### KMeans Did Not Work 2.0\n",
    "\n",
    "Although a nice way to begin, KMeans didn't work.\n",
    "\n",
    "**Q: Do any of you have more experience with KMeans than I do that can make it work to produce good journal clusters? :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac9ad4a-14b1-4e8c-bb06-309b309f8b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Dimension Reduction with UMAP\n",
    "reducer = umap.UMAP()\n",
    "reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d769fd55-8f03-4f09-a539-c277e2dc0eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_paraphrase_embeddings = reducer.fit_transform(node_df[\"Journal-ref-Letters-Paraphrase-Embedding\"].tolist())\n",
    "reduced_embeddings, reduced_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2d63b-b61b-4e28-98a1-cb920f01dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_all_embeddings = reducer.fit_transform(node_df[\"Journal-ref-Letters-All-Embedding\"].tolist())\n",
    "reduced_all_embeddings, reduced_all_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105337a3-7a0a-4c2d-a11e-2e5ec61b93a4",
   "metadata": {},
   "source": [
    "#### Clustering with DBSCAN\n",
    "\n",
    "ChatGPT and I say:\n",
    "\n",
    "> DBSCAN groups together closely packed 2-dimensional data points based on a specified distance measure and minimum number of points. It starts with an arbitrary point, expands clusters from suitable points, and identifies noise points that don't belong to any cluster. This makes it good for finding arbitrary shaped clusters and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca51b0b-2fcb-4aab-b6eb-c8b610b01e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Clustering with DBSCAN - you can search for the best hyperparameters\n",
    "dbscan = DBSCAN(eps=0.6, min_samples=200)\n",
    "clusters = dbscan.fit_predict(reduced_embeddings)\n",
    "\n",
    "# Count the clusters and their \n",
    "np.unique(clusters, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a7069e-2819-44ee-acbf-4d8f9df80c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a2cab6-e936-4716-a3d3-bc3ceb093043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "094f9728-a24b-4c89-98e7-b430e1b250ab",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25db28a3-a002-48c9-b073-037738379e70",
   "metadata": {},
   "source": [
    "### Nominating Labels for our Clusters\n",
    "\n",
    "Before we even evaluate our clusters by displaying them alongside our `Journal-ref` and `Journal-ref-Letters` in the `node_df` `pd.DataFrame`, I want to perform a trick for labeling clusters using a `GROUP BY`, [MapReduce](https://en.wikipedia.org/wiki/MapReduce) or [split-apply-combine strategy] as you prefer :)\n",
    "\n",
    "We will group by the cluster ID, take [pd.Series.value_counts](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html) [a `pd.Series` is a `pd.DataFrame` column] and assign the top value count as the label for the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97be23-179f-4283-b858-ee93ad15af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign clusters to the DataFrame\n",
    "node_df[\"Journal-ref-DBSCAN\"] = clusters\n",
    "\n",
    "node_df[[\"Journal-ref\", \"Journal-ref-Letters\", \"Journal-ref-DBSCAN\"]].groupby(\"Journal-ref-DBSCAN\").value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35832642-1143-4251-9366-bf3503c08e2b",
   "metadata": {},
   "source": [
    "#### ChatGPT and Me :)\n",
    "\n",
    "I could not get the above code to preview all the clusters... so I enlisted help.\n",
    "\n",
    "Note: this code was written by ChatGPT-4 after 4 attempts. It shows that the `Journal-ref-Count`s are too low to nominate a good name. This led me to allow `.` and ` ` [spaces] in the find/replace above that looked like:\n",
    "\n",
    "```python\n",
    "node_df[\"Journal-ref-Letters\"] = node_df[\"Journal-ref\"].str.replace(r\"[^a-zA-Z]\", \"\", regex=True).str.strip()\n",
    "```\n",
    "\n",
    "The `r\"[^a-zA-Z]\"` now looks like `r\"[^a-zA-Z. ]\"` to include periods and spaces:\n",
    "\n",
    "```python\n",
    "node_df[\"Journal-ref-Letters\"] = node_df[\"Journal-ref\"].str.replace(r\"[^a-zA-Z. ]\", \"\", regex=True).str.strip()\n",
    "```\n",
    "\n",
    "And then things looked good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed603631-3eed-4e1c-b6fc-3ab284dfb6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_values(group, n=5):\n",
    "    result = {}\n",
    "    for column in [\"Journal-ref\", \"Journal-ref-Letters\"]:\n",
    "        top_values = group[column].value_counts().nlargest(n)\n",
    "        result[f\"{column}-Value\"] = top_values.index.tolist()\n",
    "        result[f\"{column}-Count\"] = top_values.values.tolist()\n",
    "    return pd.Series(result)\n",
    "\n",
    "# Grouping by 'Journal-ref-DBSCAN' and applying the function\n",
    "top_values_df = node_df.groupby(\"Journal-ref-DBSCAN\").apply(get_top_n_values).reset_index()\n",
    "\n",
    "top_values_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54ebadd-71d5-4129-8ceb-16826377e867",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df[[\"Journal-ref\", \"Journal-ref-Letters\", \"Journal-ref-DBSCAN\", \"Journal-ref-KMeans\"]].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391cf07-c757-4972-9ce4-100b4cad98f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, each point has a cluster label, which could be -1 for noise points\n",
    "# node_df[\"Cluster\"] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128102e0-1194-45e0-b7fd-3b1b16046b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.DataFrame(data=reduced_embeddings, columns=['UMAP 1', 'UMAP 2'])\n",
    "plot_data['Cluster ID'] = clusters  # Add cluster labels to the DataFrame\n",
    "\n",
    "# Step 2: Plot the data\n",
    "# Initialize the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Use seaborn's scatterplot function to plot UMAP dimensions,\n",
    "# coloring the points by their cluster ID.\n",
    "# The 'palette' argument specifies the colors to use for each cluster.\n",
    "sns.scatterplot(\n",
    "    x='UMAP 1',  # X-axis: first dimension from UMAP\n",
    "    y='UMAP 2',  # Y-axis: second dimension from UMAP\n",
    "    hue='Cluster ID',  # Color by cluster ID\n",
    "    palette=sns.color_palette(\"hsv\", len(plot_data['Cluster ID'].unique())),  # Use a color palette with enough colors\n",
    "    data=plot_data,  # Data source\n",
    "    legend=\"full\",  # Display a legend\n",
    "    alpha=0.5  # Make points semi-transparent to see overlapping points\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "plt.title('Clusters in 2D UMAP space')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d473a3-6978-4692-81e7-d03d3aeea0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from transformers import RobertaTokenizer, RobertaModel\n",
    "# import torch\n",
    "\n",
    "# # Load the tokenizer and model\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "# def get_word_embeddings(sentences):\n",
    "#     # Tokenize all sentences in the input array\n",
    "#     inputs = tokenizer(list(sentences), return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "#     # Obtain output embeddings from the model\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "#     # The embeddings of the tokens are contained in the last_hidden_state\n",
    "#     embeddings = outputs.last_hidden_state\n",
    "\n",
    "#     # Convert the tensor to a numpy array\n",
    "#     embeddings = embeddings.detach().numpy()\n",
    "\n",
    "#     return embeddings\n",
    "\n",
    "# # Example usage\n",
    "# sentences = np.array([\"Hello, how are you?\", \"I am fine, thank you! This is a longer sentence.\"])\n",
    "# word_embeddings = get_word_embeddings(sentences)\n",
    "# word_embeddings, word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1aa154-9a17-424f-a14f-8cd109926f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assume sentence is a list of words and get_word_embeddings is a function that returns word-level embeddings\n",
    "# sentence = [\"This\", \"is\", \"a\", \"test\", \"sentence\"]\n",
    "# word_embeddings = get_word_embeddings(node_df[\"Journal-ref\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9924fe0a-6624-4650-a042-c7b85d302239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017653cc-c004-4d50-9837-81963ccf95e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Design a kernel such that the weight decreases as the position increases\n",
    "# weights = np.linspace(1, 0.1, len(word_embeddings.shape[]))\n",
    "\n",
    "# # Apply the kernel\n",
    "# weighted_embeddings = word_embeddings * weights[:, None]\n",
    "\n",
    "# # Sum the weighted embeddings to get the sentence embedding\n",
    "# roberta_sentence_embedding = np.sum(weighted_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81e7a54-75ee-4efc-a117-c72ca39b0435",
   "metadata": {},
   "source": [
    "## Building K-Nearest-Neighbor Networks\n",
    "\n",
    "We are going to use the sentence encoded abstracts in `node_df[\"Abstracts-All-Embedding\"]` to create a KNN Network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee46dd-a0c0-4246-ab89-675494665237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
